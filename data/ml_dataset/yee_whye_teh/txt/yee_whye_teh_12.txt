Collapsed Variational Inference for HDP

Yee Whye Teh
Gatsby Unit

University College London
ywteh@gatsby.ucl.ac.uk

Kenichi Kurihara

Dept. of Computer Science
Tokyo Institute of Technology

Max Welling

ICS

UC Irvine

kurihara@mi.cs.titech.ac.jp

welling@ics.uci.edu

Abstract

A wide variety of Dirichlet-multinomial ‘topic’ models have found interesting ap-
plications in recent years. While Gibbs sampling remains an important method of
inference in such models, variational techniques have certain advantages such as
easy assessment of convergence, easy optimization without the need to maintain
detailed balance, a bound on the marginal likelihood, and side-stepping of issues
with topic-identiﬁability. The most accurate variational technique thus far, namely
collapsed variational latent Dirichlet allocation, did not deal with model selection
nor did it include inference for hyperparameters. We address both issues by gen-
eralizing the technique, obtaining the ﬁrst variational algorithm to deal with the
hierarchical Dirichlet process and to deal with hyperparameters of Dirichlet vari-
ables. Experiments show a signiﬁcant improvement in accuracy.

1

Introduction

Many applications of graphical models have traditionally dealt with discrete state spaces, where
each variable is multinomial distributed given its parents [1]. Without strong prior knowledge on
the structure of dependencies between variables and their parents, the typical Bayesian prior over
parameters has been the Dirichlet distribution. This is because the Dirichlet prior is conjugate to
the multinomial, leading to simple and efﬁcient computations for both the posterior over parameters
and the marginal likelihood of data. When there are latent or unobserved variables, the variational
Bayesian approach to posterior estimation, where the latent variables are assumed independent from
the parameters, has proven successful [2].
In recent years there has been a proliferation of graphical models composed of a multitude of multi-
nomial and Dirichlet variables interacting in various inventive ways. The major classes include the
latent Dirichlet allocation (LDA) [3] and many other topic models inspired by LDA, and the hier-
archical Dirichlet process (HDP) [4] and many other nonparametric models based on the Dirichlet
process (DP). LDA pioneered the use of Dirichlet distributed latent variables to represent shades
of membership to different clusters or topics, while the HDP pioneered the use of nonparametric
models to sidestep the need for model selection.
For these Dirichlet-multinomial models the inference method of choice is typically collapsed Gibbs
sampling, due to its simplicity, speed, and good predictive performance on test sets. However there
are drawbacks as well: it is often hard to access convergence of the Markov chains, it is harder still
to accurately estimate the marginal probability of the training data or the predictive probability of
test data (if latent variables are associated with the test data), averaging topic-dependent quantities
based on samples is not well-deﬁned because the topic labels may have switched during sampling
and avoiding local optima through large MCMC moves such as split and merge algorithms are tricky
to implement due to the need to preserve detailed balance. Thus there seems to be a genuine need to
consider alternatives to sampling.
For LDA and its cousins, there are alternatives based on variational Bayesian (VB) approximations
[3] and on expectation propagation (EP) [5]. [6] found that EP was not efﬁcient enough for large

scale applications, while VB suffered from signiﬁcant bias resulting in worse predictive performance
than Gibbs sampling. [7] addressed these issues by proposing an improved VB approximation based
on the idea of collapsing, that is, integrating out the parameters while assuming that other latent
variables are independent. As for nonparametric models, a number of VB approximations have
been proposed for DP mixture models [8, 9], while to our knowledge none has been proposed for
the HDP thus far ([10] derived a VB inference for the HDP, but dealt only with point estimates for
higher level parameters).
In this paper we investigate a new VB approach to inference for the class of Dirichlet-multinomial
models. To be concrete we focus our attention on an application of the HDP to topic modeling [4],
though the approach is more generally applicable. Our approach is an extension of the collapsed
VB approximation for LDA (CV-LDA) presented in [7], and represents the ﬁrst VB approximation
to the HDP1. We call this the collapsed variational HDP (CV-HDP). The advantage of CV-HDP
over CV-LDA is that the optimal number of variational components is not ﬁnite. This implies, apart
from local optima, that we can keep adding components indeﬁnitely while the algorithm will take
care removing unnecessary clusters. Ours is also the ﬁrst variational algorithm to treat full posterior
distributions over the hyperparameters of Dirichlet variables, and we show experimentally that this
results in signiﬁcant improvements in both the variational bound and test-set likelihood. We expect
our approach to be generally applicable to a wide variety of Dirichlet-multinomial models beyond
what we have described here.

2 A Nonparametric Hierarchical Bayesian Topic Model

We consider a document model where each document in a corpus is modelled as a mixture over
topics, and each topic is a distribution over words in the vocabulary. Let there be D documents in
the corpus, and W words in the vocabulary. For each document d = 1, . . . , D, let θd be a vector of
mixing proportions over topics. For each topic k, let φk be a vector of probabilities for words in that
topic. Words in each document are drawn as follows: ﬁrst choose a topic k with probability θdk,
then choose a word w with probability φkw. Let xid be the ith word token in document d, and zid
its chosen topic. We have,

zid | θd ∼ Mult(θd)

xid | zid, φzid ∼ Mult(φzid)

We place Dirichlet priors on the parameters θd and φk,

θd | π ∼ Dir(απ)

φk | τ ∼ Dir(βτ)

(1)

(2)

where π is the corpus-wide distribution over topics, τ is the corpus-wide distribution over the vo-
cabulary, and α and β are concentration parameters describing how close θd and φk are to their
respective prior means π and τ.
If the number of topics K is ﬁnite and ﬁxed, the above model is LDA. As we usually do not know
the number of topics a priori, and would like a model that can determine this automatically, we
consider a nonparametric extension reposed on the HDP [4]. Speciﬁcally, we have a countably inﬁ-
nite number of topics (thus θd and π are inﬁnite-dimensional vectors), and we use a stick-breaking
representation [11] for π:

(cid:81)k−1
l=1 (1 − ˜πl)

πk = ˜πk

DP(γ, Dir(βτ)), where Gd =(cid:80)∞

(3)
In the normal Dirichlet process notation, we would equivalently have Gd ∼ DP(α, G0) and G0 ∼
k=1 πkδφk are sums of point masses, and
Dir(βτ) is the base distribution. Finally, in addition to the prior over π, we place priors over the
other hyperparameters α, β, γ and τ of the model as well,

for k = 1, 2, . . .

˜πk|γ ∼ Beta(1, γ)

k=1 θdkδφk and G0 =(cid:80)∞

α ∼ Gamma(aα, bα)

β ∼ Gamma(aβ, bβ)

γ ∼ Gamma(aγ, bγ)

τ ∼ Dir(aτ )

(4)

The full model is shown graphically in Figure 1(left).

1In this paper, by HDP we shall mean the two level HDP topic model in Section 2. We do not claim to have
derived a VB inference for the general HDP in [4], which is signiﬁcantly more difﬁcult; see ﬁnal discussions.

Figure 1: Left: The HDP topic model. Right: Factor graph of the model with auxiliary variables.

3 Collapsed Variational Bayesian Inference for HDP

There is substantial empirical evidence that marginalizing out variables is helpful for efﬁcient infer-
ence. For instance, in [12] it was observed that Gibbs sampling enjoys better mixing, while in [7] it
was shown that variational inference is more accurate in this collapsed space. In the following we
will build on this experience and propose a collapsed variational inference algorithm for the HDP,
based upon ﬁrst replacing the parameters with auxiliary variables, then effectively collapsing out the
auxiliary variables variationally. The algorithm is fully Bayesian in the sense that all parameter pos-
teriors are treated exactly and full posterior distributions are maintained for all hyperparameters. The
only assumptions made are independencies among the latent topic variables and hyperparameters,
and that there is a ﬁnite upper bound on the number of topics used (which is found automatically).
The only inputs required of the modeller are the values of the top-level parameters aα, bα, ....

D(cid:89)

d=1

3.1 Replacing parameters with auxiliary variables
In order to obtain efﬁcient variational updates, we shall replace the parameters θ = {θd} and φ =
{φk} with auxiliary variables. Speciﬁcally, we ﬁrst integrate out the parameters; this gives a joint
distribution over latent variables z = {zid} and word tokens x = {xid} as follows:

(cid:81)K

K(cid:89)

(cid:81)W

p(z, x|α, β, γ, π, τ) =

Γ(απk+ndk·)

Γ(β)

Γ(βτw+n·kw)

Γ(α)

k=1

Γ(απk)

Γ(α+nd··)

(5)
with ndkw = #{i : xid = w, zid = k}, dot denoting sum over that index, and K denoting an index
such that zid ≤ K for all i, d. The ratios of gamma functions in (5) result from the normalization
constants of the Dirichlet densities of θ and φ, and prove to be nuisances for updating the hyperpa-
rameter posteriors. Thus we introduce four sets of auxiliary variables: ηd and ξk taking values in
[0, 1], and sdk and tkw taking integral values. This results in a joint probability distribution over an
expanded system,

Γ(β+n·k·)

Γ(βτw)

w=1

k=1

p(z, x, η, ξ, s, t|α, β, γ, π, τ)
k=1[ndk·

(1−ηd)nd··−1QK

D(cid:89)

ηα−1

d

sdk ](απk)sdk

K(cid:89)

=

Γ(nd··)

(1−ξk)n·k·−1QW

ξβ−1

k

w=1[n·kw

tkw ](βτw)tkw

Γ(n·k·)

(6)

d=1

k=1

where [ n
m] are unsigned Stirling numbers of the ﬁrst kind, and bold face letters denote sets of the
corresponding variables. It can be readily veriﬁed that marginalizing out η, ξ, s and t reduces (6)
to (5). The main insight is that conditioned on z and x the auxiliary variables are independent and
have well-known distributions. Speciﬁcally, ηd and ξk are Beta distributed, while sdk (respectively
tkw) is the random number of occupied tables in a Chinese restaurant process with ndk· (respectively
n·kw) customers and a strength parameter of απk (respectively βτw) [13, 4].

3.2 The Variational Approximation

We assume the following form for the variational posterior over the auxiliary variables system:

q(z, η, ξ, s, t, α, β, γ, τ, π) = q(α)q(β)q(γ)q(τ)q(π)q(η, ξ, s, t|z)

q(zid)

(7)

where the dependence of auxiliary variables on z is modelled exactly. [7] showed that modelling
exactly the dependence of a set of variables on another set is equivalent to integrating out the ﬁrst

d=1

i=1

D(cid:89)

nd··(cid:89)

topics k=1...∞        document d=1...Dwords i=1...ndπzidxidθdγαβτκφktopics k=1...∞        document d=1...Dwords i=1...ndπzidxidβταγηdsdξktkκset. Thus we can interpret (7) as integrating out the auxiliary variables with respect to z. Given the
above factorization, q(π) further factorizes so that the ˜πk’s are independent, as do the posterior over
auxiliary variables.
For computational tractability, we also truncated our posterior representation to K topics. Specif-
ically, we assumed that q(zid > K) = 0 for every i and d. A consequence is that observations
have no effect on ˜πk and φk for all k > K, and these parameters can be exactly marginalized out.
Notice that our approach to truncation is different from that in [8], who implemented a truncation
at T by instead ﬁxing the posterior for the stick weight q(vT = 1) = 1, and from that in [9], who
assumed that the variational posteriors for parameters beyond the truncation level are set at their
priors. Our truncation approximation is nested like that in [9], and unlike that in [8]. Our approach
is also simpler than that in [9], which requires computing an inﬁnite sum which is intractable in the
case of HDPs. We shall treat K as a parameter of the variational approximation, possibly optimized
by iteratively splitting or merging topics (though we have not explored these in this paper; see dis-
cussion section). As in [9], we reordered the topic labels such that E[n·1·] > E[n·2·] > ··· . An
expression for the variational bound on the marginal log-likelihood is given in appendix A.

3.3 Variational Updates

∂y

In this section we shall derive the complete set of variational updates for the system. In the following
E[log y] the geometric expectation, and V[y] = E[y2] −
E[y] denotes the expectation of y, G[y] = e
E[y]2 the variance. Let Ψ(y) = ∂ log Γ(y)
be the digamma function. We shall also employ index
summation shorthands: · sums out that index, while >l sums over i where i > l.
Hyperparameters. Updates for the hyperparameters are derived using the standard fully factorized
variational approach, since they are assumed independent from each other and from other variables.
For completeness we list these here, noting that α, β, γ are gamma distributed in the posterior, ˜πk’s
are beta distributed, and τ is Dirichlet distributed:

q(α) ∝ αaα+E[s··]−1e−α(bα−P
q(β) ∝ βaβ +E[t··]−1e−β(bβ−P
q(γ) ∝ γaγ +K−1e−γ(bγ−PK
(cid:81)
In subsequent updates we will need averages and geometric averages of these quantities which can be
k ⇒ G[xk] = eΨ(ak)/eΨ(P
extracted using the following identities: p(x) ∝ xa−1e−bx ⇒ E[x] = a/b, G[x] = eΨ(a)/b, p(x) ∝
G[απk] = G[α]G[πk], G[βτw] = G[β]G[τw] and G[πk] = G[˜πk](cid:81)k−1
k xak−1
k ak). Note also that the geometric expectations factorizes:

E[s·k]
k
w=1 τ aτ +E[t·w]−1

E[log ηd])
E[log ξk])
E[log(1−˜πk)]

q(τ) ∝(cid:81)W

(1 − ˜πk)E[γ]+E[s·>k]−1

q(˜πk) ∝ ˜π

G[1 − ˜πl].

(8)

k=1

d

k

w

l=1

Auxiliary variables. The variational posteriors for the auxiliary variables depend on z through the
counts ndkw. ηd and ξk are beta distributed. If ndk· = 0 then q(sdk = 0) = 1 otherwise q(sdk) > 0
only if 1 ≤ sdk ≤ ndk·. Similarly for tkw. The posteriors are:

q(ηd|z) ∝ η
q(ξk|z) ∝ ξ

E[α]−1
d
E[β]−1
k

(1 − ηd)nd··−1
(1 − ξk)n·k·−1

q(sdk = m|z) ∝ [ndk·
q(tkw = m|z) ∝ [n·kw

m ] (G[απk])m
m ] (G[βτw])m

(9)

To obtain expectations of the auxiliary variables in (8) we will have to average over z as well. For
ηd this is E[log ηd] = Ψ(E[α]) − Ψ(E[α] + nd··) where nd·· is the (ﬁxed) number of words in
document d. For the other auxiliary variables these expectations depend on counts which can take
on many values and a na¨ıve computation can be expensive. We derive computationally tractable
approximations based upon an improvement to the second-order approximation in [7]. As we see in
the experiments these approximations are very accurate. Consider E[log ξk]. We have,

E[log ξk|z] = Ψ(E[β]) − Ψ(E[β] + n·k·)

(10)
and we need to average over n·k· as well. [7] tackled a similar problem with log instead of Ψ using
a second order Taylor expansion to log. Unfortunately such an approximation failed to work in our
case as the digamma function Ψ(y) diverges much more quickly than log y at y = 0. Our solution
is to treat the case n·k· = 0 exactly, and apply the second-order approximation when n·k· > 0. This
leads to the following approximation:

E[log ξk] ≈ P+[n·k·](cid:0)Ψ(E[β]) − Ψ(E[β] + E+[n·k·]) − 1

V+[n·k·]Ψ(cid:48)(cid:48)(E[β] + E+[n·k·])(cid:1)

(11)

2

where P+ is the “probability of being positive” operator: P+[y] = q(y > 0), and E+[y], V+[y] are
the expectation and variance conditional on y > 0. The other two expectations are derived similarly,
making use of the fact that sdk and tkw are distributionally equal to the random numbers of tables
in Chinese restaurant processes:
E[sdk] ≈ G[απk]P+[ndk·]
E[tkw] ≈ G[βτw]P+[n·kw]

Ψ(G[απk]+E+[ndk·])−Ψ(G[απk])+ V+[ndk·]Ψ(cid:48)(cid:48)(G[απk]+E+[ndk·])
Ψ(G[βτw]+E+[n·kw])−Ψ(G[βτw])+ V+[n·kw]Ψ(cid:48)(cid:48)(G[βτw]+E+[n·kw])

(cid:16)
(cid:16)

(cid:17)

(cid:17)

(12)

2

2

E[ndk·] =(cid:80)

As in [7], we can efﬁciently track the relevant quantities above by noting that each count is a sum of
independent Bernoulli variables. Consider ndk· as an example. We keep track of three quantities:
i log q(zid(cid:54)= k) (13)

i q(zid = k)q(zid(cid:54)= k) Z[ndk·] =(cid:80)

i q(zid = k) V[ndk·] =(cid:80)

Some algebraic manipulations now show that:

P+[ndk·] = 1 − e

Z[ndk·] E+[ndk·] = E[ndk·]
P+[ndk·]

V+[ndk·] = V[ndk·]

P+[ndk·] − e

Z[ndk·]E+[ndk·]

(14)

[7] showed that if the dependence of a set of variables, say A, on
Topic assignment variables.
another set of variables, say z, is modelled exactly, then in deriving the updates for z we may
equivalently integrate out A. Applying to our situation with A = {η, ξ, s, t}, we obtain updates
similar to those in [7], except that the hyperparameters are replaced by either their expectations
or their geometric expectations, depending on which is used in the updates for the corresponding
auxiliary variables:

q(zid = k) ∝G(cid:2)G[απk] + n¬id
≈∝(cid:0)G[απk] + E[n¬id

(cid:18)

−

2(G[απk]+E[n¬id

exp

4 Experiments

dk·(cid:3)G(cid:2)G[βτxid] + n¬id·kxid
dk· ](cid:1)(cid:0)G[βτxid] + E[n¬id·kxid
dk· ])2 −

V[n¬id
dk· ]

V[n¬id·kxid

]

2(G[βτxid ]+E[n¬id·kxid

(cid:3)G(cid:2)E[β] + n¬id·k·(cid:3)−1
](cid:1)(cid:0)E[β] + E[n¬id·k· ](cid:1)−1

])2 +

V[n¬id·k· ]

2(E[β]+E[n¬id·k· ])2

(cid:19)

(15)

We implemented and compared performances for 5 inference algorithms for LDA and HDP: 1)
variational LDA (V-LDA) [3], collapsed variational LDA (CV-LDA) [7], collapsed variational HDP
(CV-HDP, this paper), collapsed Gibbs sampling for LDA (G-LDA) [12] and the direct assignment
Gibbs sampler for HDP (G-HDP) [4].
We report results on the following 3 datasets: i) KOS (W = 6906, D = 3430, number of word-
tokens N = 467, 714), ii) a subset of the Reuters dataset consisting of news-topics with a number
of documents larger than 300 (W = 4593, D = 8433, N = 566, 298), iii) a subset of the 20News-
groups dataset consisting of the topics ‘comp.os.ms-windows.misc’, ‘rec.autos’, ‘rec.sport.baseball’,
‘sci.space’ and ‘talk.politics.misc’ (W = 8424, D = 4716, N = 437, 850).
For G-HDP we use the released code at http://www.gatsby.ucl.ac.uk/∼ywteh/research/software.html.
The variables β, τ are not adapted in that code, so we ﬁxed them at β = 100 and τw = 1/W
for all algorithms (see below for discussion regarding adapting these in CV-HDP). G-HDP was
initialized with either 1 topic (G-HDP1) or with 100 topics (G-HDP100). For CV-HDP we use
the following initialization: E[β] = G[β] = 100 and G[τw] = 1/W (kept ﬁxed to compare with
G-HDP), E[α] = aα/bα, G[α] = eΨ(aα)/bα, E[γ] = aγ/bγ, G[πk] = 1/K and q(zij = k) ∝ 1 + u
with u ∼ U[0, 1]. We set2 hyperparameters aα, bα, aβ, bβ in the range between [2, 6], while aγ, bγ
was chosen in the range [5, 10] and aτ in [30 − 50]/W . The number of topics used in CV-HDP
was truncated at 40, 80, and 120 topics, corresponding to the number of topics used in the LDA
algorithms. Finally, for all LDA algorithms we used α = 0.1, π = 1/K.

2We actually set these values using a ﬁxed but somewhat elaborate scheme which is the reason they ended
up different for each dataset. Note that this scheme simply converts prior expectations about the number of
topics and amount of sharing into hyperparameter values, and that they were never tweaked. Since they always
ended up in these compact ranges and since we do not expect a strong dependence on their values inside these
ranges we choose to omit the details.

Performance was evaluated by comparing i) the in-sample (train) variational bound on the log-
likelihood for all three variational methods and ii) the out-of-sample (test) log-likelihood for all ﬁve
methods. All inference algorithms were run on 90% of the words in each document while test-
set performance was evaluated on the remaining 10% of the words. Test-set log-likelihood was
computed as follows for the variational methods:

¯θjk

¯φkxtest

ij

¯θjk = απk+Eq[njk·]
α+Eq[nj··]

¯φkw = βτw+Eq[n·kw]
β+Eq[n·k·]

(16)

k

ij

p(xtest) =(cid:81)

(cid:80)

Note that we used estimated mean values of θjk and φkw [14]. For CV-HDP we replaced all hy-
perparameters by their expectations. For the Gibbs sampling algorithms, given S samples from the
posterior, we used:

p(xtest) =(cid:81)

(cid:80)S

(cid:80)

1
S

ij

s=1

kxtest
ij

k θs

jk = αsπs
θs

k+ns
jk·
αs+ns
j··

jkφs

kw = βτw+ns·kw
φs
β+ns·k·

(17)
We used all samples obtained by the Gibbs sampling algorithms after an initial burn-in period; each
point in the predictive probabilities plots below is obtained from the samples collected thus far.
The results, shown in Figure 2, display a signiﬁcant improvement in accuracy of CV-HDP over
CV-LDA, both in terms of the bound on the training log-likelihood as well as for the test-set log-
likelihood. This is caused by the fact that CV-HDP is learning the variational distributions over the
hyperparameters. We note that we have not trained β or τ for any of these methods. In fact, initial
results for CV-HDP show no additional improvement in test-set log-likelihood, in some cases even
a deterioration of the results. A second observation is that convergence of all variational methods
is faster than for the sampling methods. Thirdly, we see signiﬁcant local optima effects in our
simulations. For example, G-HDP100 achieves the best results, better than G-HDP1, indicating that
pruning topics is a better way than adding topics to escape local optima in these models and leads to
better posterior modes.
In further experiments we have also found that the variational methods beneﬁt from better initializa-
tions due to local optima. In Figure 3 we show results when the variational methods were initialized
at the last state obtained by G-HDP100. We see that indeed the variational methods were able to ﬁnd
signiﬁcantly better local optima in the vicinity of the one found by G-HDP100, and that CV-HDP is
still consistently better than the other variational methods.

5 Discussion

In this paper we have explored collapsed variational inference for the HDP. Our algorithm is the ﬁrst
to deal with the HDP and with posteriors over the parameters of Dirichlet distributions. We found
that the CV-HDP performs signiﬁcantly better than the CV-LDA on both test-set likelihood and the
variational bound. A caveat is that CV-HDP gives slightly worse test-set likelihood than collapsed
Gibbs sampling. However, as discussed in the introduction, we believe there are advantages to
variational approximations that are not available to sampling methods. A second caveat is that our
variational approximation works only for two layer HDPs—a layer of group-speciﬁc DPs, and a
global DP tying the groups together. It would be interesting to explore variational approximations
for more general HDPs.
CV-HDP presents an improvement over CV-LDA in two ways. Firstly, we use a more sophisticated
variational approximation that can infer posterior distributions over the higher level variables in the
model. Secondly, we use a more sophisticated HDP based model with an inﬁnite number of topics,
and allow the model to ﬁnd an appropriate number of topics automatically. These two advances are
coupled, because we needed the more sophisticated variational approximation to deal with the HDP.
Along the way we have also proposed two useful technical tricks. Firstly, we have a new truncation
technique that guarantees nesting. As a result we know that the variational bound on the marginal
log-likelihood will reach its highest value (ignoring local optima issues) when K → ∞. This fact
should facilitate the search over number of topics or clusters, e.g. by splitting and merging topics, an
aspect that we have not yet fully explored, and for which we expect to gain signiﬁcantly from in the
face of the observed local optima issues in the experiments. Secondly, we have an improved second-
order approximation that is able to handle the often encountered digamma function accurately.
An issue raised by the reviewers and in need of more thought by the community is the need for better
evaluation criteria. The standard evaluation criteria in this area of research are the variational bound

Figure 2: Left column: KOS, Middle column: Reuters and Right column: 20Newsgroups. Top row:
log p(xtest) as a function of K, Middle row: log p(xtest) as a function of number of steps (deﬁned as number of
iterations multiplied by K) and Bottom row: variational bounds as a function of K. Log probabilities are on a
per word basis. Shown are averages and standard errors obtained by repeating the experiments 10 times with
random restarts. The distribution over the number of topics found by G-HDP1 are: KOS: K = 113.2 ± 11.4,
Reuters: K = 60.4 ± 6.4, 20News: K = 83.5 ± 5.0. For G-HDP100 we have: KOS: K = 168.3 ± 3.9,
Reuters: K = 122.2 ± 5.0, 20News: K = 128.1 ± 6.6.

Figure 3: G-HDP100 initialized variational methods (K = 130), compared against variational methods ini-
tialized in the usual manner with K = 130 as well. Results were averaged over 10 repeats.

4080120!8!7.8!7.6!7.4!7.2K4080120!8.4!8.2!8!7.8!7.6K04000800012000!8!7.8!7.6!7.4!7.2#steps4080120!6.6!6.4!6.2!6!5.8K4080120!7!6.8!6.6!6.4K04000800012000!7!6.8!6.6!6.4!6.2!6!5.8#steps4080120!7.4!7.2!7!6.8K4080120!8.2!8!7.8!7.6!7.4K04000800012000!8!7.8!7.6!7.4!7.2!7!6.8#steps  GHDP100GHDP1GLDACVHDPCVLDAVLDA  CVHDPCVLDAVLDA  GHDP100GHDP1GLDACVHDPCVLDAVLDA05000#0000!9!8&5!8!7&5!7#ste,svariational bound0500010000!7.8!7.6!7.4!7.2!7!6.8!6.6#stepslog p(test) / N  GHDP100Gibbs init. CVHDPGibbs init. CVLDAGibbs init. VLDArandom init. CVHDPrandom init. CVLDArandom init. VLDAand the test-set likelihood. However both confound improvements to the model and improvements
to the inference method. An alternative is to compare the computed posteriors over latent variables
on toy problems with known true values. However such toy problems are much smaller than real
world problems, and inferential quality on such problems may be of limited interest to practitioners.
We expect the proliferation of Dirichlet-multinomial models and their many exciting applications to
continue. For some applications variational approximations may prove to be the most convenient
tool for inference. We believe that the methods presented here are applicable to many models of this
general class and we hope to provide general purpose software to support inference in these models
in the future.

A Variational lower bound
E[log p(z,x|α,π,τ )−log q(z)]−KL[q(α)(cid:107)p(α)]−KL[q(β)(cid:107)p(β)]−PK
=P

Γ(G[α]G[πk]+ndk·)

Fh

log

log

d log

k

Fh
i
+P
E[α]P
E[β]P

d

k

Γ(G[α]G[πk])
G[α]

Γ(aα)

Γ(aα+E[s··])

E[s··]e

E[log ηd]−P

Γ(aβ )

Γ(aβ +E[t··])

G[β]

E[t··]e

E[log ξk]

Γ(E[α])

Γ(E[α]+nd··) +P
(bα−P
(bβ−P

E[log ηd])aα+E[s··]
E[log ξk])aβ +E[t··]

aα
α

dk

k

d

b

aβ
b
β

−log

−log

−P

k log

Γ(1+γ+E[s·k]+E[s·>k])
γΓ(1+E[s·k])Γ(γ+E[s·>k])

where F[f (n)]=P+[n](f (E+[n])+ 1

2

Acknowledgements

k=1 KL[q(˜πk)(cid:107)p(˜πk)]−KL[q(τ )(cid:107)p(τ )]
Γ(E[β])
Γ(E[β]+n·k·)

Γ(G[β]G[τw ])

Γ(G[β]G[τw ]+n·kw )

log

kw

Fh

(18)

i

dk

i=1 q(zid=k) log q(zid=k)

i
+P
Pnd

Q

G[˜πk]

E[s·k]G[1−˜πk]

G[τw]
V+[n]f(cid:48)(cid:48)(E+[n])) is the improved second order approximation.

E[s·>k]−log Γ(κ+E[t··])

Γ(κτw +E[t·w ])

Γ(κτw )

Γ(κ)

w

E[t·w ]

We thank the reviewers for thoughtful and constructive comments. MW was supported by NSF
grants IIS-0535278 and IIS-0447903.

References
[1] R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter. Probabilistic Networks and Expert

Systems. Springer-Verlag, 1999.

[2] M. J. Beal and Z. Ghahramani. Variational Bayesian learning of directed graphical models with hidden

variables. Bayesian Analysis, 1(4), 2006.

[3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning

Research, 3:993–1022, 2003.

[4] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal of the

American Statistical Association, 101(476):1566–1581, 2006.

[5] T. P. Minka and J. Lafferty. Expectation propagation for the generative aspect model. In Proceedings of

the Conference on Uncertainty in Artiﬁcial Intelligence, volume 18, 2002.

[6] W. Buntine and A. Jakulin. Applying discrete PCA in data analysis. In Proceedings of the Conference on

Uncertainty in Artiﬁcial Intelligence, volume 20, 2004.

[7] Y. W. Teh, D. Newman, and M. Welling. A collapsed variational Bayesian inference algorithm for latent

Dirichlet allocation. In Advances in Neural Information Processing Systems, volume 19, 2007.

[8] D. M. Blei and M. I. Jordan. Variational inference for Dirichlet process mixtures. Bayesian Analysis,

1(1):121–144, 2006.

[9] K. Kurihara, M. Welling, and N. Vlassis. Accelerated variational DP mixture models. In Advances in

Neural Information Processing Systems, volume 19, 2007.

[10] P. Liang, S. Petrov, M. I. Jordan, and D. Klein. The inﬁnite PCFG using hierarchical Dirichlet processes.

In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2007.
[11] J. Sethuraman. A constructive deﬁnition of Dirichlet priors. Statistica Sinica, 4:639–650, 1994.
[12] T.L. Grifﬁths and M. Steyvers. A probabilistic approach to semantic representation. In Proceedings of

the 24th Annual Conference of the Cognitive Science Society, 2002.

[13] C. E. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems.

Annals of Statistics, 2(6):1152–1174, 1974.

[14] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, Gatsby Computa-

tional Neuroscience Unit, University College London, 2003.

