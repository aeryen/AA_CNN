2013 IEEE Conference on Computer Vision and Pattern Recognition
2013 IEEE Conference on Computer Vision and Pattern Recognition
2013 IEEE Conference on Computer Vision and Pattern Recognition

Pedestrian Detection with

Unsupervised Multi-Stage Feature Learning

Pierre Sermanet

Soumith Chintala

Koray Kavukcuoglu

Yann LeCun

Courant Institute of Mathematical Sciences, New York University

sermanet,koray,soumith,yann@cs.nyu.edu

Abstract

Pedestrian detection is a problem of considerable prac-
tical interest. Adding to the list of successful applications
of deep learning methods to vision, we report state-of-the-
art and competitive results on all major pedestrian datasets
with a convolutional network model. The model uses a few
new twists, such as multi-stage features, connections that
skip layers to integrate global shape information with local
distinctive motif information, and an unsupervised method
based on convolutional sparse coding to pre-train the ﬁlters
at each stage.

1. Introduction

Pedestrian detection is a key problem for surveillance,
automotive safety and robotics applications. The wide vari-
ety of appearances of pedestrians due to body pose, occlu-
sions, clothing, lighting and backgrounds makes this task
challenging.

All existing state-of-the-art methods use a combination
of hand-crafted features such as Integral Channel Fea-
tures [9], HoG [5] and their variations [13, 33] and com-
binations [38], followed by a trainable classiﬁer such as
SVM [13, 28], boosted classiﬁers [9] or random forests [7].
While low-level features can be designed by hand with good
success, mid-level features that combine low-level features
are difﬁcult to engineer without the help of some sort of
learning procedure. Multi-stage recognizers that learn hier-
archies of features tuned to the task at hand can be trained
end-to-end with little prior knowledge. Convolutional Net-
works (ConvNets) [23] are examples of such hierarchical
systems with end-to-end feature learning that are trained
in a supervised fashion. Recent works have demonstrated
the usefulness of unsupervised pre-training for end-to-end
training of deep multi-stage architectures using a variety
of techniques such as stacked restricted Boltzmann ma-
chines [16], stacked auto-encoders [4] and stacked sparse
auto-encoders [32], and using new types of non-linear trans-
forms at each layer [17, 20].

1063-6919/13 $26.00 © 2013 IEEE
1063-6919/13 $26.00 © 2013 IEEE
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.465
DOI 10.1109/CVPR.2013.465
DOI 10.1109/CVPR.2013.465

3624
3624
3626

Figure 1: 128 9 × 9 ﬁlters trained on grayscale INRIA im-
ages using Algorithm 1. It can be seen that in addition to edge
detectors at multiple orientations, our systems also learns more
complicated features such as corner and junction detectors.

Supervised ConvNets have been used by a number of au-
thors for such applications as face, hand detection [37, 29,
15, 31, 14, 36]. More recently, a large ConvNet by [21]
achieved a breakthrough on a 1000-class ImageNet detec-
tion task. The main contribution of this paper is to show
that the ConvNet model, with a few important twists, con-
sistently yields state of the art and competitive results on
all major pedestrian detection benchmarks. The system
uses unsupervised convolutional sparse auto-encoders to
pre-train features at all levels from the relatively small IN-
RIA dataset [5], and end-to-end supervised training to train
the classiﬁer and ﬁne-tune the features in an integrated fash-
ion. Additionally, multi-stage features with layer-skipping
connections enable output stages to combine global shape
detectors with local motif detectors.

Processing speed in pedestrian detection has recently
seen great progress, enabling real-time operation with-
out sacriﬁcing quality. [3] manage to entirely avoid image
rescaling for detection while observing quality improve-
ments. While processing speed is not the focus of this pa-
per, features and classiﬁer approximations introduced by [8]
and [3] may be applicable to deep learning models for faster
detection, in addition to GPU optimizations.

2. Learning Feature Hierarchies

Much of the work on pedestrian detection have focused
on designing representative and powerful features [5, 9, 8,
38]. In this work, we show that generic feature learning al-
gorithms can produce successful feature extractors that can
achieve state-of-the-art results.

Supervised learning of end-to-end systems on images
have been shown to work well when there is abundant la-
beled samples [23], including for detection tasks [37, 29,
15, 31, 14, 36]. However, for many input domains, it is
hard to ﬁnd adequate number of labeled data. In this case,
one can resort to designing useful features by using domain
knowledge, or an alternative way is to use unsupervised
learning algorithms. Recently unsupervised learning algo-
rithms have been demonstrated to produce good features for
generic object recognition problems [24, 25, 18, 20].

In [16], it was shown that unsupervised learning can be
used to train deep hierarchical models and the ﬁnal repre-
sentation achieved is actually useful for a variety of differ-
ent tasks [32, 24, 4]. In this work, we also follow a similar
approach and train a generic unsupervised model at each
layer using the output representation from the layer before.
This process is then followed by supervised updates to the
whole hierarchical system using label information.

Figure 2: A subset of 7 × 7 second layer ﬁlters trained on
grayscale INRIA images using Algorithm 2. Each row in the ﬁg-
ure shows ﬁlters that connect to a common output feature map.
It can be seen that they extract features at similar locations and
shapes, e.g. the bottom row tends to aggregate horizontal features
towards the bottom of the ﬁlters.

2.1. Hierarchical Model

A hierarchical feature extraction system consists of mul-
tiple levels of feature extractors that perform the same ﬁl-
tering and non-linear transformation functions in successive
layers. Using a particular generic parametrized function one
can then map the inputs into gradually more higher level (or
abstract) representations [23, 16, 4, 32, 24]. In this work
we use sparse convolutional feature hierarchies as proposed
in [20]. Each layer of the unsupervised model contains a
convolutional sparse coding algorithm and a predictor func-
tion that can be used for fast inference. After the last layer

3625
3625
3627

a classiﬁer is used to map the feature representations into
class labels. Both the sparse coding dictionary and the pre-
dictor function do not contain any hard-coded parameter
and are trained from the input data.

The training procedure for this model is similar to [16].
Each layer is trained in an unsupervised manner using the
representation from previous layer (or the input image for
the initial layer) separately. After the whole multi-stage sys-
tem is trained in a layer-wise fashion, the complete architec-
ture followed by a classiﬁer is ﬁne-tuned using labeled data.

2.2. Unsupervised Learning

Recently sparse coding has seen much interest in many
ﬁelds due to its ability to extract useful feature representa-
tions from data, The general formulation of sparse coding is
a linear reconstruction model using an overcomplete dictio-
nary D ∈ Rm×n where m > n and a regularization penalty
on the mixing coefﬁcients z ∈ Rn.

z

∗ = arg min

z

(cid:3)x − Dz(cid:3)2

2 + λs(z)

(1)

The aim is to minimize equation 1 with respect to z to obtain
the optimal sparse representation z∗ that correspond to input
x ∈ Rm. The exact form of s(z) depends on the particular
sparse coding algorithm that is used, here, we use the (cid:3).(cid:3)1
norm penalty, which is the sum of the absolute values of
all elements of z. It is immediately clear that the solution of
this system requires an optimization process. Many efﬁcient
algorithms for solving the above convex system has been
proposed in recent years [1, 6, 2, 26]. However, our aim is
to also learn generic feature extractors. For that reason we
minimize equation 1 wrt D too.

∗

z

, D∗ = arg min
z,D

(cid:3)x − Dz(cid:3)2

2 + λ(cid:3)z(cid:3)1

(2)

This resulting equation is non-convex in D and z at the same
time, however keeping one ﬁxed, the problem is still convex
wrt to the other variable. All sparse modeling algorithms
that adopt the dictionary matrix D exploit this property
and perform a coordinate descent like minimization pro-
cess where each variable is updated in succession. Follow-
ing [30] many authors have used sparse dictionary learning
to represent images [27, 1, 19]. However, most of the sparse
coding models use small image patches as input x to learn
the dictionary D and then apply the resulting model to every
overlapping patch location on the full image. This approach
assumes that the sparse representation for two neighboring
patches with a single pixel shift is completely independent,
thus produces very redundant representations. [20, 39] have
introduced convolutional sparse modeling formulations for
feature learning and object recognition and we use the Con-
volutional Predictive Sparse Decomposition (CPSD) model
proposed in [20] since it is the only convolutional sparse
coding model providing a fast predictor function that is suit-
able for building multi-stage feature representations. The

particular predictor function we use is similar to a single
layer ConvNet of the following form:

f (x; g, k, b) = ˜z = {˜zj}j=1..n

˜zj = gj × tanh(x ⊗ kj + bj)

(3)
(4)

where ⊗ operator represents convolution operator that ap-
plies on a single input and single ﬁlter. In this formulation
x is a p × p grayscale input image, k ∈ Rn×m×m is a set
of 2D ﬁlters where each ﬁlter is kj ∈ Rm×m, g ∈ Rn and
b ∈ Rn are vectors with n elements, the predictor output
˜z ∈ R
n×p−m+1×p−m+1 is a set of feature maps where each
of ˜zj is of size p − m + 1 × p − m + 1. Considering this gen-
eral predictor function, the ﬁnal form of the convolutional
unsupervised energy for grayscale inputs is as follows:

(cid:3)

(cid:2)(cid:2)(cid:2)x −

ECP SD = EConvSC + βEP red

EConvSC =

Dj ⊗ zj

j

EP red = (cid:3)z

∗ − f (x; g, k, b)(cid:3)2

2

(cid:2)(cid:2)(cid:2)2

2

+ λ(cid:3)z(cid:3)1

(5)

(6)

(7)

∂ECP SD

where D is a dictionary of ﬁlters the same size as k and
β is a hyper-parameter. The unsupervised learning proce-
dure is a two step coordinate descent process. At each it-
eration, (1) Inference: The parameters W = {D, g, k, b}
are kept ﬁxed and equation 6 is minimized to obtain the
optimal sparse representation z ∗, (2) Update: Keeping z∗
ﬁxed, the parameters W updated using a stochastic gradi-
ent step: W ← W − η
∂W where η is the learning rate
parameter. The inference procedure requires us to carry out
the sparse coding problem solution. For this step we use
the FISTA method proposed in [2]. This method is an ex-
tension of the original iterative shrinkage and thresholding
algorithm [6] using an improved step size calculation with
a momentum-like term. We apply the FISTA algorithm in
the image domain adopting the convolutional formulation.
For color images or other multi-modal feature represen-
tations, the input x is a set of feature maps indexed by i and
the representation z is a set of feature maps indexed by j
for each input map i. We deﬁne a map of connections P
th output feature map is con-
from input x to features z. A j
nected to a set Pj of input feature maps. Thus, the predictor
function in Algorithm 1 is deﬁned as:

˜zj = gj × tanh

(xi ⊗ kj,i) + bj

(8)

and the reconstruction is computed using the inverse map
¯P :

(cid:3)

EConvSC =

(cid:3)xi −

Di,j ⊗ zj(cid:3)2

2 + λ(cid:3)z(cid:3)1

(9)

i

j∈ ¯Pi

For a fully connected layer, all the input features are con-
nected to all the output features, however it is also common

3626
3626
3628

⎞
⎠

⎛
⎝(cid:3)

i∈Pj

(cid:3)

to use sparse connection maps to reduce the number of pa-
rameters. The online training algorithm for unsupervised
training of a single layer is:

Algorithm 1 Single layer unsupervised training.
function Unsup(x, D, P, {λ, β}, {g, k, b}, η)

Set: f (x; g, k, b) from eqn 8, W
Initialize: z = ∅, D and W
repeat

p randomly.

p = {g, k, b}.

Perform inference, minimize equation 9 wrt z using
FISTA [2]
Do a stochastic update on D and W
∂EP red
η
∂W p

p. D ← D −

p ← W

and W

p − η

∂EConvSC

∂D

until convergence
Return: {D, g, k, b}

end function

2.3. Non-Linear Transformations

Once the unsupervised learning for a single stage is com-
pleted, the next stage is trained on the feature representation
from the previous one. In order to obtain the feature repre-
sentation for the next stage, we use the predictor function
f (x) followed by non-linear transformations and pooling.
Following the multi-stage framework used in [20], we ap-
ply absolute value rectiﬁcation, local contrast normalization
and average down-sampling operations.
Absolute Value Rectiﬁcation is applied component-wise to
the whole feature output from f (x) in order to avoid cancel-
lation problems in contrast normalization and pooling steps.
Local Contrast Normalization is a non-linear process that
enhances the most active feature and suppresses the other
ones. The exact form of the operation is as follows:

(cid:8)(cid:3)

vi = xi − xi ⊗ w , σ =

yi =

vi

max(c, σ)

i

w ⊗ v2
i

(10)

(11)

(cid:9)

where i is the feature map index and w is a 9 × 9 Gaus-
sian weighting function with normalized weights so that
ipq wpq = 1. For each sample, the constant c is set to

mean(σ) in the experiments.
Average Down-Sampling operation is performed using a
ﬁxed size boxcar kernel with a certain step size. The size
of the kernel and the stride are given for each experiment in
the following sections.

Once a single layer of the network is trained, the features
for training a successive layer is extracted using the predic-
tor function followed by non-linear transformations. De-
tailed procedure of training an N layer hierarchical model
is explained in Algorithm 2.

The ﬁrst layer features can be easily displayed in the pa-
rameter space since the parameter space and the input space
is same, however visualizing the second and higher level
features in the input space can only be possible when only

Algorithm 2 Multi-layer unsupervised training.
function HierarUnsup(x, ni, mi, Pi, {λi, βi}, {wi, si},
i = {1..N }, ηi)

Set: i = 1, X1 = x, lcn(x) using equations 10-11,
ds(X, w, s) as the down-sampling operator using box-
car kernel of size w × w and stride of size s in both
directions.
repeat

Set: Di, ki ∈ Rni×mi×mi, gi, bi ∈ Rni.
{Di, ki, gi, ki, bi} =

U nsup(Xi, Di, Pi, {λi, βi}, {gi, ki, bi}, ηi)

˜z = f (Xi; gi, ki, bi) using equation 8.
˜z = |˜z|
˜z = lcn(˜z)
Xi+1 = ds(˜z, wi, si)
i = i + 1
until i = N
end function

Figure 3: A multi-scale convolutional network. The top
row of maps constitute a regular ConvNet [17]. The bottom
row in which the 1st stage output is branched, subsampled
again and merged into the classiﬁer input provides a multi-
stage component to the classiﬁer stage. The multi-stage fea-
tures coming out of the 2nd stage extracts a global structure
as well as local details.

invertible operations are used in between layers. However,
since we use absolute value rectiﬁcation and local contrast
normalization operations mapping the second layer features
onto input space is not possible. In Figure 2 we show a sub-
set of 1664 second layer features in the parameter space.

2.4. Supervised Training

After the unsupervised learning of the hierarchical fea-
ture extraction system is completed using Algorithm 2, we
append a classiﬁer function, usually in the form of a linear
logistic regression, and perform stochastic online training
using labeled data.

2.5. Multi-Stage Features

ConvNets are usually organized in a strictly feed-
forward manner where one layer only takes the output of
the previous layer as input. Features extracted this way tend
to be high level features after a few stages of convolutions

3627
3627
3629

and subsampling. By branching lower levels’ outputs into
the top classiﬁer (Fig. 3), one produces features that extract
both global shapes and structures and local details, such as
a global silhouette and face components in the case of hu-
man detection. Contrary to [12], the output of the ﬁrst stage
is branched after the non-linear transformations and pool-
ing/subsampling operations rather than before.

We also use color information on the training data. For
this purpose we convert all images into YUV image space
and subsample the UV features by 3 since color information
is in much lower resolution. Then at the ﬁrst stage, we keep
feature extraction systems for Y and UV channels separate.
On the Y channel, we use 32 7 × 7 features followed by ab-
solute value rectiﬁcation, contrast normalization and 3 × 3
subsampling. On the subsampled UV channels, we extract
6 5 × 5 features followed by absolute value rectiﬁcation and
contrast normalization, skipping the usual subsampling step
since it was performed beforehand. These features are then
concatanated to produce 38 feature maps that are input to
the ﬁrst layer. The second layer feature extraction takes 38
feature maps and produces 68 output features using 2040
9 × 9 features. A randomly selected 20% of the connec-
tions in mapping from input features to output features is
removed to limit the computational requirements and break
the symmetry [23]. The output of the second layer features
are then transformed using absolute value rectiﬁcation and
contrast normalization followed by 2 × 2 subsampling. This
results in 17824 dimensional feature vector for each sample
which is then fed into a linear classiﬁer.

In Table 1, we show that multi-stage features improve ac-
curacy for different tasks, with different magnitudes. Great-
est improvements are obtained for pedestrian detection and
trafﬁc-sign classiﬁcation while only minimal gains are ob-
tained for house numbers classiﬁcation, a less complex task.

2.6. Bootstrapping

Bootstrapping is typically used in detection settings by
multiple phases of extracting the most offending negative
answers and adding these samples to the existing dataset
while training. For this purpose, we extract 3000 nega-
tive samples per bootstrapping pass and limit the number
of most offending answers to 5 for each image. We perform
3 bootstrapping passes in addition to the original training
phase (i.e. 4 training passes in total).

2.7. Non-Maximum Suppression

Non-maximum suppression (NMS) is used to resolve
conﬂicts when several bounding boxes overlap. For both
INRIA and Caltech experiments we use the widely accepted
PASCAL overlap criteria to determine a matching score be-
tween two bounding boxes ( intersection
) and if two boxes
overlap by more than 60%, only the one with the highest
score is kept. In [10]’s addendum, the matching criteria is
modiﬁed by replacing the union of the two boxes with the
minimum of the two. Therefore, if a box is fully contained
in another one the small box is selected. The goal for this

union

Task

Single-Stage features Multi-Stage features

Improvement %

Pedestrians detection (INRIA) (Fig. 4)

Trafﬁc Signs classiﬁcation (GTSRB) [35]
House Numbers classiﬁcation (SVHN) [34]

23.39%
1.80%
5.54%

17.29%
0.83%
5.36%

26.1%
54%
3.2%

Table 1: Error rates improvements of multi-stage features over single-stage features for different types of objects detection and
classiﬁcation. Improvements are signiﬁcant for multi-scale and textured objects such as trafﬁc signs and pedestrians but minimal for house
numbers.

modiﬁcation is to avoid false positives that are due to pedes-
trian body parts. However, a drawback to this approach
is that it always disregards one of the overlapping pedes-
trians from detection. Instead of changing the criteria, we
actively modify our training set before each bootstrapping
phase. We include body part images that cause false posi-
tive detection into our bootstrapping image set. Our model
can then learn to suppress such responses within a positive
window and still detect pedestrians within bigger windows
more reliably.

3. Experiments

We evaluate our system on 5 standard pedestrian de-
tection datasets. However, like most other systems, we
only train on the INRIA dataset. We also demonstrate im-
provements brought by unsupervised training and multi-
stage features. In the following we name our model Con-
vNet with variants of unsupervised (Convnet-U) and fully-
supervised training (Convnet-F) and multi-stage features
(Convnet-U-MS and ConvNet-F-MS).

3.1. Data Preparation

The ConvNet

is trained on the INRIA pedestrian
dataset [5]. Pedestrians are extracted into windows of 126
pixels in height and 78 pixels in width. The context ratio
is 1.4, i.e. pedestrians are 90 pixels high and the remaining
36 pixels correspond to the background. Each pedestrian
image is mirrored along the horizontal axis to expand the
dataset. Similarly, we add 5 variations of each original sam-
ple using 5 random deformations such as translations and
scale. Translations range from -2 to 2 pixels and scale ratios
from 0.95 to 1.05. These deformations enforce invariance
to small deformations in the input. The range of each de-
formation determines the trade-off between recognition and
localization accuracy during detection. An equal amount
of background samples are extracted at random from the
negative images and taking approximately 10% of the ex-
tracted samples for validation yields a validation set with
2000 samples and training set with 21845 samples. Note
that the unsupervised training phase is performed on this
initial data before the bootstrapping phase.

3.2. Evaluation Protocol

sampled. The up-sampling ratio is 1.3 while the sub-
sampling ratio is limited by 0.75 times the network’s mini-
mum input (126×78). We use a scale stride of 1.10 between
each scale, while other methods typically use either 1.05 or
1.20 [11]. A higher scale stride is desirable as it implies less
computations.

For evaluation we use the bounding boxes ﬁles published
on the Caltech Pedestrian website 1 and the evaluation soft-
ware provided by Piotr Dollar (version 3.0.1).
In an ef-
fort to provide a more accurate evaluation, we improved
on both the evaluation formula and the INRIA annotations
as follows. The evaluation software was slightly modiﬁed
to compute the continuous area under curve (AUC) in the
entire [0, 1] range rather than from 9 discrete points only
(0.01, 0.0178, 0.0316, 0.0562, 0.1, 0.1778, 0.3162, 0.5623
and 1.0 in version 3.0.1). Instead, we compute the entire
area under the curve by summing the areas under the piece-
wise linear interpolation of the curve, between each pair of
points. In addition, we also report a ’ﬁxed’ version of the
annotations for INRIA dataset, which has missing positive
labels. The added labels are only used to avoid counting
false errors and wrongly penalizing algorithms. The modi-
ﬁed code and extra INRIA labels are available at 2. Table 2
reports results for both original and ﬁxed INRIA datasets.
Notice that the continuous AUC and ﬁxed INRIA annota-
tions both yield a reordering of the results (see supplemen-
tary material for further evidence that the impact of these
modiﬁcations is signiﬁcant enough to be used). To avoid
ambiguity, all results with the original discrete AUC are re-
ported in the supplementary paper.

To ensure a fair comparison, we separated systems
trained on INRIA (the majority) from systems trained on
TUD-MotionPairs and the only system trained on Caltech
in table 2. For clarity, only systems trained on INRIA were
represented in Figure 5, however all results for all systems
are still reported in table 2.

3.3. Results

In Figure 4, we plot DET curves, i.e. miss rate ver-
sus false positives per image (FPPI), on the ﬁxed INRIA
dataset and rank algorithms along two measures:
the er-
ror rate at 1 FPPI and the area under curve (AUC) rate
in the [0, 1] FPPI range. This graph shows the indi-

During testing and bootstrapping phases using the IN-
the images are both up-sampled and sub-

RIA dataset,

1http://www.vision.caltech.edu/Image Datasets/CaltechPedestrians
2http://cs.nyu.edu/∼sermanet/data.html#inria

3628
3628
3630

t

e
a
r
 
s
s
m

i

1
.80
.64
.50
.40

.30

.20

.10

.05

 

10−2

 

66.39% PoseInv
66.10% Shapelet
58.99% VJ
42.81% FtrMine
31.77% HOG
28.47% HikSvm
27.80% Pls
27.18% LatSvm−V1
24.79% HogLbp
23.39% ConvNet−F
22.62% MultiFtr
17.32% FeatSynth
17.29% ConvNet−F−MS
17.10% ConvNet−U
16.57% MLS
14.79% MultiFtr+CSS
11.68% FPDW
11.58% ChnFtrs
10.69% CrossTalk
10.55% ConvNet−U−MS
9.90% LatSvm−V2
9.13% VeryFast

101

102

t

e
a
r
 
s
s
m

i

1

.80

.64

.50

.40

.30

.20

.10

.05

 

10−2

 

23.39% ConvNet−F
17.29% ConvNet−F−MS
17.10% ConvNet−U
10.55% ConvNet−U−MS

101

102

10−1

100

false positives per image

10−1

100

false positives per image

Figure 4: DET curves on the ﬁxed-INRIA dataset for large pedestrians measure report false positives per image (FPPI) against miss
rate. Algorithms are sorted from top to bottom using the proposed continuous area under curve measure between 0 and 1 FPPI. On the
right, only the ConvNet variants are displayed to highlight the individual contributions of unsupervised learning (ConvNet-U) and
multi-stage features learning (ConvNet-F-MS) and their combination (ConvNet-U-MS) compared to the fully-supervised system
without multi-stage features (ConvNet-F).

vidual contributions of unsupervised learning (ConvNet-
U) and multi-stage features learning (ConvNet-F-MS) and
their combination (ConvNet-U-MS) compared to the fully-
supervised system without multi-stage features (ConvNet-
F). With 17.1% error rate, unsupervised learning exhibits
the most improvements compared to the baseline ConvNet-
F (23.39%). Multi-stage features without unsupervised
learning reach 17.29% error while their combination yields
the competitive error rate of 10.55%.

Extensive results comparison of all major pedestrian
datasets and published systems is provided in Table 2. Mul-
tiple types of measures proposed by [10] are reported. For
clarity, we also plot in Figure 5 two of these measures,
’reasonable’ and ’large’, for INRIA-trained systems. The
’large’ plot shows that the ConvNet results in state-of-the-
art performance with some margin on the ETH, Caltech and
TudBrussels datasets and is closely behind LatSvm-V2 and
VeryFast for INRIA and Daimler datasets. In the ’reason-
able’ plot, the ConvNet yields competitive results for IN-
RIA, Daimler and ETH datasets but performs poorly on the
Caltech dataset. We suspect the ConvNet with multi-stage
features trained at high-resolution is more sensitive to reso-
lution loss than other methods. In future work, a ConvNet
trained at multiple resolution will likely learn to use appro-
priate cues for each resolution regime.

models where the low-level features are hand-designed, our
model learns all the features at all levels in a hierarchy. We
used the method of [20] as a baseline, and extended it by
combining high and low resolution features in the model,
and by learning features on the color channels of the in-
put. Using the INRIA dataset, we have shown that these
improvements provide clear performance beneﬁts. The re-
sulting model provides state of the art or competitive re-
sults on most measures of all publicly available datasets.
Small-scale pedestrian measures can be improved in future
work by training multiple scale models relying less on high-
resolution details. While computational speed was not the
focus and hence was not reported here, our model was suc-
cessfully used with near real-time speed in a haptic belt sys-
tem [22] using parallel hardware. In future work, models
designed for speed combined to highly optimized parallel
computing on graphics cards is expected to yield competi-
tive computational performance.
References

[1] M. Aharon, M. Elad, and A. M. Bruckstein. K-SVD and its
non-negative variant for dictionary design. In M. Papadakis,
A. F. Laine, and M. A. Unser, editors, Society of Photo-Optical
Instrumentation Engineers (SPIE) Conference Series, volume
5914, pages 327–339, Aug. 2005. 2

4. Discussion

[2] A. Beck and M. Teboulle.

iterative shrinkage-
thresholding algorithm for linear inverse problems. SIAM J.
Img. Sci., 2(1):183–202, 2009. 2, 3

A fast

We have introduced a new feature learning model with
an application to pedestrian detection. Contrary to popular

[3] R. Benenson, M. Mathias, R. Timofte, and L. Van Gool.
Pedestrian detection at 100 frames per second. In Computer

3629
3629
3631

Pedestrian Detection AUC, Reasonable (> 50 pixels  no/partial occlusion)

Pedestrian Detection AUC, Large (> 100 pixels)

)

%

(
 
I
P
P
F
1

 

 

d
n
a
 
0

 

n
e
e
w
t
e
b

 
e
v
r
u
C

 
r
e
d
n
U
 
a
e
r
A

 100

 80

 60

 40

 20

 0

ConvNet
ChnFtrs
CrossTalk
FPDW
FeatSynth
FtrMine
HOG
HikSvm
HogLbp
LatSvm-V1
LatSvm-V2
MLS
MultiFtr
Pls
PoseInv
Shapelet
VJ
VeryFast

)

%

(
 
I
P
P
F
1

 

 

d
n
a
 
0

 

n
e
e
w
t
e
b

 
e
v
r
u
C

 
r
e
d
n
U
 
a
e
r
A

 100

 80

 60

 40

 20

 0

ConvNet
ChnFtrs
CrossTalk
FPDW
FeatSynth
FtrMine
HOG
HikSvm
HogLbp
LatSvm-V1
LatSvm-V2
MLS
MultiFtr
Pls
PoseInv
Shapelet
VJ
VeryFast

INRIA-fixed

INRIA

Daimler

ETH

Caltech-UsaTest TudBrussels

INRIA-fixed

INRIA

Daimler

ETH

Caltech-UsaTest TudBrussels

Datasets

Datasets

Figure 5: Reasonable and Large measures for all INRIA-trained systems on all major datasets, using the proposed continuous
AUC percentage. The AUC is computed from DET curves (smaller AUC means more accuracy and less false positives). For a clearer
overall performance, each ConvNet point is connected by dotted lines. While only the ’reasonable’ and ’large’ measures are plotted here,
all measures are reported in table 2. The ConvNet system yields state-of-the-art or competitive results on most datasets and measures,
except for the low resolutions measures on the Caltech dataset because of higher reliance on high-resolution cues than other methods.

ConvNet ChnFtrs CrossTalk FPDW FeatSynth FtrMine HOG HikSvm HogLbp LatSvm-V1 LatSvm-V2 MLS MultiFtr Pls PoseInv Shapelet VJ VeryFast

INRIA

Trained on

INRIA-ﬁxed

INRIA
Daimler

ETH

Caltech-UsaTest

TudBrussels

INRIA-ﬁxed

INRIA
Daimler

ETH

Caltech-UsaTest

TudBrussels

INRIA-ﬁxed

INRIA
Daimler

ETH

Caltech-UsaTest

TudBrussels

INRIA-ﬁxed

INRIA
Daimler

ETH

Caltech-UsaTest

TudBrussels

INRIA-ﬁxed

INRIA
Daimler

ETH

Caltech-UsaTest

TudBrussels

12.0
12.7
58.6
47.1
90.9
66.8

12.0
12.7
24.9
38.9
71.5
59.1

10.5
11.2
7.8
24.4
14.8
33.5

11.3
11.9
10.0
28.9
27.3
40.4

33.1
33.1
54.2
55.4
92.2
67.8

Reasonable - AUC % - >50 pixels & no/partial occlusion

13.3
13.9

-

48.7
77.1
57.6

12.7
12.9

-

43.8
77.8
55.0

13.6
14.0

-

51.5
78.1
59.0

13.3
13.9

-

44.2
46.4
48.8

11.6
12.2

-

30.2
24.1
36.2

12.7
12.9

-

39.1
46.0
47.0

10.7
11.0

-

28.2
25.8
37.3

13.6
14.0

-

46.8
46.9
50.4

11.7
12.1

-

33.4
26.4
35.0

19.0
19.6

-
-

78.1

-

19.0
19.6

-
-

-

49.2

17.3
18.0

-
-

-

43.8
44.8

86.7

43.8
44.8

-
-

-

66.3

42.8
43.9

-
-

-

-
-

-

28.6

47.8

32.9
34.3
67.9
54.9
85.5
73.6

All - AUC %
26.2
29.9
28.0
31.4
69.8
62.4
51.1
61.6
87.9
86.8
76.4
77.2

26.2
28.0
40.3
43.7
62.2
71.8

29.9
31.4
38.9
59.2
62.0
72.4

32.9
34.3
46.2
51.1
57.8
68.1
Large - AUC % - >100 pixels
31.8
33.2
31.7
33.1
28.0
56.2

28.5
30.0
25.2
36.4
26.5
52.2

24.8
26.6
11.8
29.5
18.4
46.3

Near - AUC % - >80 pixels

28.8
29.8
64.2
69.1
91.7
85.7

28.8
29.8
47.2
66.6
73.4
84.0

27.2
28.2
22.9
47.6
40.7
64.5

11.6
12.2

-

35.2
27.4
39.5

100.0
100.0

-

42.9
69.5
57.4

11.0
11.2

-

30.9
28.9
40.3

99.7
99.7

-

42.1
70.6
55.5

11.9
12.3

-

37.5
28.4
38.8

17.3
17.9

-
-

29.5

-

42.6
43.7

-
-

48.9

-

100.0
100.0

-

45.4
70.6
59.7

100.0
100.0

-
-

70.2

100.0
100.0

-
-

82.1

-

-

28.5
30.0
30.4
45.6
34.3
58.7

24.7
26.5
10.9
31.7
24.7
50.5

31.5
32.9
36.8
40.5
33.1
61.1
Medium - AUC % - 30-80 pixels
100.0
100.0
62.1
49.9
81.4
71.4

100.0
100.0
54.4
54.7
82.6
74.9

85.3
85.3
70.7
61.2
91.5
82.9

27.5
28.5
27.6
52.2
47.2
70.9

85.3
85.3
58.5
71.5
91.1
85.5

12.9
13.3
62.3
49.3
84.2
67.2

12.9
13.3
29.2
41.1
56.0
59.6

9.9
10.3
6.9
26.8
22.5
43.1

11.1
11.5
10.8
31.4
26.7
47.1

99.7
99.7
60.0
57.3
80.8
68.2

17.8
18.2
51.8
42.8
83.4
59.2

17.8
18.2
18.3
37.1
51.9
52.0

16.6
17.0
13.9
24.8
22.7
41.8

16.5
16.8
14.7
29.5
29.1
45.3

100.0
100.0
44.7
43.9
80.6
59.1

24.2
25.3
68.8
51.7
83.4
70.5

24.2
25.3
45.5
47.5
59.3
64.8

22.6
23.7
30.9
35.1
34.3
55.5

22.7
23.8
33.7
39.4
40.8
57.2

86.1
86.1
63.2
47.3
77.8
68.7

29.0
30.1

-

47.4
81.2
66.1

66.7
70.1

-

86.5
92.6
83.8

29.0
30.1

-

42.2
52.9
59.1

27.8
28.8

-

26.6
30.4
43.3

66.7
70.1

-

85.2
78.2
80.8

66.4
69.9

-

63.9
54.5
70.0

27.7
28.8

-

34.1
31.2
49.6

66.0
69.4

-

80.6
66.8
80.0

100.0
100.0

-

45.0
75.8
65.0

99.7
99.7

-

73.9
88.8
79.4

66.8
68.7
94.9
85.6
95.4
93.8

66.8
68.7
90.2
83.9
87.0
92.5

66.1
68.1
72.3
75.8
69.6
80.3

66.1
68.1
78.3
79.9
75.7
85.6

99.7
99.7
95.2
74.5
94.7
94.1

59.4
60.7
94.8
84.5
99.1
92.7

59.4
60.7
91.3
83.6
91.8
91.1

59.0
60.3
83.9
76.7
80.9
86.0

58.7
60.0
86.3
80.0
85.3
89.0

91.5
91.5
93.7
71.2
98.7
91.7

10.3
10.5

46.9

-

-
-

10.3
10.5

42.5

-

-
-

9.1
9.4
-

24.4

-
-

9.7
9.9
-

29.8

-
-

27.9
27.9

48.3

-

-
-

Table 2: Performance of all systems on all datasets using the proposed continuous AUC percentage over the range [0,1] from DET
curves. The top performing results (among INRIA-trained models) are highlighted in bold for each row. DET curves plot false positives
per image (FPPI) against miss rate. Hence a smaller AUC% means a more accurate system with lower amount of false positives. The
ConvNet model (ConvNet-U-MS here) holds several state-of-the-art or competitive scores. We report the multiple measures introduced
by [10] for all major pedestrian datasets. For readibility, not all measures are reported nor are models not trained on INRIA. All results
however are reported in the supplementary paper.

Vision and Pattern Recognition (CVPR), 2012 IEEE Confer-
ence on, pages 2903–2910. IEEE, 2012. 1

[4] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle.
In Advances

Greedy layer-wise training of deep networks.

3630
3630
3632

in Neural Information Processing Systems 19, pages 153–160.
MIT Press, 2007. 1, 2

[5] N. Dalal and B. Triggs. Histograms of oriented gradients for
human detection.
In C. Schmid, S. Soatto, and C. Tomasi,
editors, CVPR’05, volume 2, pages 886–893, June 2005. 1, 2,
5

[6] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresh-
olding algorithm for linear inverse problems with a sparsity
constraint. Communications on Pure and Applied Mathemat-
ics, 57(11):1413–1457, 2004. 2, 3

[7] P. Doll´ar, R. Appel, and W. Kienzle. Crosstalk cascades for

frame-rate pedestrian detection. 1

[8] P. Doll´ar, S. Belongie, and P. Perona. The fastest pedestrian

detector in the west. In BMVC 2010, Aberystwyth, UK. 1, 2

[9] P. Doll´ar, Z. Tu, P. Perona, and S. Belongie. Integral channel

features. In BMVC 2009, London, England. 1, 2

[10] P. Doll´ar, C. Wojek, B. Schiele, and P. Perona. Pedestrian
detection: A benchmark. In CVPR’09. IEEE, June 2009. 4, 6,
7

[11] P. Doll´ar, C. Wojek, B. Schiele, and P. Perona. Pedestrian
detection: An evaluation of the state of the art. PAMI, 99,
2011. 5

[12] J. Fan, W. Xu, Y. Wu, and Y. Gong. Human tracking using
convolutional neural networks. Neural Networks, IEEE Trans-
actions on, 21(10):1610 –1623, 2010. 4

[13] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-
manan. Object detection with discriminatively trained part
based models. In PAMI 2010. 1

[14] A. Frome, G. Cheung, A. Abdulkader, M. Zennaro, B. Wu,
A. Bissacco, H. Adam, H. Neven, and L. Vincent. Large-scale
privacy protection in street-level imagery. In ICCV’09. 1, 2

[15] C. Garcia and M. Delakis. Convolutional face ﬁnder: A neu-
ral architecture for fast and robust face detection. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 2004.
1, 2

[16] G. E. Hinton and R. R. Salakhutdinov.

dimensionality of data with neural networks.
313(5786):504–507, 2006. 1, 2

Reducing the
Science,

[17] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun.
What is the best multi-stage architecture for object recogni-
tion? In ICCV’09. IEEE, 2009. 1, 4

[18] K. Kavukcuoglu, M. Ranzato, R. Fergus, and Y. LeCun.
Learning invariant features through topographic ﬁlter maps. In
CVPR’09. IEEE, 2009. 2

[19] K. Kavukcuoglu, M. Ranzato, and Y. LeCun. Fast inference
in sparse coding algorithms with applications to object recog-
nition. Technical report, CBLL, Courant Institute, NYU, 2008.
CBLL-TR-2008-12-01. 2

[20] K. Kavukcuoglu, P. Sermanet, Y. Boureau, K. Gregor,
M. Mathieu, and Y. LeCun. Learning convolutional feature
hierachies for visual recognition. In Advances in Neural Infor-
mation Processing Systems (NIPS 2010), 2010. 1, 2, 3, 6

[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-
In NIPS

siﬁcation with deep convolutional neural networks.
2012: Neural Information Processing Systems. 1

[22] Q. Le, M. Quigley, J. Feng, J. Chen, Y. Zou, W, M. Rasi,
T. Low, and A. Ng. Haptic belt with pedestrian detection. In
NIPS, 2011 (Demonstrations). 6

3631
3631
3633

[23] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceedings
of the IEEE, 86(11):2278–2324, November 1998. 1, 2, 4

[24] H. Lee, A. Battle, R. Raina, and A. Y. Ng. Efﬁcient sparse
coding algorithms. In B. Sch¨olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing Systems
19, pages 801–808. MIT Press, Cambridge, MA, 2007. 2

[25] H. Lee, R. Grosse, R. Ranganath, and A. Ng. Convolutional
deep belief networks for scalable unsupervised learning of hi-
erarchical representations. In ICML’09, pages 609–616. ACM,
2009. 2

[26] Y. Li and S. Osher. Coordinate Descent Optimization for
l1 Minimization with Application to Compressed Sensing; a
Greedy Algorithm. CAM Report, pages 09–17. 2

[27] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman.
Discriminative learned dictionaries for local image analysis.
Computer Vision and Pattern Recognition, 2008. CVPR 2008.
IEEE Conference on, pages 1–8, June 2008. 2

[28] S. Maji, A. C. Berg, and J. Malik. Classiﬁcation using inter-
section kernel support vector machines is efﬁcient. volume 0,
pages 1–8, Los Alamitos, CA, USA, 2008. IEEE Computer
Society. 1

[29] S. Nowlan and J. Platt. A convolutional neural network hand
tracker. pages 901–908, San Mateo, CA, 1995. Morgan Kauf-
mann. 1, 2

[30] B. A. Olshausen and D. J. Field. Sparse coding with an over-
complete basis set: a strategy employed by v1? Vision Re-
search, 37(23):3311–3325, 1997. 2

[31] M. Osadchy, Y. LeCun, and M. Miller. Synergistic face de-
tection and pose estimation with energy-based models. Journal
of Machine Learning Research, 8:1197–1215, May 2007. 1, 2
[32] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efﬁcient
learning of sparse representations with an energy-based model.
In NIPS’07. MIT Press, 2007. 1, 2

[33] W. R. Schwartz, A. Kembhavi, D. Harwood, and L. S. Davis.
Human detection using partial least squares analysis. In Com-
puter Vision, 2009 IEEE 12th International Conference on,
pages 24 –31, 29 2009-oct. 2 2009. 1

[34] P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neu-
ral networks applied to house numbers digit classiﬁcation. In
Proceedings of International Conference on Pattern Recogni-
tion, 2012. 5

[35] P. Sermanet and Y. LeCun. Trafﬁc sign recognition with
multi-scale convolutional networks. In Proceedings of Inter-
national Joint Conference on Neural Networks, 2011. 5

[36] G. Taylor, R. Fergus, G. Williams, I. Spiro, and C. Bregler.
Pose-sensitive embedding by nonlinear nca regression. In Ad-
vances in Neural Information Processing Systems NIPS 23,
2010. 1, 2

[37] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach
for the localisation of objects in images. IEE Proc on Vision,
Image, and Signal Processing, 141(4):245–250, August 1994.
1, 2

[38] S. Walk, N. Majer, K. Schindler, and B. Schiele. New fea-
tures and insights for pedestrian detection. In CVPR 2010, San
Francisco, California. 1, 2

[39] M. Zeiler, D. Krishnan, G. Taylor, and R. Fergus. Deconvo-

lutional Networks. In CVPR’10. IEEE, 2010. 2

