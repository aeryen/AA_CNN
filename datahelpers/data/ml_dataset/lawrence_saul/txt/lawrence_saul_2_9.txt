Kernel Methods for Deep Learning

Youngmin Cho and Lawrence K. Saul

Department of Computer Science and Engineering

University of California, San Diego
9500 Gilman Drive, Mail Code 0404
{yoc002,saul}@cs.ucsd.edu

La Jolla, CA 92093-0404

Abstract

We introduce a new family of positive-deﬁnite kernel functions that mimic the
computation in large, multilayer neural nets. These kernel functions can be used
in shallow architectures, such as support vector machines (SVMs), or in deep
kernel-based architectures that we call multilayer kernel machines (MKMs). We
evaluate SVMs and MKMs with these kernel functions on problems designed to
illustrate the advantages of deep architectures. On several problems, we obtain
better results than previous, leading benchmarks from both SVMs with Gaussian
kernels as well as deep belief nets.

1

Introduction

Recent work in machine learning has highlighted the circumstances that appear to favor deep archi-
tectures, such as multilayer neural nets, over shallow architectures, such as support vector machines
(SVMs) [1]. Deep architectures learn complex mappings by transforming their inputs through mul-
tiple layers of nonlinear processing [2]. Researchers have advanced several motivations for deep
architectures: the wide range of functions that can be parameterized by composing weakly non-
linear transformations, the appeal of hierarchical distributed representations, and the potential for
combining unsupervised and supervised methods. Experiments have also shown the beneﬁts of
deep learning in several interesting applications [3, 4, 5].
Many issues surround the ongoing debate over deep versus shallow architectures [1, 6]. Deep ar-
chitectures are generally more difﬁcult to train than shallow ones. They involve difﬁcult nonlinear
optimizations and many heuristics. The challenges of deep learning explain the early and continued
appeal of SVMs, which learn nonlinear classiﬁers via the “kernel trick”. Unlike deep architectures,
SVMs are trained by solving a simple problem in quadratic programming. However, SVMs cannot
seemingly beneﬁt from the advantages of deep learning.
Like many, we are intrigued by the successes of deep architectures yet drawn to the elegance of ker-
nel methods. In this paper, we explore the possibility of deep learning in kernel machines. Though
we share a similar motivation as previous authors [7], our approach is very different. Our paper
makes two main contributions. First, we develop a new family of kernel functions that mimic the
computation in large neural nets. Second, using these kernel functions, we show how to train multi-
layer kernel machines (MKMs) that beneﬁt from many advantages of deep learning.
The organization of this paper is as follows.
In section 2, we describe a new family of kernel
functions and experiment with their use in SVMs. Our results on SVMs are interesting in their own
right; they also foreshadow certain trends that we observe (and certain choices that we make) for the
MKMs introduced in section 3. In this section, we describe a kernel-based architecture with multiple
layers of nonlinear transformation. The different layers are trained using a simple combination of
supervised and unsupervised methods. Finally, we conclude in section 4 by evaluating the strengths
and weaknesses of our approach.

1

2 Arc-cosine kernels

In this section, we develop a new family of kernel functions for computing the similarity of vector
inputs x, y ∈ (cid:60)d. As shorthand, let Θ(z) = 1
2(1 + sign(z)) denote the Heaviside step function. We
deﬁne the nth order arc-cosine kernel function via the integral representation:

kn(x, y) = 2

dw e− (cid:107)w(cid:107)2
(2π)d/2

2

Θ(w · x) Θ(w · y) (w · x)n (w · y)n

(1)

(cid:90)

The integral representation makes it straightforward to show that these kernel functions are positive-
semideﬁnite. The kernel function in eq. (1) has interesting connections to neural computation [8]
that we explore further in sections 2.2–2.3. However, we begin by elucidating its basic properties.

2.1 Basic properties

We show how to evaluate the integral in eq. (1) analytically in the appendix. The ﬁnal result is most
easily expressed in terms of the angle θ between the inputs:

(cid:19)

(cid:18) x · y

(cid:107)x(cid:107)(cid:107)y(cid:107)

θ = cos−1

.

(2)

The integral in eq. (1) has a simple, trivial dependence on the magnitudes of the inputs x and y, but
a complex, interesting dependence on the angle between them. In particular, we can write:

kn(x, y) =

(cid:107)x(cid:107)n(cid:107)y(cid:107)nJn(θ)

1
π

(3)

where all the angular dependence is captured by the family of functions Jn(θ). Evaluating the
integral in the appendix, we show that this angular dependence is given by:

Jn(θ) = (−1)n(sin θ)2n+1

.

(4)

(cid:18) 1

∂
∂θ

sin θ

(cid:19)n(cid:18) π − θ

(cid:19)

sin θ

For n = 0, this expression reduces to the supplement of the angle between the inputs. However, for
n >0, the angular dependence is more complicated. The ﬁrst few expressions are:

J0(θ) = π − θ
J1(θ) = sin θ + (π − θ) cos θ
J2(θ) = 3 sin θ cos θ + (π − θ)(1 + 2 cos2 θ)

(5)
(6)
(7)

π cos−1 x·y

We describe eq. (3) as an arc-cosine kernel because for n = 0,
it takes the simple form
k0(x, y) = 1− 1
(cid:107)x(cid:107)(cid:107)y(cid:107). In fact, the zeroth and ﬁrst order kernels in this family are strongly
motivated by previous work in neural computation. We explore these connections in the next section.
Arc-cosine kernels have other intriguing properties. From the magnitude dependence in eq. (3),
we observe the following: (i) the n = 0 arc-cosine kernel maps inputs x to the unit hypersphere
in feature space, with k0(x, x) = 1; (ii) the n = 1 arc-cosine kernel preserves the norm of inputs,
with k1(x, x) = (cid:107)x(cid:107)2; (iii) higher order (n >1) arc-cosine kernels expand the dynamic range of the
inputs, with kn(x, x) ∼ (cid:107)x(cid:107)2n. Properties (i)–(iii) are shared respectively by radial basis function
(RBF), linear, and polynomial kernels. Interestingly, though, the n = 1 arc-cosine kernel is highly
nonlinear, also satisfying k1(x,−x) = 0 for all inputs x. As a practical matter, we note that arc-
cosine kernels do not have any continuous tuning parameters (such as the kernel width in RBF
kernels), which can be laborious to set by cross-validation.

2.2 Computation in single-layer threshold networks

Consider the single-layer network shown in Fig. 1 (left) whose weights Wij connect the jth input
unit to the ith output unit. The network maps inputs x to outputs f(x) by applying an elementwise
nonlinearity to the matrix-vector product of the inputs and the weight matrix: f(x) = g(Wx). The
nonlinearity is described by the network’s so-called activation function. Here we consider the family
of one-sided polynomial activation functions gn(z) = Θ(z)zn illustrated in the right panel of Fig. 1.

2

Figure 1: Single layer network and activation functions

m(cid:88)

For n = 0, the activation function is a step function, and the network is an array of perceptrons. For
n = 1, the activation function is a ramp function (or rectiﬁcation nonlinearity [9]), and the mapping
f(x) is piecewise linear. More generally, the nonlinear (non-polynomial) behavior of these networks
is induced by thresholding on weighted sums. We refer to networks with these activation functions
as single-layer threshold networks of degree n.
Computation in these networks is closely connected to computation with the arc-cosine kernel func-
tion in eq. (1). To see the connection, consider how inner products are transformed by the mapping
in single-layer threshold networks. As notation, let the vector wi denote ith row of the weight
matrix W. Then we can express the inner product between different outputs of the network as:

f(x) · f(y) =

Θ(wi · x)Θ(wi · y)(wi · x)n(wi · y)n,

(8)

i=1

where m is the number of output units. The connection with the arc-cosine kernel function emerges
in the limit of very large networks [10, 8].
Imagine that the network has an inﬁnite number of
output units, and that the weights Wij are Gaussian distributed with zero mean and unit vari-
In this limit, we see that eq. (8) reduces to eq. (1) up to a trivial multiplicative factor:
ance.
m f(x) · f(y) = kn(x, y). Thus the arc-cosine kernel function in eq. (1) can be viewed
limm→∞ 2
as the inner product between feature vectors derived from the mapping of an inﬁnite single-layer
threshold network [8].
Many researchers have noted the general connection between kernel machines and neural networks
with one layer of hidden units [1]. The n = 0 arc-cosine kernel in eq. (1) can also be derived from
an earlier result obtained in the context of Gaussian processes [8]. However, we are unaware of any
previous theoretical or empirical work on the general family of these kernels for degrees n≥0.
Arc-cosine kernels differ from polynomial and RBF kernels in one especially interesting respect.
As highlighted by the integral representation in eq. (1), arc-cosine kernels induce feature spaces
that mimic the sparse, nonnegative, distributed representations of single-layer threshold networks.
Polynomial and RBF kernels do not encode their inputs in this way. In particular, the feature vector
induced by polynomial kernels is neither sparse nor nonnegative, while the feature vector induced
by RBF kernels resembles the localized output of a soft vector quantizer. Further implications of
this difference are explored in the next section.

2.3 Computation in multilayer threshold networks

A kernel function can be viewed as inducing a nonlinear mapping from inputs x to fea-
ture vectors Φ(x).
in the induced feature space:
k(x, y) = Φ(x)·Φ(y). In this section, we consider how to compose the nonlinear mappings in-
duced by kernel functions. Speciﬁcally, we show how to derive new kernel functions

The kernel computes the inner product

k((cid:96))(x, y) = Φ(Φ(...Φ

(y)))

(9)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:96) times

(cid:123)(cid:122)

(x))) · Φ(Φ(...Φ
(cid:96) times

(cid:125)

(cid:124)

which compute the inner product after (cid:96) successive applications of the nonlinear mapping Φ(·). Our
motivation is the following: intuitively, if the base kernel function k(x, y) = Φ(x) · Φ(y) mimics
the computation in a single-layer network, then the iterated mapping in eq. (9) should mimic the
computation in a multilayer network.

3

f2f3fix1x2xj. . . . . . f1fmxdW. . . . . . −10100.51Step (n=0)−10100.51Ramp (n=1)−10100.51Quarter−pipe (n=2)Figure 2: Left: examples from the rectangles-image data set. Right: classiﬁcation error rates on the
test set. SVMs with arc-cosine kernels have error rates from 22.36–25.64%. Results are shown for
kernels of varying degree (n) and levels of recursion ((cid:96)). The best previous results are 24.04% for
SVMs with RBF kernels and 22.50% for deep belief nets [11]. See text for details.

We ﬁrst examine the results of this procedure for widely used kernels. Here we ﬁnd that the iterated
mapping in eq. (9) does not yield particularly interesting results. Consider the two-fold composition
that maps x to Φ(Φ(x)). For linear kernels k(x, y) = x · y, the composition is trivial: we obtain
the identity map Φ(Φ(x)) = Φ(x) = x. For homogeneous polynomial kernels k(x, y) = (x · y)d,
the composition yields:

Φ(Φ(x)) · Φ(Φ(y)) = (Φ(x) · Φ(y))d = ((x · y)d)d = (x · y)d2

.

(10)

The above result is not especially interesting: the kernel implied by this composition is also polyno-
mial, just of higher degree (d2 versus d) than the one from which it was constructed. Likewise, for
RBF kernels k(x, y) = e−λ(cid:107)x−y(cid:107)2, the composition yields:

Φ(Φ(x)) · Φ(Φ(y)) = e−λ(cid:107)Φ(x)−Φ(y)(cid:107)2 = e−2λ(1−k(x,y)).

(11)

Though non-trivial, eq. (11) does not represent a particularly interesting computation. Recall that
RBF kernels mimic the computation of soft vector quantizers, with k(x, y) (cid:28) 1 when (cid:107)x−y(cid:107) is
large compared to the kernel width. It is hard to see how the iterated mapping Φ(Φ(x)) would
generate a qualitatively different representation than the original mapping Φ(x).
Next we consider the (cid:96)-fold composition in eq. (9) for arc-cosine kernel functions. We state the
result in the form of a recursion. The base case is given by eq. (3) for kernels of depth (cid:96) = 1 and
degree n. The inductive step is given by:

k(l+1)
n

(x, y) =

1
π

n (x, x) k(l)
k(l)

n (y, y)

Jn

θ((cid:96))
n

,

(12)

where θ((cid:96))
composition. In particular, we can write:

n is the angle between the images of x and y in the feature space induced by the (cid:96)-fold

n = cos−1
θ((cid:96))

n (x, y)
k((cid:96))

n (x, x) k((cid:96))
k((cid:96))

n (y, y)

.

(13)

The recursion in eq. (12) is simple to compute in practice. The resulting kernels mimic the com-
putations in large multilayer threshold networks. Above, for simplicity, we have assumed that the
arc-cosine kernels have the same degree n at every level (or layer) (cid:96) of the recursion. We can also
use kernels of different degrees at different layers. In the next section, we experiment with SVMs
whose kernel functions are constructed in this way.

2.4 Experiments on binary classiﬁcation
We evaluated SVMs with arc-cosine kernels on two challenging data sets of 28 × 28 grayscale pixel
images. These data sets were speciﬁcally constructed to compare deep architectures and kernel
machines [11]. In the ﬁrst data set, known as rectangles-image, each image contains an occluding
rectangle, and the task is to determine whether the width of the rectangle exceeds its height; ex-
amples are shown in Fig. 2 (left). In the second data set, known as convex, each image contains a
white region, and the task is to determine whether the white region is convex; examples are shown

4

(cid:104)

(cid:18)

(cid:104)

(cid:105)n/2

(cid:16)

(cid:17)

(cid:105)−1/2(cid:19)

222426DBN−3SVM−RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter−pipe (n=2)222426DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure2:Left:examplesfromtherectangles-imagedataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom22.36–25.64%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare24.04%forSVMswithRBFkernelsand22.50%fordeepbeliefnets[2].Seetextfordetails.1718192021DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure3:Left:examplesfromtheconvexdataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom17.15–20.51%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare19.13%forSVMswithRBFkernelsand18.63%fordeepbeliefnets[2].Seetextfordetails.2000trainingexamplesasavalidationsettochoosethemarginpenaltyparameter;afterchoosingthisparameterbycross-validation,wethenretrainedeachSVMusingallthetrainingexamples.Forreference,wealsoreportthebestresultsobtainedpreviouslyfromthreelayerdeepbeliefnets(DBN-3)andSVMswithRBFkernels(SVM-RBF).Thesereferencesarerepresentativeofthecurrentstate-of-the-artfordeepandshallowarchitecturesonthesedatasets.Therightpanelsofﬁgures2and3showthetestseterrorratesfromarccosinekernelsofvaryingdegree(n)andlevelsofrecursion(!).Weexperimentedwithkernelsofdegreen=0,1and2,correspondingtosinglelayerthresholdnetworkswith“step”,“ramp”,and“quarter-pipe”activationfunctions.Wealsoexperimentedwiththemultilayerkernelsdescribedinsection2.3,composedfromonetosixlevelsofrecursion.Overall,theﬁguresshowthatonthesetwodatasets,manydifferentarccosinekernelsoutperformthebestresultspreviouslyreportedforSVMswithRBFkernelsanddeepbeliefnets.Wegivemoredetailsontheseexperimentsbelow.Atahighlevel,though,wenotethatSVMswitharccosinekernelsareverystraightforwardtotrain;unlikeSVMswithRBFkernels,theydonotrequiretuningakernelwidthparameter,andunlikedeepbeliefnets,theydonotrequiresolvingadifﬁcultnonlinearoptimizationorsearchingoverpossiblearchitectures.Inourexperiments,wequicklydiscoveredthatthemultilayerkernelsonlyperformedwellwhenn=1kernelswereusedathigher(!>1)levelsintherecursion.Figs.2and3thereforeshowonlythesesetsofresults;inparticular,eachgroupofbarsshowsthetesterrorrateswhenaparticularkernel(ofdegreen=0,1,2)wasusedattheﬁrstlayerofnonlinearity,whilethen=1kernelwasusedatsuccessivelayers.Wedonothaveaformalexplanationforthiseffect.However,recallthatonlythen=1arccosinekernelpreservesthenormofitsinputs:then=0kernelmapsallinputsontoaunithypersphereinfeaturespace,whilehigher-order(n>1)kernelsmayinducefeaturespaceswithseverelydistorteddynamicranges.Therefore,wehypothesizethatonlyn=1arccosinekernelspreservesufﬁcientinformationaboutthemagnitudeoftheirinputstoworkeffectivelyincompositionwithotherkernels.Finally,theresultsonbothdatasetsrevealaninterestingtrend:themultilayerarccosinekernelsoftenperformbetterthantheirsinglelayercounterparts.ThoughSVMsareshallowarchitectures,5Figure 3: Left: examples from the convex data set. Right: classiﬁcation error rates on the test set.
SVMs with arc-cosine kernels have error rates from 17.15–20.51%. Results are shown for kernels
of varying degree (n) and levels of recursion ((cid:96)). The best previous results are 19.13% for SVMs
with RBF kernels and 18.63% for deep belief nets [11]. See text for details.

in Fig. 3 (left). The rectangles-image data set has 12000 training examples, while the convex data
set has 8000 training examples; both data sets have 50000 test examples. These data sets have
been extensively benchmarked by previous authors [11]. Our experiments in binary classiﬁcation
focused on these data sets because in previously reported benchmarks, they exhibited the biggest
performance gap between deep architectures (e.g., deep belief nets) and traditional SVMs.
We followed the same experimental methodology as previous authors [11]. SVMs were trained using
libSVM (version 2.88) [12], a publicly available software package. For each SVM, we used the last
2000 training examples as a validation set to choose the margin penalty parameter; after choosing
this parameter by cross-validation, we then retrained each SVM using all the training examples.
For reference, we also report the best results obtained previously from three-layer deep belief nets
(DBN-3) and SVMs with RBF kernels (SVM-RBF). These references appear to be representative of
the current state-of-the-art for deep and shallow architectures on these data sets.
Figures 2 and 3 show the test set error rates from arc-cosine kernels of varying degree (n) and levels
of recursion ((cid:96)). We experimented with kernels of degree n = 0, 1 and 2, corresponding to thresh-
old networks with “step”, “ramp”, and “quarter-pipe” activation functions. We also experimented
with the multilayer kernels described in section 2.3, composed from one to six levels of recursion.
Overall, the ﬁgures show that many SVMs with arc-cosine kernels outperform traditional SVMs,
and a certain number also outperform deep belief nets. In addition to their solid performance, we
note that SVMs with arc-cosine kernels are very straightforward to train; unlike SVMs with RBF
kernels, they do not require tuning a kernel width parameter, and unlike deep belief nets, they do not
require solving a difﬁcult nonlinear optimization or searching over possible architectures.
Our experiments with multilayer kernels revealed that these SVMs only performed well when arc-
cosine kernels of degree n = 1 were used at higher ((cid:96) > 1) levels in the recursion. Figs. 2 and
3 therefore show only these sets of results; in particular, each group of bars shows the test error
rates when a particular kernel (of degree n = 0, 1, 2) was used at the ﬁrst layer of nonlinearity,
while the n = 1 kernel was used at successive layers. We hypothesize that only n = 1 arc-cosine
kernels preserve sufﬁcient information about the magnitude of their inputs to work effectively in
composition with other kernels. Recall that only the n = 1 arc-cosine kernel preserves the norm of
its inputs: the n = 0 kernel maps all inputs onto a unit hypersphere in feature space, while higher-
order (n >1) kernels induce feature spaces with different dynamic ranges.
Finally, the results on both data sets reveal an interesting trend: the multilayer arc-cosine kernels
often perform better than their single-layer counterparts. Though SVMs are (inherently) shallow
architectures, this trend suggests that for these problems in binary classiﬁcation, arc-cosine kernels
may be yielding some of the advantages typically associated with deep architectures.

3 Deep learning

In this section, we explore how to use kernel methods in deep architectures [7]. We show how to train
deep kernel-based architectures by a simple combination of supervised and unsupervised methods.
Using the arc-cosine kernels in the previous section, these multilayer kernel machines (MKMs)
perform very competitively on multiclass data sets designed to foil shallow architectures [11].

5

222426DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure2:Left:examplesfromtherectangles-imagedataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom22.36–25.64%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare24.04%forSVMswithRBFkernelsand22.50%fordeepbeliefnets[2].Seetextfordetails.1718192021DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure3:Left:examplesfromtheconvexdataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom17.15–20.51%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare19.13%forSVMswithRBFkernelsand18.63%fordeepbeliefnets[2].Seetextfordetails.2000trainingexamplesasavalidationsettochoosethemarginpenaltyparameter;afterchoosingthisparameterbycross-validation,wethenretrainedeachSVMusingallthetrainingexamples.Forreference,wealsoreportthebestresultsobtainedpreviouslyfromthreelayerdeepbeliefnets(DBN-3)andSVMswithRBFkernels(SVM-RBF).Thesereferencesarerepresentativeofthecurrentstate-of-the-artfordeepandshallowarchitecturesonthesedatasets.Therightpanelsofﬁgures2and3showthetestseterrorratesfromarccosinekernelsofvaryingdegree(n)andlevelsofrecursion(!).Weexperimentedwithkernelsofdegreen=0,1and2,correspondingtosinglelayerthresholdnetworkswith“step”,“ramp”,and“quarter-pipe”activationfunctions.Wealsoexperimentedwiththemultilayerkernelsdescribedinsection2.3,composedfromonetosixlevelsofrecursion.Overall,theﬁguresshowthatonthesetwodatasets,manydifferentarccosinekernelsoutperformthebestresultspreviouslyreportedforSVMswithRBFkernelsanddeepbeliefnets.Wegivemoredetailsontheseexperimentsbelow.Atahighlevel,though,wenotethatSVMswitharccosinekernelsareverystraightforwardtotrain;unlikeSVMswithRBFkernels,theydonotrequiretuningakernelwidthparameter,andunlikedeepbeliefnets,theydonotrequiresolvingadifﬁcultnonlinearoptimizationorsearchingoverpossiblearchitectures.Inourexperiments,wequicklydiscoveredthatthemultilayerkernelsonlyperformedwellwhenn=1kernelswereusedathigher(!>1)levelsintherecursion.Figs.2and3thereforeshowonlythesesetsofresults;inparticular,eachgroupofbarsshowsthetesterrorrateswhenaparticularkernel(ofdegreen=0,1,2)wasusedattheﬁrstlayerofnonlinearity,whilethen=1kernelwasusedatsuccessivelayers.Wedonothaveaformalexplanationforthiseffect.However,recallthatonlythen=1arccosinekernelpreservesthenormofitsinputs:then=0kernelmapsallinputsontoaunithypersphereinfeaturespace,whilehigher-order(n>1)kernelsmayinducefeaturespaceswithseverelydistorteddynamicranges.Therefore,wehypothesizethatonlyn=1arccosinekernelspreservesufﬁcientinformationaboutthemagnitudeoftheirinputstoworkeffectivelyincompositionwithotherkernels.Finally,theresultsonbothdatasetsrevealaninterestingtrend:themultilayerarccosinekernelsoftenperformbetterthantheirsinglelayercounterparts.ThoughSVMsareshallowarchitectures,51718192021DBN−3SVM−RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter−pipe (n=2)3.1 Multilayer kernel machines

We explored how to train MKMs in stages that involve kernel PCA [13] and feature selection [14] at
intermediate hidden layers and large-margin nearest neighbor classiﬁcation [15] at the ﬁnal output
layer. Speciﬁcally, for (cid:96)-layer MKMs, we considered the following training procedure:

1. Prune uninformative features from the input space.
2. Repeat (cid:96) times:

(a) Compute principal components in the feature space induced by a nonlinear kernel.
(b) Prune uninformative components from the feature space.

3. Learn a Mahalanobis distance metric for nearest neighbor classiﬁcation.

The individual steps in this procedure are well-established methods; only their combination is new.
While many other approaches are worth investigating, our positive results from the above procedure
provide a ﬁrst proof-of-concept. We discuss each of these steps in greater detail below.
Kernel PCA. Deep learning in MKMs is achieved by iterative applications of kernel PCA [13]. This
use of kernel PCA was suggested over a decade ago [16] and more recently inspired by the pre-
training of deep belief nets by unsupervised methods. In MKMs, the outputs (or features) from
kernel PCA at one layer are the inputs to kernel PCA at the next layer. However, we do not strictly
transmit each layer’s top principal components to the next layer; some components are discarded if
they are deemed uninformative. While any nonlinear kernel can be used for the layerwise PCA in
MKMs, arc-cosine kernels are natural choices to mimic the computations in large neural nets.
Feature selection. The layers in MKMs are trained by interleaving a supervised method for feature
selection with the unsupervised method of kernel PCA. The feature selection is used to prune away
uninformative features at each layer in the MKM (including the zeroth layer which stores the raw
inputs).
Intuitively, this feature selection helps to focus the unsupervised learning in MKMs on
statistics of the inputs that actually contain information about the class labels. We prune features
at each layer by a simple two-step procedure that ﬁrst ranks them by estimates of their mutual
information, then truncates them using cross-validation. More speciﬁcally, in the ﬁrst step, we
discretize each real-valued feature and construct class-conditional and marginal histograms of its
discretized values; then, using these histograms, we estimate each feature’s mutual information with
the class label and sort the features in order of these estimates [14]. In the second step, considering
only the ﬁrst w features in this ordering, we compute the error rates of a basic kNN classiﬁer using
Euclidean distances in feature space. We compute these error rates on a held-out set of validation
examples for many values of k and w and record the optimal values for each layer. The optimal w
determines the number of informative features passed onto the next layer; this is essentially the
width of the layer. In practice, we varied k from 1 to 15 and w from 10 to 300; though exhaustive,
this cross-validation can be done quickly and efﬁciently by careful bookkeeping. Note that this
procedure determines the architecture of the network in a greedy, layer-by-layer fashion.
Distance metric learning. Test examples in MKMs are classiﬁed by a variant of kNN classiﬁcation
on the outputs of the ﬁnal layer. Speciﬁcally, we use large margin nearest neighbor (LMNN) clas-
siﬁcation [15] to learn a Mahalanobis distance metric for these outputs, though other methods are
equally viable [17]. The use of LMNN is inspired by the supervised ﬁne-tuning of weights in the
training of deep architectures [18]. In MKMs, however, this supervised training only occurs at the
ﬁnal layer (which underscores the importance of feature selection in earlier layers). LMNN learns a
distance metric by solving a problem in semideﬁnite programming; one advantage of LMNN is that
the required optimization is convex. Test examples are classiﬁed by the energy-based decision rule
for LMNN [15], which was itself inspired by earlier work on multilayer neural nets [19].

3.2 Experiments on multiway classiﬁcation

We evaluated MKMs on the two multiclass data sets from previous benchmarks [11] that exhibited
the largest performance gap between deep and shallow architectures. The data sets were created from
the MNIST data set [20] of 28 × 28 grayscale handwritten digits. The mnist-back-rand data set was
generated by ﬁlling the image background by random pixel values, while the mnist-back-image data
set was generated by ﬁlling the image background with random image patches; examples are shown
in Figs. 4 and 5. Each data set contains 12000 and 50000 training and test examples, respectively.

6

Figure 4: Left: examples from the mnist-back-rand data set. Right: classiﬁcation error rates on the
test set for MKMs with different kernels and numbers of layers (cid:96). MKMs with arc-cosine kernel
have error rates from 6.36–7.52%. The best previous results are 14.58% for SVMs with RBF kernels
and 6.73% for deep belief nets [11].

Figure 5: Left: examples from the mnist-back-image data set. Right: classiﬁcation error rates on the
test set for MKMs with different kernels and numbers of layers (cid:96). MKMs with arc-cosine kernel
have error rates from 18.43–29.79%. The best previous results are 22.61% for SVMs with RBF
kernels and 16.31% for deep belief nets [11].

We trained MKMs with arc-cosine kernels and RBF kernels in each layer. For each data set, we
initially withheld the last 2000 training examples as a validation set. Performance on this validation
set was used to determine each MKM’s architecture, as described in the previous section, and also
to set the kernel width in RBF kernels, following the same methodology as earlier studies [11].
Once these parameters were set by cross-validation, we re-inserted the validation examples into the
training set and used all 12000 training examples for feature selection and distance metric learning.
For kernel PCA, we were limited by memory requirements to processing only 6000 out of 12000
training examples. We chose these 6000 examples randomly, but repeated each experiment ﬁve
times to obtain a measure of average performance. The results we report for each MKM are the
average performance over these ﬁve runs.
The right panels of Figs. 4 and 5 show the test set error rates of MKMs with different kernels and
numbers of layers (cid:96). For reference, we also show the best previously reported results [11] using
traditional SVMs (with RBF kernels) and deep belief nets (with three layers). MKMs perform sig-
niﬁcantly better than shallow architectures such as SVMs with RBF kernels or LMNN with feature
selection (reported as the case (cid:96) = 0). Compared to deep belief nets, the leading MKMs obtain
slightly lower error rates on one data set and slightly higher error rates on another.
We can describe the architecture of an MKM by the number of selected features at each layer (in-
cluding the input layer). The number of features essentially corresponds to the number of units in
each layer of a neural net. For the mnist-back-rand data set, the best MKM used an n=1 arc-cosine
kernel and 300-90-105-136-126-240 features at each layer. For the mnist-back-image data set, the
best MKM used an n=0 arc-cosine kernel and 300-50-130-240-160-150 features at each layer.
MKMs worked best with arc-cosine kernels of degree n = 0 and n = 1. The kernel of degree n = 2
performed less well in MKMs, perhaps because multiple iterations of kernel PCA distorted the
dynamic range of the inputs (which in turn seemed to complicate the training for LMNN). MKMs
with RBF kernels were difﬁcult to train due to the sensitive dependence on kernel width parameters.
It was extremely time-consuming to cross-validate the kernel width at each layer of the MKM. We
only obtained meaningful results for one and two-layer MKMs with RBF kernels.

7

5678DBN−3Test error rate (%) 0 1    2    3    4   5      Step (n=0) 1    2    3    4   5     Ramp (n=1)            1    2Quarter−pipe (n=2) 1    2  RBF222426DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure2:Left:examplesfromtherectangles-imagedataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom22.36–25.64%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare24.04%forSVMswithRBFkernelsand22.50%fordeepbeliefnets[2].Seetextfordetails.1718192021DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure3:Left:examplesfromtheconvexdataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom17.15–20.51%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare19.13%forSVMswithRBFkernelsand18.63%fordeepbeliefnets[2].Seetextfordetails.2000trainingexamplesasavalidationsettochoosethemarginpenaltyparameter;afterchoosingthisparameterbycross-validation,wethenretrainedeachSVMusingallthetrainingexamples.Forreference,wealsoreportthebestresultsobtainedpreviouslyfromthreelayerdeepbeliefnets(DBN-3)andSVMswithRBFkernels(SVM-RBF).Thesereferencesarerepresentativeofthecurrentstate-of-the-artfordeepandshallowarchitecturesonthesedatasets.Therightpanelsofﬁgures2and3showthetestseterrorratesfromarccosinekernelsofvaryingdegree(n)andlevelsofrecursion(!).Weexperimentedwithkernelsofdegreen=0,1and2,correspondingtosinglelayerthresholdnetworkswith“step”,“ramp”,and“quarter-pipe”activationfunctions.Wealsoexperimentedwiththemultilayerkernelsdescribedinsection2.3,composedfromonetosixlevelsofrecursion.Overall,theﬁguresshowthatonthesetwodatasets,manydifferentarccosinekernelsoutperformthebestresultspreviouslyreportedforSVMswithRBFkernelsanddeepbeliefnets.Wegivemoredetailsontheseexperimentsbelow.Atahighlevel,though,wenotethatSVMswitharccosinekernelsareverystraightforwardtotrain;unlikeSVMswithRBFkernels,theydonotrequiretuningakernelwidthparameter,andunlikedeepbeliefnets,theydonotrequiresolvingadifﬁcultnonlinearoptimizationorsearchingoverpossiblearchitectures.Inourexperiments,wequicklydiscoveredthatthemultilayerkernelsonlyperformedwellwhenn=1kernelswereusedathigher(!>1)levelsintherecursion.Figs.2and3thereforeshowonlythesesetsofresults;inparticular,eachgroupofbarsshowsthetesterrorrateswhenaparticularkernel(ofdegreen=0,1,2)wasusedattheﬁrstlayerofnonlinearity,whilethen=1kernelwasusedatsuccessivelayers.Wedonothaveaformalexplanationforthiseffect.However,recallthatonlythen=1arccosinekernelpreservesthenormofitsinputs:then=0kernelmapsallinputsontoaunithypersphereinfeaturespace,whilehigher-order(n>1)kernelsmayinducefeaturespaceswithseverelydistorteddynamicranges.Therefore,wehypothesizethatonlyn=1arccosinekernelspreservesufﬁcientinformationaboutthemagnitudeoftheirinputstoworkeffectivelyincompositionwithotherkernels.Finally,theresultsonbothdatasetsrevealaninterestingtrend:themultilayerarccosinekernelsoftenperformbetterthantheirsinglelayercounterparts.ThoughSVMsareshallowarchitectures,51718192021DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)222426DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure2:Left:examplesfromtherectangles-imagedataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom22.36–25.64%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare24.04%forSVMswithRBFkernelsand22.50%fordeepbeliefnets[2].Seetextfordetails.1718192021DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)Figure3:Left:examplesfromtheconvexdataset.Right:classiﬁcationerrorratesonthetestset.SVMswitharccosinekernelshaveerrorratesfrom17.15–20.51%.Resultsareshownforkernelsofvaryingdegree(n)andlevelsofrecursion(!).Thebestpreviousresultsare19.13%forSVMswithRBFkernelsand18.63%fordeepbeliefnets[2].Seetextfordetails.2000trainingexamplesasavalidationsettochoosethemarginpenaltyparameter;afterchoosingthisparameterbycross-validation,wethenretrainedeachSVMusingallthetrainingexamples.Forreference,wealsoreportthebestresultsobtainedpreviouslyfromthreelayerdeepbeliefnets(DBN-3)andSVMswithRBFkernels(SVM-RBF).Thesereferencesarerepresentativeofthecurrentstate-of-the-artfordeepandshallowarchitecturesonthesedatasets.Therightpanelsofﬁgures2and3showthetestseterrorratesfromarccosinekernelsofvaryingdegree(n)andlevelsofrecursion(!).Weexperimentedwithkernelsofdegreen=0,1and2,correspondingtosinglelayerthresholdnetworkswith“step”,“ramp”,and“quarter-pipe”activationfunctions.Wealsoexperimentedwiththemultilayerkernelsdescribedinsection2.3,composedfromonetosixlevelsofrecursion.Overall,theﬁguresshowthatonthesetwodatasets,manydifferentarccosinekernelsoutperformthebestresultspreviouslyreportedforSVMswithRBFkernelsanddeepbeliefnets.Wegivemoredetailsontheseexperimentsbelow.Atahighlevel,though,wenotethatSVMswitharccosinekernelsareverystraightforwardtotrain;unlikeSVMswithRBFkernels,theydonotrequiretuningakernelwidthparameter,andunlikedeepbeliefnets,theydonotrequiresolvingadifﬁcultnonlinearoptimizationorsearchingoverpossiblearchitectures.Inourexperiments,wequicklydiscoveredthatthemultilayerkernelsonlyperformedwellwhenn=1kernelswereusedathigher(!>1)levelsintherecursion.Figs.2and3thereforeshowonlythesesetsofresults;inparticular,eachgroupofbarsshowsthetesterrorrateswhenaparticularkernel(ofdegreen=0,1,2)wasusedattheﬁrstlayerofnonlinearity,whilethen=1kernelwasusedatsuccessivelayers.Wedonothaveaformalexplanationforthiseffect.However,recallthatonlythen=1arccosinekernelpreservesthenormofitsinputs:then=0kernelmapsallinputsontoaunithypersphereinfeaturespace,whilehigher-order(n>1)kernelsmayinducefeaturespaceswithseverelydistorteddynamicranges.Therefore,wehypothesizethatonlyn=1arccosinekernelspreservesufﬁcientinformationaboutthemagnitudeoftheirinputstoworkeffectivelyincompositionwithotherkernels.Finally,theresultsonbothdatasetsrevealaninterestingtrend:themultilayerarccosinekernelsoftenperformbetterthantheirsinglelayercounterparts.ThoughSVMsareshallowarchitectures,51718192021DBN!3SVM!RBFTest error rate (%)1   2    3   4    5   6       Step (n=0)1   2    3   4    5   6      Ramp (n=1)1   2    3   4    5   6Quarter!pipe (n=2)15202530DBN−3SVM−RBFTest error rate (%) 0 1    2    3    4   5      Step (n=0) 1    2    3    4   5     Ramp (n=1)            1    2Quarter−pipe (n=2) 1    2  RBFWe brieﬂy summarize many results that we lack space to report in full. We also experimented
on multiclass data sets using SVMs with single and multi-layer arc-cosine kernels, as described in
section 2. For multiclass problems, these SVMs compared poorly to deep architectures (both DBNs
and MKMs), presumably because they had no unsupervised training that shared information across
examples from all different classes. In further experiments on MKMs, we attempted to evaluate the
individual contributions to performance from feature selection and LMNN classiﬁcation. Feature
selection helped signiﬁcantly on the mnist-back-image data set, but only slightly on the mnist-back-
random data set. Finally, LMNN classiﬁcation in the output layer yielded consistent improvements
over basic kNN classiﬁcation provided that we used the energy-based decision rule [15].

4 Discussion

In this paper, we have developed a new family of kernel functions that mimic the computation in
large, multilayer neural nets. On challenging data sets, we have obtained results that outperform pre-
vious SVMs and compare favorably to deep belief nets. More signiﬁcantly, our experiments validate
the basic intuitions behind deep learning in the altogether different context of kernel-based archi-
tectures. A similar validation was provided by recent work on kernel methods for semi-supervised
embedding [7]. We hope that our results inspire more work on kernel methods for deep learning.
There are many possible directions for future work. For SVMs, we are currently experimenting with
arc-cosine kernel functions of fractional and (even negative) degree n. For MKMs, we are hoping
to explore better schemes for feature selection [21, 22] and kernel selection [23]. Also, it would be
desirable to incorporate prior knowledge, such as the invariances modeled by convolutional neural
nets [24, 4], though it is not obvious how to do so. These issues and others are left for future work.

A Derivation of kernel function

(cid:90)

In this appendix, we show how to evaluate the multidimensional integral in eq. (1) for the arc-cosine
kernel. Let θ denote the angle between the inputs x and y. Without loss of generality, we can take x
to lie along the w1 axis and y to lie in the w1w2-plane. Integrating out the orthogonal coordinates
of the weight vector w, we obtain the result in eq. (3) where Jn(θ) is the remaining integral:

dw1 dw2 e− 1

2 (w2

1+w2

Jn(θ) =
1 (w1 cos θ + w2 sin θ)n. (14)
Changing variables to u = w1 and v = w1 cos θ+w2 sin θ, we simplify the domain of integration to
the ﬁrst quadrant of the uv-plane:

2) Θ(w1) Θ(w1 cos θ + w2 sin θ) wn

Jn(θ) =

(15)
The prefactor of (sin θ)−1 in eq. (15) is due to the Jacobian. To simplify the integral further, we
4 ). Then, integrating out the radius
adopt polar coordinates u = r cos( ψ
coordinate r, we obtain:

4 ) and v = r sin( ψ

dv e−(u2+v2−2uv cos θ)/(2 sin2 θ) unvn.

2 + π

2 + π

sin θ

0

0

du

(cid:90) ∞

(cid:90) ∞

1

Jn(θ) = n! (sin θ)2n+1

dψ

cosn ψ

(1 − cos θ cos ψ)n+1 .

(16)

To evaluate eq. (16), we ﬁrst consider the special case n=0. The following result can be derived by
contour integration in the complex plane [25]:

(cid:90) π

2

0

8

(cid:90) π/2

0

dψ

1 − cos θ cos ψ

= π − θ
sin θ

.

(17)

Substituting eq. (17) into our expression for the angular part of the kernel function in eq. (16), we
recover our earlier claim that J0(θ) = π− θ. Related integrals for the special case n = 0 can also be
found in earlier work [8].For the case n >0, the integral in eq. (16) can be performed by the method
of differentiating under the integral sign. In particular, we note that:

dψ

cosn ψ

(1 − cos θ cos ψ)n+1 =

1
n!

∂n

∂(cos θ)n

dψ

1 − cos θ cos ψ

.

(18)

Substituting eq. (18) into eq. (16), then appealing to the previous result in eq. (17), we recover the
expression for Jn(θ) in eq. (4).

(cid:90) π/2

0

(cid:90) π

2

0

References
[1] Y. Bengio and Y. LeCun. Scaling learning algorithms towards AI. MIT Press, 2007.
[2] G.E. Hinton, S. Osindero, and Y.W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-

tation, 18(7):1527–1554, 2006.

[3] G.E. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,

313(5786):504–507, July 2006.

[4] M.A. Ranzato, F.J. Huang, Y.L. Boureau, and Y. LeCun. Unsupervised learning of invariant feature
In Proceedings of the 2007 IEEE Conference on

hierarchies with applications to object recognition.
Computer Vision and Pattern Recognition (CVPR-07), pages 1–8, 2007.

[5] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: deep neural net-
works with multitask learning. In Proceedings of the 25th International Conference on Machine Learning
(ICML-08), pages 160–167, 2008.

[6] Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, to appear,

2009.

[7] J. Weston, F. Ratle, and R. Collobert. Deep learning via semi-supervised embedding. In Proceedings of

the 25th International Conference on Machine Learning (ICML-08), pages 1168–1175, 2008.

[8] C.K.I. Williams. Computation with inﬁnite neural networks. Neural Computation, 10(5):1203–1216,

1998.

[9] R.H.R. Hahnloser, H.S. Seung, and J.J. Slotine. Permitted and forbidden sets in symmetric threshold-

linear networks. Neural Computation, 15(3):621–638, 2003.

[10] R.M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc., 1996.
[11] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep archi-
tectures on problems with many factors of variation. In Proceedings of the 24th International Conference
on Machine Learning (ICML-07), pages 473–480, 2007.

[12] C.C. Chang and C.J. Lin. LIBSVM: a library for support vector machines, 2001. Software available at

http://www.csie.ntu.edu.tw/˜cjlin/libsvm.

[13] B. Sch¨olkopf, A. Smola, and K. M¨uller. Nonlinear component analysis as a kernel eigenvalue problem.

Neural Computation, 10(5):1299–1319, 1998.

[14] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine Learning

Research, 3:1157–1182, 2003.

[15] K.Q. Weinberger and L.K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation.

Journal of Machine Learning Research, 10:207–244, 2009.

[16] B. Sch¨olkopf, A. J. Smola, and K.-R. M¨uller. Nonlinear component analysis as a kernel eigenvalue

problem. Technical Report 44, Max-Planck-Institut f¨ur biologische Kybernetik, 1996.

[17] J. Goldberger, S. Roweis, G.E. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In
L.K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages
513–520. MIT Press, 2005.

[18] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In
B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19,
pages 153–160. MIT Press, 2007.

[19] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to
face veriﬁcation. In Proceedings of the 2005 IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR-05), pages 539–546, 2005.

[20] Y. LeCun and C. Cortes. The MNIST database of handwritten digits. http://yann.lecun.com/

exdb/mnist/.

[21] M. Tipping. Sparse kernel principal component analysis. In Advances in Neural Information Processing

Systems 13. MIT Press, 2001.

[22] A.J. Smola, O.L. Mangasarian, and B. Sch¨olkopf. Sparse kernel feature analysis. Technical Report 99-04,

University of Wisconsin, Data Mining Institute, Madison, 1999.

[23] G. Lanckriet, N. Cristianini, P. Bartlett, L.E. Ghaoui, and M.I. Jordan. Learning the kernel matrix with

semideﬁnite programming. Journal of Machine Learning Research, 5:27–72, 2004.

[24] Y. LeCun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. Backpropa-

gation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.

[25] G.F. Carrier, M. Krook, and C.E. Pearson. Functions of a Complex Variable: Theory and Technique.

Society for Industrial and Applied Mathematics, 2005.

9

