Machine  Learning,  20, 273-297 (1995)
©  1995 Kluwer Academic  Publishers, Boston.  Manufactured in The  Netherlands.

Support-Vector  Networks

CORINNA CORTES 
VLADIMIR VAPNIK 
AT&T Bell Labs., Holmdel, NJ 07733, USA

Editor: Lorenza Saitta

corinna@neural.att.com
vlad@neural.att.com

Abstract.  The support-vector  network is a new learning  machine for two-group  classification  problems.  The
machine  conceptually  implements  the  following  idea:  input  vectors  are  non-linearly  mapped  to  a  very  high-
dimension  feature space.  In this feature space a linear decision surface  is constructed.  Special properties  of the
decision  surface ensures high generalization  ability  of the learning  machine.  The idea behind the  support-vector
network  was previously  implemented  for the restricted  case where  the training  data can be separated  without
errors.  We here extend this result to non-separable  training data.

High generalization  ability  of support-vector  networks  utilizing  polynomial  input transformations  is  demon-
strated.  We also compare the performance  of the support-vector  network to various classical learning  algorithms
that all took part in a benchmark study of Optical  Character  Recognition.

Keywords:  pattern recognition,  efficient  learning  algorithms, neural networks, radial basis  function  classifiers,
polynomial  classifiers.

1.  Introduction

More than 60 years ago R.A. Fisher (Fisher,  1936) suggested the first algorithm for pattern
recognition.  He considered  a model  of two normal distributed populations, N(m1,  EI)
and N(m2,  E2)  of n dimensional vectors x with mean vectors m1 and m2  and co-variance
matrices E1 and E2, and showed that the optimal (Bayesian) solution is a quadratic decision
function:

In the case where  E1  =  E2  =  E the quadratic decision function (1) degenerates to a linear
function:

To estimate the quadratic decision function one has to determine  "("+3) free parameters.  To
estimate the linear function only n free parameters have to be determined.  In the case where
the number of observations  is small (say less than 10n2)  estimating o(n2) parameters is not
reliable.  Fisher  therefore  recommended,  even in the case of EI  ^  £2,  to use the linear
discriminator function  (2) with £  of the form:

where  T is  some constant1.  Fisher also recommended  a linear decision  function  for  the
case  where  the  two  distributions  are  not  normal.  Algorithms  for  pattern  recognition

274

CORTES  AND VAPNIK

Figure  1.  A  simple feed-forward perceptron  with 8 input units, 2 layers of hidden units, and 1 output unit.  The
gray-shading of the vector entries  reflects their numeric value.

were  therefore  from  the  very  beginning associated  with  the  construction  of  linear  deci-
sion  surfaces.

In  1962  Rosenblatt  (Rosenblatt,  1962) explored  a different  kind of learning  machines:
perceptrons  or neural networks. The perceptron  consists of connected neurons, where each
neuron  implements  a separating  hyperplane,  so the perceptron  as a whole  implements  a
piecewise linear separating  surface.  See Fig. 1.

No algorithm that minimizes the error on a set of vectors by adjusting  all the weights of
the network was found in Rosenblatt's  time, and Rosenblatt suggested a scheme where only
the weights  of the output unit were  adaptive.  According  to the fixed setting  of the  other
weights the input vectors are non-linearly transformed into the feature space, Z,  of the last
layer of units.  In this space  a linear decision function  is  constructed:

by adjusting the weights ai  from the ith hidden unit to the output unit so as to minimize some
error  measure  over the training data.  As a result of Rosenblatt's  approach, construction of
decision  rules  was  again  associated  with the  construction  of  linear  hyperplanes  in some
space.

An algorithm that allows for all weights of the neural network to adapt in order locally to
minimize the error on a set of vectors belonging to a pattern recognition problem was found
in 1986 (Rumelhart, Hinton & Williams, 1986,1987; Parker,  1985; LeCun, 1985) when the
back-propagation  algorithm  was discovered.  The solution  involves a slight  modification
of the mathematical  model of neurons. Therefore,  neural networks implement "piece-wise
linear-type" decision  functions.

In this article we construct a new type of learning machine, the so-called  support-vector
network.  The  support-vector  network  implements  the  following idea:  it maps  the input
vectors  into  some  high  dimensional  feature  space  Z  through  some  non-linear  mapping
chosen  a priori.  In this space a linear decision surface is constructed with special  properties
that ensure high generalization  ability of the network.

SUPPORT-VECTOR  NETWORKS

275

Figure 2.  An example of a separable problem in a 2 dimensional  space. The support vectors, marked with grey
squares, define the margin of largest separation between the two classes.

EXAMPLE.  To obtain a decision surface corresponding to a polynomial of degree two, one
can create a feature space,  Z, which has N  =  2&±21 coordinates  of the form:

where x =  (x\,..., xn).  The hyperplane is then constructed in this space.

Two problems arise in the above approach:  one conceptual and one technical. The con-
ceptual problem is how to find a separating hyperplane that will generalize well: the dimen-
sionality of the feature space will be huge, and not all hyperplanes that separate the training
data will necessarily  generalize  well2.  The technical problem is how computationally to
treat  such  high-dimensional  spaces:  to construct  polynomial of degree 4  or 5 in a 200
dimensional  space  it may  be necessary  to construct hyperplanes in a billion dimensional
feature space.

The conceptual  part of this problem was solved  in  1965  (Vapnik, 1982) for the case of
optimal  hyperplanes  for separable classes.  An optimal hyperplane is here defined  as  the
linear decision  function  with maximal margin between the vectors  of the two classes,  see
Fig. 2.  It was observed  that to construct such optimal hyperplanes one only has to take into
account a small amount of the training data, the so called support  vectors, which determine
this margin.  It  was shown that if the training vectors  are separated  without errors  by an
optimal hyperplane the expectation value of the probability of committing an error on a test
example is bounded by the ratio between the expectation  value of the number of support
vectors  and the number of training vectors:

276

CORTES  AND VAPNIK

Note that this bound does not explicitly contain the dimensionality of the space of separation.
It follows from  this bound, that if the optimal hyperplane can be constructed  from  a small
number of support vectors  relative  to the training set size the generalization  ability will be
high—even in an infinite dimensional space.  In Section 5 we will demonstrate that the ratio
(5) for a real  life problems  can be as low as 0.03  and the optimal hyperplane  generalizes
well in a billion dimensional  feature  space.

Let

be  the  optimal  hyperplane  in  feature  space.  We  will show,  that  the  weights W0 for  the
optimal  hyperplane  in  the  feature  space  can  be  written  as  some  linear  combination of
support  vectors

The linear decision function  / (z) in the feature space will accordingly  be of the form:

where zi- • z is the dot-product between  support vectors zi and vector z in feature space.  The
decision  function  can therefore be described  as a two layer network (Fig.  3).

However,  even if the optimal  hyperplane  generalizes  well the technical problem  of how
to treat the high dimensional  feature space remains.  In  1992 it was shown (Boser, Guyon,
&  Vapnik,  1992),  that  the  order  of  operations  for  constructing  a  decision  function  can
be interchanged:  instead  of making a non-linear  transformation of the  input vectors  fol-
lowed  by  dot-products  with  support vectors  in feature  space,  one can first compare  two
vectors  in  input space (by e.g. 
taking their dot-product  or some  distance  measure),  and
then  make  a  non-linear  transformation  of  the  value  of  the  result  (see  Fig.  4).  This  en-
ables the construction of rich classes  of decision surfaces, for example polynomial decision
surfaces  of arbitrary  degree.  We will  call  this type of learning machine a  support-vector
network3.

The technique of support-vector  networks was first developed  for the restricted  case of
separating  training data without errors.  In this article  we extend  the approach  of support-
vector networks to cover when separation without error on the training vectors is impossible.
With  this  extension  we consider  the  support-vector  networks as a new class  of learning
machine, as powerful and universal as neural networks.  In Section 5 we will  demonstrate
how well it generalizes  for high degree  polynomial decision  surfaces (up to order 7) in a
high dimensional space (dimension 256).  The performance  of the algorithm is  compared
to that of classical  learning machines e.g.  linear classifiers, ^-nearest neighbors classifiers,
and neural networks.  Sections  2, 3, and 4 are devoted to the major  points of the derivation
of the  algorithm  and a discussion of some of its properties.  Details  of the derivation  are
relegated  to an appendix.

SUPPORT-VECTOR  NETWORKS

277

Figure 3.  Classification by a support-vector  network of an unknown pattern is conceptually done by first trans-
forming the pattern  into some high-dimensional feature space.  An optimal hyperplane constructed in this feature
space determines  the output. The similarity to a two-layer  perceptron can be seen by comparison to Fig. 1.

2.  Optimal  Hyperplanes

In this section  we review the method of optimal hyperplanes (Vapnik, 1982) for separation
of training data without errors.  In the next section  we introduce a notion of soft margins,
that will allow for an analytic treatment of learning with errors on the training set.

2.1.  The Optimal Hyperplane  Algorithm

The set of labeled  training patterns

is said to be linearly separable if there exists a vector w and a scalar b such that the inequalities

278

CORTES AND VAPNIK

Figure  4.  Classification  of  an  unknown  pattern  by  a  support-vector  network.  The  pattern  is in  input space
compared  to  support  vectors.  The  resulting  values  are  non-linearly  transformed.  A  linear  function  of  these
transformed  values determine the output of the classifier.

are valid for all elements  of the training set (8).  Below  we write the inequalities  (9) in the
form4:

The optimal hyperplane

is the unique one  which separates the training data  with a maximal margin:  it  determines
the direction  w/|w|  where the distance between  the projections of the training vectors of
two different classes is maximal, recall Fig. 2.  This distance p ( w , b)  is given by

The optimal hyperplane  (W0, b0) is the arguments that maximize the distance (12). It follows
from  (12)  and (10)  that

SUPPORT-VECTOR  NETWORKS

279

This  means that the optimal  hyperplane  is the unique one that minimizes w • w under the
constraints (10).  Constructing an optimal hyperplane is therefore a quadratic  programming
problem.

Vectors xi for which yi (w • x, +  6) =  1 will be termed  support  vectors. In Appendix A. 1
we show that the vector WQ that determines the optimal hyperplane can be written as a linear
combination  of training vectors:

where a°  >  0.  Since a  > 0 only for support vectors  (see  Appendix),  the expression  (14)
represents a compact form of writing w0.  We also show that to find the vector of parameters
ai:

one has to solve the following quadratic programming  problem:

with respect to Ar  =  (a 1,..., at),  subject to the constraints:

where 1T  =  (1,...,  1) is an  -dimensional unit vector, Yr  =  (yi 
sional vector of labels, and D is a symmetric I  x  ^-matrix with elements

y^) is the £-dimen-

The inequality (16) describes the nonnegative quadrant. We therefore have to maximize the
quadratic form (15)  in the nonnegative quadrant, subject to the constraints (17).

When the training data (8) can be separated  without errors we also show in Appendix A
the following relationship between the maximum of the functional  (15), the pair (Ao,  b0),
and the maximal margin po from (13):

If for some A* and large constant  W0 the inequality

is valid, one can accordingly assert that all hyperplanes that separate  the training data (8)
have a margin

280

CORTES  AND VAPNIK

If the  training  set  (8) cannot  be separated  by a hyperplane, the  margin between  patterns
of the two classes becomes  arbitrary  small, resulting in the value of the functional  W(A)
turning  arbitrary  large.  Maximizing  the  functional  (15)  under  constraints  (16)  and  (17)
one therefore  either  reaches a maximum  (in this case one has constructed  the  hyperplane
with the maximal  margin po), or one finds that the maximum exceeds some given  (large)
constant  Wo (in  which  case  a  separation  of  the  training  data  with  a  margin  larger  then
V^/Wo  is  impossible).

The problem  of maximizing functional (15) under constraints (16) and (17) can be solved
very  efficiently  using  the  following  scheme. Divide the  training  data  into  a  number  of
portions  with a reasonable small number  of training vectors  in each portion.  Start out by
solving the quadratic programming problem determined by the first portion of training data.
For this problem  there are two possible outcomes:  either this portion  of the data cannot  be
separated  by a hyperplane (in which case the full  set of data as well cannot be  separated),
or the optimal hyperplane  for separating  the first portion of the training data is found.

Let the vector that maximizes functional (15) in the case of separation of the first portion
be A1.  Among  the coordinates  of vector A1 some  are equal  to zero.  They  correspond  to
non-support  training vectors  of this portion.  Make  a new set of training data containing
the  support  vectors  from  the  first  portion of  training data  and  the  vectors  of  the  second
portion  that do  not  satisfy  constraint  (10),  where  w is determined  by A1.  For this  set a
new  functional  W2(A) is constructed  and  maximized  at  A2.  Continuing this process of
incrementally  constructing  a solution  vector  A* covering all the portions  of the  training
data one either finds  that it is impossible  to separate the training set without error, or one
constructs  the optimal  separating  hyperplane for the full  data  set,  A,  =  A0-.  Note,  that
during this  process  the  value of the  functional  W(A)  is monotonically increasing,  since
more and more training vectors are considered  in the optimization, leading to a smaller and
smaller separation  between the two  classes.

3.  The Soft Margin  Hyperplane

Consider the case where the training data cannot be separated  without error.  In this case
one may want to separate the training set with a minimal number of errors. To express this
formally let us introduce some non-negative variables  £,- >  0,  i =  I,...,(,.

We can now minimize the  functional

for  small a  > 0, subject to the constraints

For sufficiently  small a  the functional  (21) describes  the number of the training errors5.

Minimizing (21) one finds some minimal subset of training errors:

SUPPORT-VECTOR  NETWORKS

281

If these  data are excluded  from  the training set one can separate  the remaining part of the
training  set  without errors.  To separate  the  remaining  part  of the  training data  one can
construct  an optimal  separating hyperplane.

This idea can be expressed formally as: minimize the  functional

subject to constraints (22) and (23), where F(u)  is a monotonic convex function  and C is
a constant.

For  sufficiently  large  C  and  sufficiently  small a,  the  vector  wo and constant  b0,  that
minimize  the  functional  (24)  under constraints  (22)  and  (23),  determine  the hyperplane
that minimizes the number of errors on the training set and separate the rest of the elements
with maximal margin.

Note,  however,  that  the  problem  of  constructing a  hyperplane  which minimizes the
number of errors on the training set is in general NP-complete.  To avoid  NP-completeness
of  our  problem  we  will  consider  the case of a  — 1 (the  smallest  value  of a  for which
the  optimization  problem  (15)  has  a  unique solution).  In  this  case  the  functional  (24)
describes  (for  sufficiently  large  C)  the problem  of constructing a  separating  hyperplane
which  minimizes  the sum  of  deviations, £,  of  training errors  and maximizes  the margin
for  the correctly  classified vectors.  If the training data can be separated  without errors  the
constructed hyperplane coincides with the optimal margin hyperplane.

In contrast to the case with a  <  I there exists an efficient method for finding the solution

of (24) in the case of a  =  1. Let us call this solution the soft  margin  hyperplane.

In Appendix A we consider the problem of minimizing the  functional

subject to the constraints (22)  and (23), where F(u)  is a monotonic convex function  with
F(0)  =  0. To simplify the formulas we only describe the case of F(u)  = u2 in this section.
For this function the optimization problem remains a quadratic programming problem.

In Appendix A we show that the vector w, as for the optimal hyperplane algorithm, can

be written as a linear combination of support vectors  x,:

To find the vector  AT  =  («i,..., at)  one has to solve the dual quadratic programming
problem of maximizing

subject to constraints

282

CORTES  AND VAPNIK

where  1, A, Y,  and  D  are  the  same  elements  as  used  in  the  optimization  problem  for
constructing  an  optimal  hyperplane,  S is  a scalar,  and (29) describes  coordinate-wise  in-
equalities.

Note that (29) implies that the smallest admissible value S in functional  (26) is

Therefore  to find a soft margin classifier one has to find a vector A that maximizes

under the constraints A  >  0 and (27).  This problem differs from the problem of constructing
an optimal  margin  classifier only  by the additional term with amax  in the functional (30).
Due  to this  term  the solution  to the problem  of constructing  the soft  margin  classifier is
unique and exists  for any data set.

The  functional (30) is not quadratic  because  of the term  with amax.  Maximizing (30)
subject  to the constraints  A  >  0 and (27) belongs  to the group  of so-called  convex pro-
gramming  problems.  Therefore,  to construct a soft  margin classifier  one can either solve
the convex programming problem  in the ^-dimensional space  of the parameters  A, or one
can solve the quadratic programming problem in the dual t  + 1 space of the parameters  A
and  S.  In  our experiments  we construct  the  soft  margin hyperplanes by  solving the dual
quadratic programming  problem.

4.  The Method  of Convolution  of the Dot-Product in Feature  Space

The algorithms described  in the previous sections construct hyperplanes in the input space.
To construct  a hyperplane  in a feature  space  one first has to transform the  n-dimensional
input vector x into an //-dimensional  feature vector through a choice of an  N-dimensional
vector  function  0:

An  N  dimensional  linear  separator  w  and  a  bias  b  is  then  constructed  for  the  set  of

transformed  vectors

Classification  of an unknown vector x is done by first transforming the vector to the  sepa-
rating space (x  i-»-  0 (x)) and then taking the sign of the  function

According  to  the  properties  of  the  soft  margin  classifier  method  the  vector  w  can  be

written as a linear combination of support vectors  (in the feature space).  That means

SUPPORT-VECTOR  NETWORKS

283

The linearity of the dot-product implies, that the classification  function  /  in (31) for an

unknown vector x only depends on the dot-products:

The idea  of constructing support-vector networks comes from  considering general  forms
of the dot-product in a Hilbert space (Anderson & Bahadur, 1966):

According  to  the  Hilbert-Schmidt  Theory  (Courant  &  Hilbert,  1953)  any symmetric

function  K(u, v), with K ( u , v) e  LI,  can be expanded in the form

where A, e  SK and  fa  are eigenvalues and eigenfunctions

of the integral operator  defined by the kernel K(u,  v).  A sufficient  condition to ensure that
(34) defines a dot-product in a feature space is that all the eigenvalues in the expansion (35)
are positive.  To guarantee that these coefficients  are positive, it is necessary  and  sufficient
(Mercer's Theorem) that the condition

is satisfied for all g such that

Functions that satisfy Mercer's theorem can therefore be used as dot-products.  Aizerman,
Braverman and Rozonoer (1964) consider  a convolution of the dot-product  in the feature
space given by function of the  form

which they call Potential Functions.

However, the convolution of the dot-product in feature space can be given by any function
satisfying  Mercer's condition; in particular, to construct a polynomial classifier of  degree
d  in n-dimensional  input space one can use the following  function

284

CORTES AND VAPNIK

Using different dot-products  K(u,  v) one can construct different  learning machines with
arbitrary types of decision  surfaces (Boser, Guyon & Vapnik, 1992).  The decision  surface
of these machines  has a form

where  xi  is the image of a support vector in input space and  ai  is the weight of a  support
vector in the feature space.

To find the vectors  xi  and weights  ai  one follows the same  solution scheme  as for the
original  optimal  margin  classifier  or  soft  margin  classifier.  The  only  difference  is  that
instead of matrix D (determined  by (18))  one uses the matrix

5.  General Features of Support-Vector  Networks

5.1.  Constructing  the Decision Rules by Support-Vector  Networks  is  Efficient

To construct a support-vector network decision rule one has to solve a quadratic optimization
problem:

under the simple  constraints:

where matrix

is determined  by the elements  of the training set, and K(u,  v) is the function determining
the convolution of the  dot-products.

The solution to the optimization problem can be found efficiently  by solving intermediate
optimization problems determined  by the training data, that currently constitute the support
vectors.  This technique is described  in Section  3.  The obtained  optimal  decision  function
is unique6.

Each optimization problem can be solved using any standard  techniques.

5.2.  The Support-Vector  Network  is a  Universal Machine

By changing the function  K (u, v) for the convolution of the dot-product one can implement
different  networks.

SUPPORT-VECTOR  NETWORKS

285

In the next section we will consider support-vector network machines that use polynomial
decision  surfaces.  To specify  polynomials of different  order  d  one can use  the following
functions  for convolution of the  dot-product

Radial Basis Function machines  with decision  functions of the form

can be implemented  by using convolutions of the type

In  this case the  support-vector  network machine  will  construct both  the centers  xi  of the
approximating  function and the weights  ai.

One  can  also  incorporate  a priori  knowledge  of  the problem  at  hand  by  constructing
special convolution functions. Support-vector  networks are therefore a rather general  class
of learning machines which changes  its set of decision  functions  simply by changing the
form of the  dot-product.

5.3.  Support- Vector Networks and Control of Generalization Ability

To control the generalization  ability of a learning machine one has to control two  different
factors:  the  error-rate  on  the  training  data  and  the  capacity  of  the  learning  machine  as
measured  by its VC-dimension (Vapnik, 1982).  There exists a bound for the probability of
errors  on the test set of the following form:  with probability  1 — r; the inequality

is valid.  In the bound  (38)  the confidence  interval  depends  on the VC-dimension  of  the
learning machine, the number of elements  in the training set, and the value of n.

The  two  factors  in (38)  form  a trade-off:  the  smaller  the VC-dimension  of  the set of
functions  of the  learning  machine,  the smaller  the confidence interval,  but the  larger  the
value of the error frequency.

A general  way for resolving this trade-off was proposed  as the principle of structural risk
minimization:  for the given data  set one has to find a solution that minimizes their  sum.
A particular  case of structural risk minimization principle is the Occam-Razor  principle:
keep  the first term equal to zero and minimize the second  one.

It is known that the VC-dimension of the set of linear indicator  functions

with  fixed  threshold  b  is  equal  to  the  dimensionality of  the  input  space.  However,  the
VC-dimension  of the subset

286

CORTES  AND VAPNIK

(the set of functions with bounded norm of the weights) can be less than the dimensionality
of the input space and will depend  on  Cw.

From  this point of view the optimal margin classifier method executes  an  Occam-Razor
principle.  It keeps  the  first  term  of  (38)  equal to  zero  (by  satisfying  the  inequality  (9))
and it minimizes  the second term  (by minimizing the functional  w • w).  This minimization
prevents  an over-fitting problem.

However,  even  in the case where the training data  are separable one may obtain better
generalization  by  minimizing the  confidence term  in  (38)  even  further  at the  expense  of
errors on the training set.  In the soft margin classifier method this can be done by choosing
appropriate values  of the parameter C.  In the  support-vector  network algorithm  one can
control the trade-off between complexity of decision rule and frequency of error by changing
the parameter  C,  even  in the more  general  case where  there exists no solution  with zero
error on the training set.  Therefore  the support-vector network can control both factors for
generalization  ability of the learning machine.

6.  Experimental Analysis

To demonstrate  the support-vector  network method  we conduct two types of experiments.
We construct  artificial sets  of patterns in the plane and experiment  with 2nd degree  poly-
nomial decision  surfaces,  and we conduct experiments  with the real-life problem  of digit
recognition.

6.1.  Experiments  in the Plane

Using dot-products  of the form

with d  =  2 we construct decision  rules for different  sets of patterns in the plane.  Results
of these  experiments  can be  visualized and provide nice illustrations of the power of  the
algorithm.  Examples are shown in Fig.  5. The 2 classes are represented  by black and white

Figure  5.  Examples  of the dot-product  (39)  with d  =  2.  Support  patterns  are indicated with  double  circles,
errors with a cross.

SUPPORT-VECTOR NETWORKS

287

Figure 6.  Examples of patterns with labels from the US Postal Service digit database.

bullets.  In the figure we indicate support patterns with a double circle,  and errors with a
cross.  The solutions are optimal in the sense that no 2nd degree polynomials exist that make
less errors.  Notice  that the numbers of support patterns relative to the number of training
patterns are small.

6.2.  Experiments  with Digit Recognition

Our experiments for constructing support-vector networks make use of two different  data-
bases for bit-mapped digit recognition, a small and a large database.  The small one is a US
Postal Service  database that contains 7,300 training patterns and 2,000 test patterns.  The
resolution of the database is 16 x  16 pixels, and some typical examples are shown in Fig. 6.
On this database we report experimental research  with polynomials of various degree.

The large database consists  of 60,000 training and 10,000 test patterns, and is a 50-50
mixture of  the NIST7  training and test sets.  The resolution of these patterns is 28  x  28
yielding an input dimensionality of 784.  On this database  we have only constructed a 4th
degree polynomial classifier. The performance of this classifier is compared to other types
of learning machines that took part in a benchmark study (Bottou, 1994).

In all our experiments ten separators, one for each class,  are constructed.  Each hyper-
surface makes use of the same dot product and pre-processing  of the data.  Classification of
an unknown patterns is done according to the maximum output of these ten classifiers.

6.2.1.  Experiments  with US Postal Service Database.  The US Postal Service  Database
has been recorded from actual mail pieces and results from this database have been reported
by several  researchers.  In Table 1 we list the performance of various classifiers  collected

Table  1.  Performance of various classifiers  collected  from publications  and own experiments.  For references
see text.

Classifier

Raw error, %

Human performance
Decision tree, CART
Decision tree,  C4.5
Best 2 layer neural network
Special architecture 5 layer network

2.5
17
16
6.6
5.1

288

CORTES  AND VAPNIK

Table  2.  Results obtained  for dot products of polynomials  of various degree.  The number of "support  vectors"
is a mean value per  classifier.

Degree of
polynomial

Raw

error, %

Support
vectors

Dimensionality of

feature space

1
2
3
4
5
6
7

12.0
4.7
4.4
4.3
4.3
4.2
4.3

200
127
148
165
175
185
190

256

-33000
-1  x  106
~1  x 109
~1  x  1012
~1  x  1014
~1  x  1016

from publications  and own experiments.  The result of human performance  was  reported
by J. Bromley  & E. Sackinger  (Bromley & Sackinger,  1991).  The result  with CART was
carried  out by Daryl  Pregibon  and Michael  D. Riley at Bell Labs.,  Murray  Hill, NJ.  The
results  of C4.5  and the best 2-layer neural network (with optimal number of hidden units)
were obtained  specially  for this paper by Corinna Cortes  and Bernard Schoelkopf  respec-
tively.  The result with a special purpose neural network architecture with 5 layers,  LeNetl,
was obtained  by Y. LeCun  et al.  (1990).

On the experiments  with the US Postal  Service  Database  we used pre-processing  (cen-
tering, de-slanting  and smoothing) to incorporate  knowledge  about the invariances  of the
problem  at hand. The effect  of smoothing of this database  as a pre-processing  for  support-
vector networks was investigated  in (Boser, Guyon & Vapnik, 1992). For our  experiments
we chose the smoothing kernel as a Gaussian with standard deviation a  = 0.75  in agreement
with (Boser, Guyon & Vapnik, 1992).

In  the  experiments  with  this  database  we  constructed  polynomial  indicator  functions
based  on dot-products  of the form  (39).  The input dimensionality was 256,  and the  order
of the polynomial ranged from  1 to 7. Table 2 describes  the results of the experiments.  The
training data are not linearly  separable.

Notice that the number of support  vectors  increases very slowly.  The 7 degree polyno-
mial has  only 30% more support  vectors than the  3rd degree polynomial—and even  less
than  the first degree polynomial.  The dimensionality of the feature space  for a 7 degree
polynomial is however  1010  times  larger than the dimensionality of the  feature space  for
a  3rd  degree polynomial  classifier.  Note  that performance  almost  does  not change with
increasing dimensionality of the space—indicating no over-fitting problems.

The  relatively  high  number of  support  vectors  for  the  linear separator  is due  to  non-
separability:  the number 200 includes both support vectors and training vectors with a non-
zero £ -value. If | > 1 the training vector is misclassified; the number of mis-classifications
on the training set averages to 34 per classifier for the linear case.  For a 2nd degree classifier
the total number of mis-classifications on the training set is down to 4.  These 4 patterns are
shown in Fig.  7.

It is remarkable  that in all our experiments the bound for generalization ability (5) holds
when we consider the number of obtained support vectors instead of the expectation  value
of this number. In all cases the upper bound on the error probability for the single classifier
does not exceed  3% (on the test data the actual  error  does not exceed  1.5% for the single
classifier).

SUPPORT-VECTOR  NETWORKS

289

Figure 7.  Labeled  examples  of errors on the training set for the 2nd degree polynomial support-vector  classifier.

The  training time for construction of polynomial classifiers does not depend  on the de-
gree of the polynomial—only the number of support vectors.  Even in the worst case it is
faster  than the best performing neural  network, constructed  specially  for the task, LeNetl
(LeCun, et al.,  1990).  The performance  of this neural network is 5.1% raw error.  Polyno-
mials with degree 2 or higher outperform  LeNetl.

6.2.2.  Experiments with the N1ST Database.  The NIST database was used for benchmark
studies conducted over just 2 weeks.  The limited time frame enabled only the construction
of  1 type of classifier, for which we chose a 4th degree polynomial with no pre-processing.
Our choice  was based  on our experience  with the US Postal  database.

Table  3 lists  the number of support vectors  for each  of the  10 classifiers  and gives  the
performance  of the classifier on the training and test  sets.  Notice that even polynomials
of degree 4 (that have more  than  108 free parameters)  commit errors  on this training set.
The average  frequency  of training errors is 0.02% ~  12 per class.  The 14 misclassified test
patterns for classifier  1 are shown in Fig.  8.  Notice  again how the upper bound (5) holds
for  the obtained  number of support  vectors.

The combined performance of the ten classifiers on the test set is 1.1% error.  This result
should be compared  to that of other participating classifiers in the benchmark study. These
other classifiers include a linear classifier, a k  =  3-nearest  neighbor classifier with 60,000
prototypes,  and  two  neural  networks specially  constructed  for digit recognition  (LeNetl
and LeNet4).  The authors only contributed  with results for support-vector  networks.  The
results of the benchmark are given in Fig.  9.

We conclude this section  by citing the paper  (Bottou, et al.,  1994) describing results of

the benchmark:

For quite a long time LeNetl  was considered  state  of the art....  Through  a  series
of experiments  in architecture,  combined  with an analysis  of the characteristics  of
recognition error, LeNet4  was crafted—
The support-vector network has excellent accuracy, which is most remarkable, be-
cause  unlike the other  high performance  classifiers, it does  not include knowledge

Table  3.  Results obtained for a 4th degree polynomial classifier on the NIST database.  The size of the training
set is 60,000, and the size of the test set is 10,000 patterns.

C1. 0

Cl.  1

Cl.  2

Cl. 3

Cl. 4

Cl. 5

Cl. 6

Cl. 7

Cl. 8

Cl. 9

Supp.  patt.
Error train
Error test

1379
7
19

989
16
14

1958
8
35

1900
11
35

1224
2
36

2024
4
49

1527
8
32

2064
16
43

2332
4
48

2765
1
63

290

CORTES  AND VAPNIK

Figure 8.  The  14 misclassified test patterns with labels for classifier 1. Patterns with label "1" are false  negative.
Patterns with other labels are false positive.

Figure 9.  Results  from the benchmark study.

about  the  geometry  of the problem.  In fact  the  classifier would do  as  well if  the
image pixels were encrypted  e.g.  by a fixed, random permutation.

The  last remark  suggests  that further improvement of the performance  of the  support-
vector  network  can  be  expected  from  the  construction  of  functions  for  the  dot-product
K(u,  v) that reflect a priori information about the problem at hand.

7.  Conclusion

This paper introduces the support-vector  network as a new learning machine for two-group
classification  problems.

The support-vector  network combines  3 ideas: the solution technique from  optimal hy-
perplanes  (that allows for an expansion of the solution vector on support vectors), the idea of
convolution of the dot-product (that extends the solution surfaces from linear to non-linear),
and the notion of soft margins (to allow for errors on the training set).

SUPPORT-VECTOR  NETWORKS

291

The algorithm  has been  tested  and compared  to the performance of other classical  al-
gorithms.  Despite  the simplicity  of the design  in its decision  surface the new  algorithm
exhibits a very fine performance in the comparison study.

Other characteristics  like capacity control and ease of changing the implemented decision
surface  render  the  support-vector  network an extremely  powerful  and universal learning
machine.

A.  Constructing Separating  Hyperplanes

In this appendix we derive both the method for constructing optimal hyperplanes and  soft
margin  hyperplanes.

A. 1.  Optimal  Hyperplane  Algorithm

It was shown in Section 2, that to construct the optimal hyperplane

which separates  a set of training data

one has to minimize a functional

subject to the constraints

To do this we use a standard optimization technique.  We construct a Lagrangian

where Ar  =  (ct\,..., at) is the vector of non-negative Lagrange multipliers corresponding
to the constraints  (41).

It is known that the solution to the optimization problem is determined by the saddle point
of this Lagrangian in the 21 +  1-dimensional  space of w, A, and b, where the minimum
should be taken with respect to the parameters  w and b, and the maximum should be taken
with respect to the Lagrange  multipliers A.

At the point of the minimum (with respect  to w and b) one obtains:

292

CORTES AND VAPNIK

From equality (43) we derive

which expresses, that the optimal hyperplane solution can be written as a linear combina-
tion of training vectors.  Note,  that only training vectors x,  with  ai  >  0 have an  effective
contribution to the sum (45).

Substituting  (45)  and (44)  into (42) we obtain

In vector notation this can be rewritten as

where 1 is an i-dimensional unit vector, and D is a symmetric t  x ^-matrix with elements

To  find  the  desired  saddle  point  it  remains  to  locate  the  maximum  of  (48)  under  the
constraints  (43)

where Yr  = (y\,...  ,yt), and

The  Kuhn-Tucker theorem  plays an important part in the theory of optimization.  Ac-
cording to this theorem, at our saddle point in W0, bo, A0,  any Lagrange multiplier af  and
its corresponding  constraint are connected by an equality

From this equality comes  that non-zero values  ai  are only achieved in the cases where

In other  words:  ai- ^  0 only for cases  were the inequality is met as an equality.  We call
vectors  xi  for which

for support-vectors.  Note, that in this terminology the Eq. (45) states that the solution vector
W0 can be expanded on support vectors.

SUPPORT-VECTOR NETWORKS

293

Another  observation,  based  on  the  Kuhn-Tucker  Eqs.  (44)  and  (45)  for  the  optimal
solution, is the relationship between the maximal value W(Ao)  and the separation  distance
A>:

Substituting this equality into the expression (46) for  W(A 0)  we obtain

Taking into account the expression  (13) from  Section 2 we obtain

where po is the margin for the optimal hyperplane.

A.2. 

Soft  Margin Hyperplane  Algorithm

Below we first consider  the case of F(u)  =  uk.  Then we describe the general result for a
monotonic convex function  F(u).

To construct a soft margin separating  hyperplane we maximize the  functional

under the constraints

The Lagrange functional for this problem  is

where the non-negative  multipliers Ar  =  (a1,  a2  ai)  arise from  the constraint  (49),
and the multipliers Rr  =  (r\,  r 2,...., r1) enforce the constraint  (50).

We have to find the  saddle  point  of this functional (the  minimum with respect to  the

variables  wi, b, and £,-, and the maximum with respect to the variables a,  and  ri).

Let us use the conditions for the minimum of this functional at the extremum point:

294

CORTES AND VAPNIK

If we denote

we can rewrite  Eq.  (54) as

From the equalities  (52)-(55) we find

Substituting the expressions  for WQ, bo, and S into the Lagrange  functional  (51) we obtain

To find the  soft  margin  hyperplane  solution  one  has to maximize  the  form  functional
(59)  under the constraints  (57)-(58) with respect to the non-negative variables ai,  ri  with
(' =  1,...,/. In vector notation (59) can be rewritten as

where A and D are as defined  above.  To find the desired  saddle point one therefore  has to
find the maximum of (60) under the constraints

and

From (62) and (64) one obtains that the vector A should satisfy  the conditions

SUPPORT-VECTOR  NETWORKS

295

From conditions (62) and (64) one can also conclude  that to maximize  (60)

Substituting this value of S into (60)  we obtain

To find the soft margin hyperplane one can therefore either find the maximum of the quadratic
form (51) under the constraints (61) and (65), or one has to find the maximum of the convex
function (60) under the constraints (61) and (56).  For the experiments reported in this paper
we used k =  2 and solved the quadratic  programming  problem  (51).

For the case of F(u)  =  u the same technique  brings us to the problem of solving the

following quadratic optimization problem:  minimize the functional

under the constraints

and

The general solution for the case of a monotone convex function F(u) can also be obtained

from  this technique. The soft margin hyperplane has a form

where Aj  =  (a°,..., o$) is the solution of the following dual convex programming prob-
lem:  maximize the functional

under the constraints

where we denote

For convex monotone functions  F(u)  with F(0)  =  0 the following inequality is valid:

Therefore the second term in square brackets is positive and goes to infinity when Ua^  goes
to  infinity.

296

CORTES  AND VAPNIK

Finally, we can consider the hyperplane that minimizes the form

subject to the constraints (49)-(50), where the second term minimizes the least square value
for  the errors.  This lead to the following  quadratic programming problem:  maximize the
functional

in the non-negative quadrant A  >  0 subject to the constraint Ar Y =  0.

Notes

1.  The  optimal coefficient for  T was  found  in the  sixties (Anderson & Bahadur, 1966).

2.  Recall Fisher's concerns  about small amounts of data and the quadratic discriminant function.

3.  With this name we emphasize how crucial the idea of expanding the solution on support vectors  is for  these
learning  machines.  In the  support-vectors  learning  algorithm  the  complexity  of the construction  does  not
depend on the dimensionality of the feature space, but on the number of support vectors.

4.  Note that in the inequalities (9) and (10) the right-hand side, but not vector w, is normalized.

5.  A training error is here defined  as a pattern where the inequality (22) holds with f  >  0.

6.  The decision  function  is unique but not its expansion  on support vectors.

7.  National Institute for Standards and Technology, Special Database  3.

References

Aizerman, M., Braverman,  E., & Rozonoer,  L. (1964). Theoretical foundations of the potential  function  method

in pattern recognition  learning. Automation and Remote Control, 25:821-837.

Anderson, T.W., & Bahadur, R.R. (1966). Classification into two multivariate  normal distributions with different

covariance  matrices. Ann. Math. Stat., 33:420-431.

Boser, B.E., Guyon, I., & Vapnik, V.N. (1992). A training algorithm for optimal margin classifiers. In Proceedings

of  the  Fifth  Annual  Workshop  of  Computational Learning Theory, 5,  144-152, Pittsburgh, ACM.

Bottou,  L., Cortes, C,  Denker, J.S.,  Drucker, H., Guyon, I., Jacket, L.D., LeCun,  Y., Sackinger,  E., Simard, P.,
Vapnik,  V.,  &  Miller,  U.A.  (1994).  Comparison  of  classifier  methods:  A  case  study  in  handwritten digit
recognition. Proceedings of  12th International  Conference  on Pattern Recognition and Neural  Network.

Bromley, J., & Sackinger, E. (1991). Neural-network and it-nearest-neighbor classifiers. Technical Report  11359-

910819-16TM, AT&T.

Courant,  R., & Hilbert,  D. (1953). Methods  of Mathematical  Physics, Interscience,  New York.
Fisher, R.A. (1936).  The use of multiple measurements in taxonomic problems. Ann. Eugenics, 7:111-132.
LeCun, Y. (1985). Une procedure d'apprentissage  pour reseau  a seuil assymetrique.  Cognitiva 85: A la Frontiere

de I'Intelligence  Artificielle  des Sciences de la Connaissance des Neurosciences, 599-604, Paris.

LeCun, Y, Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., & Jackel, L.D. (1990). Handwritten
digit recognition with a back-propagation network. Advances in Neural Information  Processing Systems, 2,396-
404,  Morgan  Kaufman.

Parker,  D.B. (1985). Learning logic. Technical  Report TR-47, Center for Computational Research  in  Economics

and Management  Science, Massachusetts  Institute of Technology, Cambridge, MA.

Rosenblatt, F. (1962).  Principles ofNeurodynamics,  Spartan Books, New York.

SUPPORT-VECTOR NETWORKS

297

Rumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986). Learning  internal representations  by  backpropagating

errors. Nature,  323:533-536.

Rumelhart, D.E., Hinton, G.E., & Williams, R.J. (1987). Learning internal representations  by error  propagation.
In James L. McClelland  & David E. Rumelhart (Eds.), Parallel Distributed Processing, 1,318-362, MIT Press.
Vapnik, V.N. (1982). Estimation of  Dependences  Based on Empirical Data, Addendum 1, New  York:  Springer-

Verlag.

Received May  15, 1993
Accepted February 20, 1995
Final Manuscript  March 8,1995

