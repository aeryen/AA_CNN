Journal of Machine Learning Research 5 (2004) 941‚Äì973

Submitted 5/03; Revised 10/03; Published 8/04

Boosting as a Regularized Path
to a Maximum Margin ClassiÔ¨Åer

Saharon Rosset
Data Analytics Research Group
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598, USA
Ji Zhu
Department of Statistics
University of Michigan
Ann Arbor, MI 48109, USA

Trevor Hastie
Department of Statistics
Stanford University
Stanford, CA 94305,USA

Editor: Robert Schapire

SROSSET@US.IBM.COM

JIZHU@UMICH.EDU

HASTIE@STAT.STANFORD.EDU

Abstract

In this paper we study boosting methods from a new perspective. We build on recent work by Efron
et al. to show that boosting approximately (and in some cases exactly) minimizes its loss criterion
with an l1 constraint on the coefÔ¨Åcient vector. This helps understand the success of boosting with
early stopping as regularized Ô¨Åtting of the loss criterion. For the two most commonly used crite-
ria (exponential and binomial log-likelihood), we further show that as the constraint is relaxed‚Äîor
equivalently as the boosting iterations proceed‚Äîthe solution converges (in the separable case) to an
‚Äúl1-optimal‚Äù separating hyper-plane. We prove that this l1-optimal separating hyper-plane has the
property of maximizing the minimal l1-margin of the training data, as deÔ¨Åned in the boosting liter-
ature. An interesting fundamental similarity between boosting and kernel support vector machines
emerges, as both can be described as methods for regularized optimization in high-dimensional
predictor space, using a computational trick to make the calculation practical, and converging to
margin-maximizing solutions. While this statement describes SVMs exactly, it applies to boosting
only approximately.
Keywords: boosting, regularized optimization, support vector machines, margin maximization

1. Introduction and Outline

Boosting is a method for iteratively building an additive model

FT (x) =

T(cid:229)

t=1

th jt (x);

(1)

where h jt 2 H ‚Äîa large (but we will assume Ô¨Ånite) dictionary of candidate predictors or ‚Äúweak
learners‚Äù; and h jt is the basis function selected as the ‚Äúbest candidate‚Äù to modify the function at
stage t. The model FT can equivalently be represented by assigning a coefÔ¨Åcient to each dictionary

c(cid:13)2004 Saharon Rosset, Ji Zhu and Trevor Hastie.

a
ROSSET, ZHU AND HASTIE

function h 2 H rather than to the selected h jt ‚Äôs only:

FT (x) =

J(cid:229)

j=1

h j(x) (cid:1) b (T )

j

;

(2)

where J = jH j and b (T )
vector b (T ) as a vector in R J or, equivalently, as the hyper-plane which has b
interpretation will play a key role in our exposition.

t. The ‚Äúb ‚Äù representation allows us to interpret the coefÔ¨Åcient
(T ) as its normal. This

j = (cid:229)

jt = j a

Some examples of common dictionaries are:
(cid:15) The training variables themselves, in which case h j(x) = x j. This leads to our ‚Äúadditive‚Äù
model FT being just a linear model in the original data. The number of dictionary functions
will be J = d, the dimension of x.

(cid:15) Polynomial dictionary of degree p, in which case the number of dictionary functions will be

J =(cid:18) p + d
d (cid:19).

(cid:15) Decision trees with up to k terminal nodes, if we limit the split points to data points (or mid-
way between data points as CART does). The number of possible trees is bounded from
above (trivially) by J (cid:20) (np)k (cid:1) 2k2. Note that regression trees do not Ô¨Åt into our framework,
since they will give J = ¬•

.

The boosting idea was Ô¨Årst introduced by Freund and Schapire (1995), with their AdaBoost
algorithm. AdaBoost and other boosting algorithms have attracted a lot of attention due to their great
success in data modeling tasks, and the ‚Äúmechanism‚Äù which makes them work has been presented
and analyzed from several perspectives. Friedman et al. (2000) develop a statistical perspective,
which ultimately leads to viewing AdaBoost as a gradient-based incremental search for a good
additive model (more speciÔ¨Åcally, it is a ‚Äúcoordinate descent‚Äù algorithm), using the exponential loss
function C(y; F) = exp((cid:0)yF), where y 2 f(cid:0)1;1g. The gradient boosting (Friedman, 2001) and
anyboost (Mason et al., 1999) generic algorithms have used this approach to generalize the boosting
idea to wider families of problems and loss functions. In particular, Friedman et al. (2000) have
pointed out that the binomial log-likelihood loss C(y; F) = log(1 + exp((cid:0)yF)) is a more natural
loss for classiÔ¨Åcation, and is more robust to outliers and misspeciÔ¨Åed data.

A different analysis of boosting, originating in the machine learning community, concentrates on
the effect of boosting on the margins yiF(xi). For example, Schapire et al. (1998) use margin-based
arguments to prove convergence of boosting to perfect classiÔ¨Åcation performance on the training
data under general conditions, and to derive bounds on the generalization error (on future, unseen
data).

In this paper we combine the two approaches, to conclude that gradient-based boosting can be
described, in the separable case, as an approximate margin maximizing process. The view we de-
velop of boosting as an approximate path of optimal solutions to regularized problems also justiÔ¨Åes
early stopping in boosting as specifying a value for ‚Äúregularization parameter‚Äù.

We consider the problem of minimizing non-negative convex loss functions (in particular the
exponential and binomial log-likelihood loss functions) over the training data, with an l1 bound on
the model coefÔ¨Åcients:

ÀÜb (c) = arg min
kb k1(cid:20)c

C(yi; h(xi)0b ):

i

942

(3)

(cid:229)
BOOSTING AS A REGULARIZED PATH

Where h(xi) = [h1(xi); h2(xi); : : : ; hJ(xi)]0 and J = jH j.1

; 8t in (1), with e

Hastie et al. (2001, Chapter 10) have observed that ‚Äúslow‚Äù gradient-based boosting (i.e., we set
t = e
small) tends to follow the penalized path ÀÜb (c) as a function of c, under
some mild conditions on this path. In other words, using the notation of (2), (3), this implies that
kb (c=e ) (cid:0) ÀÜb (c)k vanishes with e , for all (or a wide range of) values of c. Figure 1 illustrates this
equivalence between e -boosting and the optimal solution of (3) on a real-life data set, using squared
error loss as the loss function. In this paper we demonstrate this equivalence further and formally

Lasso

Forward Stagewise

lcavol

lcavol

s
t

i

n
e
c
i
f
f

e
o
C

6

.

0

4

.

0

2

.

0

0

.

0

2

.

0
-

PSfrag replacements

0.0

0.5

1.0
j (c)j

j jÀÜb

1.5

2.0

s
t

i

n
e
c
i
f
f

e
o
C

6

.

0

4

.

0

2

.

0

0

.

0

2

.

0
-

svi

lweight
pgg45

lbph

gleason

age

lcp

2.5

svi

lweight
pgg45

lbph

gleason

age

lcp

0

50

100

150
Iteration

200

250

Figure 1: Exact coefÔ¨Åcient paths(left) for l1-constrained squared error regression and ‚Äúboosting‚Äù

coefÔ¨Åcient paths (right) on the data from a prostate cancer study

state it as a conjecture. Some progress towards proving this conjecture has been made by Efron et al.
(2004), who prove a weaker ‚Äúlocal‚Äù result for the case where C is squared error loss, under some
mild conditions on the optimal path. We generalize their result to general convex loss functions.

Combining the empirical and theoretical evidence, we conclude that boosting can be viewed as

an approximate incremental method for following the l1-regularized path.

We then prove that in the separable case, for both the exponential and logistic log-likelihood

loss functions, ÀÜb (c)=c converges as c ! ¬•

to an ‚Äúoptimal‚Äù separating hyper-plane ÀÜb described by

ÀÜb = arg max
kb k1=1

min

i

yib

0h(xi):

(4)

In other words, ÀÜb maximizes the minimal margin among all vectors with l1-norm equal to 1.2 This
result generalizes easily to other lp-norm constraints. For example, if p = 2, then ÀÜb describes the
optimal separating hyper-plane in the Euclidean sense, i.e., the same one that a non-regularized
support vector machine would Ô¨Ånd.

Combining our two main results, we get the following characterization of boosting:

1. Our notation assumes that the minimum in (3) is unique, which requires some mild assumptions. To avoid notational
complications we use this slightly abusive notation throughout this paper. In Appendix B we give explicit conditions
for uniqueness of this minimum.

2. The margin maximizing hyper-plane in (4) may not be unique, and we show that in that case the limit ÀÜb

is still deÔ¨Åned

and it also maximizes the second minimal margin. See Appendix B.2 for details.

943

a
(cid:229)
ROSSET, ZHU AND HASTIE

e -Boosting can be described as a gradient-descent search, approximately following the
path of l1-constrained optimal solutions to its loss criterion, and converging, in the
separable case, to a ‚Äúmargin maximizer‚Äù in the l1 sense.

Note that boosting with a large dictionary H (in particular if n < J = jH j) guarantees that the data
will be separable (except for pathologies), hence separability is a very mild assumption here.

As in the case of support vector machines in high dimensional feature spaces, the non-regularized
‚Äúoptimal‚Äù separating hyper-plane is usually of theoretical interest only, since it typically represents
an over-Ô¨Åtted model. Thus, we would want to choose a good regularized model. Our results indicate
that Boosting gives a natural method for doing that, by ‚Äústopping early‚Äù in the boosting process. Fur-
thermore, they point out the fundamental similarity between Boosting and SVMs: both approaches
allow us to Ô¨Åt regularized models in high-dimensional predictor space, using a computational trick.
They differ in the regularization approach they take‚Äîexact l2 regularization for SVMs, approximate
l1 regularization for Boosting‚Äî-and in the computational trick that facilitates Ô¨Åtting‚Äîthe ‚Äúkernel‚Äù
trick for SVMs, coordinate descent for Boosting.

1.1 Related Work

Schapire et al. (1998) have identiÔ¨Åed the normalized margins as distance from an l1-normed sep-
arating hyper-plane. Their results relate the boosting iterations‚Äô success to the minimal margin of
the combined model. R¬®atsch et al. (2001b) take this further using an asymptotic analysis of Ad-
aBoost. They prove that the ‚Äúnormalized‚Äù minimal margin, mini yi (cid:229)
tj, is asymptoti-
cally equal for both classes. In other words, they prove that the asymptotic separating hyper-plane is
equally far away from the closest points on either side. This is a property of the margin maximizing
separating hyper-plane as we deÔ¨Åne it. Both papers also illustrate the margin maximizing effects of
AdaBoost through experimentation. However, they both stop short of proving the convergence to
optimal (margin maximizing) solutions.

tht(xi)=(cid:229)

t ja

t a

Motivated by our result, R¬®atsch and Warmuth (2002) have recently asserted the margin-maximizing

properties of e -AdaBoost, using a different approach than the one used in this paper. Their results
relate only to the asymptotic convergence of inÔ¨Ånitesimal AdaBoost, compared to our analysis of
the ‚Äúregularized path‚Äù traced along the way and of a variety of boosting loss functions, which also
leads to a convergence result on binomial log-likelihood loss.

The convergence of boosting to an ‚Äúoptimal‚Äù solution from a loss function perspective has been
analyzed in several papers. R¬®atsch et al. (2001a) and Collins et al. (2000) give results and bounds on
the convergence of training-set loss, (cid:229)
tht(xi)), to its minimum. However, in the separable
case convergence of the loss to 0 is inherently different from convergence of the linear separator to
the optimal separator. Any solution which separates the two classes perfectly can drive the expo-
nential (or log-likelihood) loss to 0, simply by scaling coefÔ¨Åcients up linearly.

iC(yi;(cid:229)

t a

Two recent papers have made the connection between boosting and l1 regularization in a slightly
different context than this paper. Zhang (2003) suggests a ‚Äúshrinkage‚Äù version of boosting which
converges to l1 regularized solutions, while Zhang and Yu (2003) illustrate the quantitative relation-
ship between early stopping in boosting and l1 constraints.

944

BOOSTING AS A REGULARIZED PATH

2. Boosting as Gradient Descent

Generic gradient-based boosting algorithms (Friedman, 2001; Mason et al., 1999) attempt to Ô¨Ånd a
good linear combination of the members of some dictionary of basis functions to optimize a given
loss function over a sample. This is done by searching, at each iteration, for the basis function which
gives the ‚Äústeepest descent‚Äù in the loss, and changing its coefÔ¨Åcient accordingly. In other words,
this is a ‚Äúcoordinate descent‚Äù algorithm in RJ, where we assign one dimension (or coordinate) for
the coefÔ¨Åcient of each dictionary function.

Assume we have data fxi; yign

i=1 with xi 2 Rd, a loss (or cost) function C(y; F), and a set of
dictionary functions fh j(x)g : Rd ! R. Then all of these algorithms follow the same essential
steps:

Algorithm 1 Generic gradient-based boosting algorithm

1. Set b (0) = 0.

2. For t = 1 : T ,

¬∂ C(yi;Fi)

¬∂ Fi

(a) Let Fi = b (t(cid:0)1)0h(xi); i = 1; : : : ; n (the current Ô¨Åt).
(b) Set wi =
(c) Identify jt = argmax j j(cid:229)
(d) Set b (t)

i wih jt (xi)) and b (t)

; i = 1; : : : ; n.

jt = b (t(cid:0)1)

jt (cid:0) a

i wih j(xi)j.

tsign((cid:229)

k = b (t(cid:0)1)

k

; k 6= jt.

Here b (t) is the ‚Äúcurrent‚Äù coefÔ¨Åcient vector and a

t > 0 is the current step size. Notice that (cid:229)

i wih jt (xi) =

i C(yi;Fi)

.

jt
As we mentioned, Algorithm 1 can be interpreted simply as a coordinate descent algorithm in
‚Äúweak learner‚Äù space. Implementation details include the dictionary H of ‚Äúweak learners‚Äù, the loss
function C(y; F), the method of searching for the optimal jt and the way in which a
t is determined.3
For example, the original AdaBoost algorithm uses this scheme with the exponential loss C(y; F) =
exp((cid:0)yF), and an implicit line search to Ô¨Ånd the best a
t once a ‚Äúdirection‚Äù jt has been chosen (see
Hastie et al., 2001; Mason et al., 1999). The dictionary used by AdaBoost in this formulation would
be a set of candidate classiÔ¨Åers, i.e., h j(xi) 2 f(cid:0)1; +1g‚Äîusually decision trees are used in practice.

2.1 Practical Implementation of Boosting

The dictionaries used for boosting are typically very large‚Äîpractically inÔ¨Ånite‚Äîand therefore the
generic boosting algorithm we have presented cannot be implemented verbatim. In particular, it is
not practical to exhaustively search for the maximizer in step 2(c). Instead, an approximate, usually
greedy search is conducted to Ô¨Ånd a ‚Äúgood‚Äù candidate weak learner h jt which makes the Ô¨Årst order
decline in the loss large (even if not maximal among all possible models).

In the common case that the dictionary of weak learners is comprised of decision trees with
up to k nodes, the way AdaBoost and other boosting algorithms solve stage 2(c) is by building a

3. The sign of a

t will always be (cid:0)sign((cid:229)

i wih jt (xi)), since we want the loss to be reduced. In most cases, the dictionary

H is negation closed, and so it can be assumed WLOG that the coefÔ¨Åcients are always positive and increasing

945

¬∂
(cid:229)
¬∂
b
ROSSET, ZHU AND HASTIE

decision tree to a re-weighted version of the data, with the weights jwij. Thus they Ô¨Årst replace step
2(c) with minimization of

jwij1fyi 6= h jt (xi)g;

i

which is easily shown to be equivalent to the original step 2(c). They then use a greedy decision-
tree building algorithm such as CART or C5 to build a k-node decision tree which minimizes this
quantity, i.e., achieves low ‚Äúweighted misclassiÔ¨Åcation error‚Äù on the weighted data. Since the tree is
built greedily‚Äîone split at a time‚Äîit will not be the global minimizer of weighted misclassiÔ¨Åcation
error among all k-node decision trees. However, it will be a good Ô¨Åt for the re-weighted data, and
can be considered an approximation to the optimal tree.

This use of approximate optimization techniques is critical, since much of the strength of the
boosting approach comes from its ability to build additive models in very high-dimensional predic-
tor spaces. In such spaces, standard exact optimization techniques are impractical: any approach
which requires calculation and inversion of Hessian matrices is completely out of the question,
and even approaches which require only Ô¨Årst derivatives, such as coordinate descent, can only be
implemented approximately.

2.2 Gradient-Based Boosting as a Generic Modeling Tool

As Friedman (2001); Mason et al. (1999) mention, this view of boosting as gradient descent allows
us to devise boosting algorithms for any function estimation problem‚Äîall we need is an appro-
priate loss and an appropriate dictionary of ‚Äúweak learners‚Äù. For example, Friedman et al. (2000)
suggested using the binomial log-likelihood loss instead of the exponential loss of AdaBoost for
binary classiÔ¨Åcation, resulting in the LogitBoost algorithm. However, there is no need to limit
boosting algorithms to classiÔ¨Åcation‚ÄîFriedman (2001) applied this methodology to regression es-
timation, using squared error loss and regression trees, and Rosset and Segal (2003) applied it to
density estimation, using the log-likelihood criterion and Bayesian networks as weak learners. Their
experiments and those of others illustrate that the practical usefulness of this approach‚Äîcoordinate
descent in high dimensional predictor space‚Äîcarries beyond classiÔ¨Åcation, and even beyond super-
vised learning.

The view we present in this paper, of coordinate-descent boosting as approximate l1-regularized
Ô¨Åtting, offers some insight into why this approach would be good in general: it allows us to Ô¨Åt regu-
larized models directly in high dimensional predictor space. In this it bears a conceptual similarity
to support vector machines, which exactly Ô¨Åt an l2 regularized model in high dimensional (RKH)
predictor space.

2.3 Loss Functions

The two most commonly used loss functions for boosting classiÔ¨Åcation models are the exponential
and the (minus) binomial log-likelihood:

Exponential :
Loglikelihood : Cl(y; F) = log(1 + exp((cid:0)yF)):

Ce(y; F) = exp((cid:0)yF);

These two loss functions bear some important similarities to each other. As Friedman et al. (2000)
show, the population minimizer of expected loss at point x is similar for both loss functions and is

946

(cid:229)
BOOSTING AS A REGULARIZED PATH

Exponential
Logistic   

‚àí1.5

‚àí1

‚àí0.5

0

0.5

1

1.5

2

2.5

3

2.5

2

1.5

1

0.5

0
‚àí2

Figure 2: The two classiÔ¨Åcation loss functions

given by

ÀÜF(x) = c (cid:1) log(cid:20) P(y = 1jx)
P(y = (cid:0)1jx)(cid:21) ;

where ce = 1=2 for exponential loss and cl = 1 for binomial loss.

More importantly for our purpose, we have the following simple proposition, which illustrates
the strong similarity between the two loss functions for positive margins (i.e., correct classiÔ¨Åca-
tions):

Proposition 1

yF (cid:21) 0 ) 0:5Ce(y; F) (cid:20) Cl(y; F) (cid:20) Ce(y; F):

(5)

In other words, the two losses become similar if the margins are positive, and both behave like
exponentials.
Proof Consider the functions f1(z) = z and f2(z) = log(1+z) for z 2 [0;1]. Then f1(0) = f2(0) = 0,
and

f1(z)
¬∂ z
f2(z)
¬∂ z

=

(cid:17) 1

1

1 + z

(cid:20) 1:

1
2

(cid:20)

Thus we can conclude 0:5 f1(z) (cid:20) f2(z) (cid:20) f1(z). Now set z = exp((cid:0)y f ) and we get the desired
result.

For negative margins the behaviors of Ce and Cl are very different, as Friedman et al. (2000)

have noted. In particular, Cl is more robust against outliers and misspeciÔ¨Åed data.

2.4 Line-Search Boosting vs. e -Boosting
As mentioned above, AdaBoost determines a
this would be

t using a line search. In our notation for Algorithm 1

t = argmina

i

C(yi; Fi + a h jt (xi)):

947

¬∂
¬∂
a
(cid:229)
ROSSET, ZHU AND HASTIE

The alternative approach, suggested by Friedman (2001); Hastie et al. (2001), is to ‚Äúshrink‚Äù all a
to a single small value e . This may slow down learning considerably (depending on how small e
is), but is attractive theoretically: the Ô¨Årst-order theory underlying gradient boosting implies that
the weak learner chosen is the best increment only ‚Äúlocally‚Äù. It can also be argued that this ap-
proach is ‚Äústronger‚Äù than line search, as we can keep selecting the same h jt repeatedly if it remains
optimal and so e -boosting dominates line-search boosting in terms of training error. In practice,
this approach of ‚Äúslowing the learning rate‚Äù usually performs better than line-search in terms of
prediction error as well (see Friedman, 2001). For our purposes, we will mostly assume e
is in-
Ô¨Ånitesimally small, so the theoretical boosting algorithm which results is the ‚Äúlimit‚Äù of a series of
boosting algorithms with shrinking e .

t

In regression terminology, the line-search version is equivalent to forward stage-wise modeling,
infamous in the statistics literature for being too greedy and highly unstable (see Friedman, 2001).
This is intuitively obvious, since by increasing the coefÔ¨Åcient until it saturates we are destroying
‚Äúsignal‚Äù which may help us select other good predictors.

3. lp Margins, Support Vector Machines and Boosting

We now introduce the concept of margins as a geometric interpretation of a binary classiÔ¨Åcation
model. In the context of boosting, this view offers a different understanding of AdaBoost from the
gradient descent view presented above. In the following sections we connect the two views.

3.1 The Euclidean Margin and the Support Vector Machine
Consider a classiÔ¨Åcation model in high dimensional predictor space: F(x) = (cid:229)
j. We say
that the model separates the training data fxi; yign
i=1 if sign(F(xi)) = yi; 8i. From a geometrical
perspective this means that the hyper-plane deÔ¨Åned by F(x) = 0 is a separating hyper-plane for this
data, and we deÔ¨Åne its (Euclidean) margin as

j h j(x)b

m2(b ) = min

i

yiF(xi)
kb k2

:

(6)

The margin-maximizing separating hyper-plane for this data would be deÔ¨Åned by b which max-
imizes m2(b ). Figure 3 shows a simple example of separable data in two dimensions, with its
margin-maximizing separating hyper-plane. The Euclidean margin-maximizing separating hyper-
plane is the (non regularized) support vector machine solution. Its margin maximizing properties
play a central role in deriving generalization error bounds for these models, and form the basis for
a rich literature.

3.2 The l1 Margin and Its Relation to Boosting
Instead of considering the Euclidean margin as in (6) we can deÔ¨Åne an ‚Äúl p margin‚Äù concept as

mp(b ) = min

i

yiF(xi)
kb kp

:

(7)

Of particular interest to us is the case p = 1. Figure 4 shows the l1 margin maximizing separating
hyper-plane for the same simple example as Figure 3. Note the fundamental difference between

948

BOOSTING AS A REGULARIZED PATH

3

2

1

0

‚àí1

‚àí2

‚àí3

‚àí3

‚àí2

‚àí1

0

1

2

3

Figure 3: A simple data example, with two observations from class ‚ÄúO‚Äù and two observations from

class ‚ÄúX‚Äù. The full line is the Euclidean margin-maximizing separating hyper-plane.

3

2

1

0

‚àí1

‚àí2

‚àí3

‚àí3

‚àí2

‚àí1

0

1

2

3

Figure 4: l1 margin maximizing separating hyper-plane for the same data set as Figure 3. The
difference between the diagonal Euclidean optimal separator and the vertical l1 optimal
separator illustrates the ‚Äúsparsity‚Äù effect of optimal l1 separation

949

ROSSET, ZHU AND HASTIE

the two solutions: the l2-optimal separator is diagonal, while the l1-optimal one is vertical. To
understand why this is so we can relate the two margin deÔ¨Ånitions to each other as

yF(x)
kb k1

=

yF(x)
kb k2

(cid:1)

kb k2
kb k1

:

(8)

From this representation we can observe that the l1 margin will tend to be big if the ratio kb k2
is
kb k1
big. This ratio will generally be big if b
is sparse. To see this, consider Ô¨Åxing the l1 norm of the
vector and then comparing the l2 norm of two candidates: one with many small components and the
other‚Äîa sparse one‚Äîwith a few large components and many zero components. It is easy to see that
the second vector will have bigger l2 norm, and hence (if the l2 margin for both vectors is equal) a
bigger l1 margin.

A different perspective on the difference between the optimal solutions is given by a theorem
due to Mangasarian (1999), which states that the l p margin maximizing separating hyper plane
maximizes the lq distance from the closest points to the separating hyper-plane, with 1
q = 1.
Thus the Euclidean optimal separator (p = 2) also maximizes Euclidean distance between the points
and the hyper-plane, while the l1 optimal separator maximizes l¬• distance. This interesting result
gives another intuition why l1 optimal separating hyper-planes tend to be coordinate-oriented (i.e.,
have sparse representations): since l¬• projection considers only the largest coordinate distance,
some coordinate distances may be 0 at no cost of decreased l¬• distance.

p + 1

Schapire et al. (1998) have pointed out the relation between AdaBoost and the l1 margin. They
prove that, in the case of separable data, the boosting iterations increase the ‚Äúboosting‚Äù margin of
the model, deÔ¨Åned as

min

i

yiF(xi)
ka k1

:

(9)

In other words, this is the l1 margin of the model, except that it uses the a
incremental representation
rather than the b ‚Äúgeometric‚Äù representation for the model. The two representations give the same
l1 norm if there is sign consistency, or ‚Äúmonotonicity‚Äù in the coefÔ¨Åcient paths traced by the model,
i.e., if at every iteration t of the boosting algorithm

6= 0 ) sign(a

t) = sign(b

jt

jt ):

(10)

As we will see later, this monotonicity condition will play an important role in the equivalence
between boosting and l1 regularization.

The l1-margin maximization view of AdaBoost presented by Schapire et al. (1998)‚Äîand a
whole plethora of papers that followed‚Äîis important for the analysis of boosting algorithms for
two distinct reasons:

(cid:15) It gives an intuitive, geometric interpretation of the model that AdaBoost is looking for‚Äîa
model which separates the data well in this l1-margin sense. Note that the view of boosting as
gradient descent in a loss criterion doesn‚Äôt really give the same kind of intuition: if the data
is separable, then any model which separates the training data will drive the exponential or
binomial loss to 0 when scaled up:

m1(b ) > 0 =) (cid:229)

C(yi; db

0xi) ! 0 as d ! ¬•

:

i

950

b
BOOSTING AS A REGULARIZED PATH

(cid:15) The l1-margin behavior of a classiÔ¨Åcation model on its training data facilitates generation
of generalization (or prediction) error bounds, similar to those that exist for support vector
machines (Schapire et al., 1998). The important quantity in this context is not the margin but
the ‚Äúnormalized‚Äù margin, which considers the ‚Äúconjugate norm‚Äù of the predictor vectors:

yib

0h(xi)
kb k1kh(xi)k¬•

:

When the dictionary we are using is comprised of classiÔ¨Åers then kh(xi)k¬• (cid:17) 1 always and
thus the l1 margin is exactly the relevant quantity. The error bounds described by Schapire
et al. (1998) allow using the whole l1 margin distribution, not just the minimal margin. How-
ever, boosting‚Äôs tendency to separate well in the l1 sense is a central motivation behind their
results.

From a statistical perspective, however, we should be suspicious of margin-maximization as a
method for building good prediction models in high dimensional predictor space. Margin maxi-
mization in high dimensional space is likely to lead to over-Ô¨Åtting and bad prediction performance.
This has been observed in practice by many authors, in particular Breiman (1999). Our results in
the next two sections suggest an explanation based on model complexity: margin maximization is
the limit of parametric regularized optimization models, as the regularization vanishes, and the reg-
ularized models along the path may well be superior to the margin maximizing ‚Äúlimiting‚Äù model, in
terms of prediction performance. In Section 7 we return to discuss these issues in more detail.

4. Boosting as Approximate Incremental l1 Constrained Fitting
In this section we introduce an interpretation of the generic coordinate-descent boosting algorithm
as tracking a path of approximate solutions to l1-constrained (or equivalently, regularized) versions
of its loss criterion. This view serves our understanding of what boosting does, in particular the
connection between early stopping in boosting and regularization. We will also use this view to
get a result about the asymptotic margin-maximization of regularized classiÔ¨Åcation models, and
by analogy of classiÔ¨Åcation boosting. We build on ideas Ô¨Årst presented by Hastie et al. (2001,
Chapter 10) and Efron et al. (2004).

Given a convex non-negative loss criterion C((cid:1); (cid:1)), consider the 1-dimensional path of optimal

solutions to l1 constrained optimization problems over the training data:

ÀÜb (c) = arg min
kb k1(cid:20)c

i

C(yi; h(xi)0b ):

(11)

As c varies, we get that ÀÜb (c) traces a 1-dimensional ‚Äúoptimal curve‚Äù through RJ. If an optimal
solution for the non-constrained problem exists and has Ô¨Ånite l1 norm c0, then obviously ÀÜb (c) =
ÀÜb (c0) = ÀÜb
; 8c > c0. in the case of separable 2-class data, using either Ce or Cl, there is no Ô¨Ånite-
norm optimal solution. Rather, the constrained solution will always have k ÀÜb (c)k1 = c.

iterations. This will give an a

A different way of building a solution which has l1 norm c, is to run our e -boosting algorithm
(c=e ) vector which has l1 norm exactly c. For the norm of the
for c=e
geometric representation b (c=e ) to also be equal to c, we need the monotonicity condition (10) to
hold as well. This condition will play a key role in our exposition.

We are going to argue that the two solution paths ÀÜb (c) and b (c=e ) are very similar for e ‚Äúsmall‚Äù.
Let us start by observing this similarity in practice. Figure 1 in the introduction shows an example of

951

(cid:229)
ROSSET, ZHU AND HASTIE

 Lasso

 Stagwise

0
0
5

0

‚Ä¢
‚Ä¢
3
9

‚Ä¢
4

‚Ä¢
7

‚Ä¢
2

‚Ä¢
‚Ä¢
10
5

‚Ä¢
8

‚Ä¢
‚Ä¢
6
1



0
0
5
-

9

3
6

4

8
7
10
1

2

5

0
0
5

0

‚Ä¢
‚Ä¢
3
9

‚Ä¢
4

‚Ä¢
7

‚Ä¢
2

‚Ä¢
‚Ä¢
10
5

‚Ä¢
8

‚Ä¢
‚Ä¢
1
6

0
0
5
-

9

3
6

4

8
7
10
1

2

5

0

2000
1000
	

 


3000

0

2000
1000
	

 


3000

Figure 5: Another example of the equivalence between the Lasso optimal solution path (left) and
e -boosting with squared error loss. Note that the equivalence breaks down when the path
of variable 7 becomes non-monotone

this similarity for squared error loss Ô¨Åtting with l1 (lasso) penalty. Figure 5 shows another example
in the same mold, taken from Efron et al. (2004). The data is a diabetes study and the ‚Äúdictionary‚Äù
used is just the original 10 variables. The panel on the left shows the path of optimal l1-constrained
solutions ÀÜb (c) and the panel on the right shows the e -boosting path with the 10-dimensional dictio-
nary (the total number of boosting iterations is about 6000). The 1-dimensional path through R10
is described by 10 coordinate curves, corresponding to each one of the variables. The interesting
phenomenon we observe is that the two coefÔ¨Åcient traces are not completely identical. Rather, they
agree up to the point where variable 7 coefÔ¨Åcient path becomes non monotone, i.e., it violates (10)
(this point is where variable 8 comes into the model, see the arrow on the right panel). This example
illustrates that the monotonicity condition‚Äîand its implication that ka k1 = kb k1‚Äîis critical for the
equivalence between e -boosting and l1-constrained optimization.

The two examples we have seen so far have used squared error loss, and we should ask ourselves
whether this equivalence stretches beyond this loss. Figure 6 shows a similar result, but this time for
the binomial log-likelihood loss, Cl. We used the ‚Äúspam‚Äù data set, taken from the UCI repository
(Blake and Merz, 1998). We chose only 5 predictors of the 57 to make the plots more interpretable
and the computations more accommodating. We see that there is a perfect equivalence between the
exact constrained solution (i.e., regularized logistic regression) and e -boosting in this case, since the
paths are fully monotone.

To justify why this observed equivalence is not surprising, let us consider the following ‚Äúl1-
increment to a given

locally optimal monotone direction‚Äù problem of Ô¨Ånding the best monotone e
model b 0:

min C(b )
s:t:

kb k1 (cid:0) kb 0k1 (cid:20) e ;
jb j (cid:23) jb 0j (component-wise):

952

(12)


BOOSTING AS A REGULARIZED PATH

Exact constrained solution

e ‚àíStagewise

s
e
u
a
v
 

l

6

5

4

3

2

1

0

‚àí1

‚àí2

0

2

4

||b

6
||1

8

10

12

s
e
u
a
v
 

l

6

5

4

3

2

1

0

‚àí1

‚àí2

0

2

4

||b

6
||1

8

10

12

Figure 6: Exact coefÔ¨Åcient paths (left) for l1-constrained logistic regression and boosting coefÔ¨Åcient
paths (right) with binomial log-likelihood loss on Ô¨Åve variables from the ‚Äúspam‚Äù data set.
The boosting path was generated using e = 0:003 and 7000 iterations.

Here we use C(b ) as shorthand for (cid:229)

iC(yi; h(xi)0b ). A Ô¨Årst order Taylor expansion gives us

C(b ) = C(b 0) + (cid:209) C(b 0)0(b (cid:0) b 0) + O(e 2):

And given the l1 constraint on the increase in kb k1, it is easy to see that a Ô¨Årst-order optimal solution
(and therefore an optimal solution as e ! 0) will make a ‚Äúcoordinate descent‚Äù step, i.e.

j 6= b 0; j ) j(cid:209) C(b 0) jj = max

k

j(cid:209) C(b 0)kj;

assuming the signs match, i.e., sign(b 0 j) = (cid:0)sign((cid:209) C(b 0) j).

So we get that if the optimal solution to (12) without the monotonicity constraint happens to be
monotone, then it is equivalent to a coordinate descent step. And so it is reasonable to expect that if
the optimal l1 regularized path is monotone (as it indeed is in Figures 1,6), then an ‚ÄúinÔ¨Ånitesimal‚Äù
e -boosting algorithm would follow the same path of solutions. Furthermore, even if the optimal
path is not monotone, we can still use the formulation (12) to argue that e -boosting would tend to
follow an approximate l1-regularized path. The main difference between the e -boosting path and
the true optimal path is that it will tend to ‚Äúdelay‚Äù becoming non-monotone, as we observe for
variable 7 in Figure 5. To understand this speciÔ¨Åc phenomenon would require analysis of the true
optimal path, which falls outside the scope of our discussion‚ÄîEfron et al. (2004) cover the subject
for squared error loss, and their discussion applies to any continuously differentiable convex loss,
using second-order approximations.

We can employ this understanding of the relationship between boosting and l1 regularization
to construct lp boosting algorithms by changing the coordinate-selection criterion in the coordinate
descent algorithm. We will get back to this point in Section 7, where we design an ‚Äúl2 boosting‚Äù
algorithm.

The experimental evidence and heuristic discussion we have presented lead us to the following

conjecture which connects slow boosting and l1-regularized optimization:

953

b
b
b
ROSSET, ZHU AND HASTIE

Conjecture 2 Consider applying the e -boosting algorithm to any convex loss function, generating
a path of solutions b (e )(t). Then if the optimal coefÔ¨Åcient paths are monotone 8c < c0, i.e., if
8 j; jÀÜb (c) jj is non-decreasing in the range c < c0, then

b (e )(c0=e ) = ÀÜb (c0):

lime !0

Efron et al. (2004, Theorem 2) prove a weaker ‚Äúlocal‚Äù result for the case of squared error loss
only. We generalize their result to any convex loss. However this result still does not prove the
‚Äúglobal‚Äù convergence which the conjecture claims, and the empirical evidence implies. For the sake
of brevity and readability, we defer this proof, together with concise mathematical deÔ¨Ånition of the
different types of convergence, to appendix A.

In the context of ‚Äúreal-life‚Äù boosting, where the number of basis functions is usually very large,
and making e small enough for the theory to apply would require running the algorithm forever,
these results should not be considered directly applicable. Instead, they should be taken as an intu-
itive indication that boosting‚Äîespecially the e version‚Äîis, indeed, approximating optimal solutions
to the constrained problems it encounters along the way.

5. lp-Constrained ClassiÔ¨Åcation Loss Functions
Having established the relation between boosting and l1 regularization, we are going to turn our
attention to the regularized optimization problem. By analogy, our results will apply to boosting
as well. We concentrate on Ce and Cl, the two classiÔ¨Åcation losses deÔ¨Åned above, and the solution
paths of their lp constrained versions:

ÀÜb (p)(c) = arg min
kb kp(cid:20)c

i

C(yi;b

0h(xi)):

(13)

where C is either Ce or Cl. As we discussed below Equation (11), if the training data is separable in
span(H ), then we have k ÀÜb (p)(c)kp = c for all values of c. Consequently

ÀÜb (p)(c)

k

c

kp = 1:

We may ask what are the convergence points of this sequence as c ! ¬•
shows that these convergence points describe ‚Äúl p-margin maximizing‚Äù separating hyper-planes.
Theorem 3 Assume the data is separable, i.e., 9b s:t:8i; yib
Then for both Ce and Cl, every convergence point of
separating hyper-plane.
If the lp-margin-maximizing separating hyper-plane is unique, then it is the unique convergence
points, i.e.

ÀÜb (c)
c corresponds to an lp-margin-maximizing

. The following theorem

0h(xi) > 0.

= arg max
kb kp=1

min

i

yib

0h(xi):

(14)

ÀÜb (p) = lim
c!¬•

ÀÜb (p)(c)

c

Proof This proof applies to both Ce and Cl, given the property in (5). Consider two separating
candidates b 1 and b 2 such that kb 1kp = kb 2kp = 1. Assume that b 1 separates better, i.e.

m1 := min

i

yib

0
1h(xi) > m2 := min

i

yib

0
2h(xi) > 0:

Then we have the following simple lemma:

954

(cid:229)
BOOSTING AS A REGULARIZED PATH

Lemma 4 There exists some D = D(m1; m2) such that 8d > D, db 1 incurs smaller loss than db 2,
in other words:

C(yi; db

1h(xi)) < (cid:229)

0

C(yi; db

0
2h(xi)):

i

i

Given this lemma, we can now prove that any convergence point of
c must be an lp-margin
maximizing separator. Assume b (cid:3) is a convergence point of
. Denote its minimal margin on
the data by m(cid:3). If the data is separable, clearly m(cid:3) > 0 (since otherwise the loss of db (cid:3) does not
even converge to 0 as d ! ¬•

Now, assume some Àúb with kÀúb kp = 1 has bigger minimal margin Àúm > m(cid:3). By continuity of the

ÀÜb (p)(c)

).

c

ÀÜb (p)(c)

minimal margin in b

, there exists some open neighborhood of b (cid:3)
: kb (cid:0) b (cid:3)k2 < d g

Nb (cid:3) = fb

and an e > 0, such that

yib

0h(xi) < Àúm (cid:0) e ; 8b 2 Nb (cid:3):

min

i

Now by the lemma we get that there exists some D = D( Àúm; Àúm (cid:0) e ) such that d Àúb
for any d > D; b 2 Nb (cid:3). Therefore b (cid:3) cannot be a convergence point of

loss than db

incurs smaller
ÀÜb (p)(c)

.

c

We conclude that any convergence point of the sequence

c must be an lp-margin maximiz-
ing separator. If the margin maximizing separator is unique then it is the only possible convergence
point, and therefore

ÀÜb (p)(c)

ÀÜb (p) = lim
c!¬•

ÀÜb (p)(c)

c

= arg max
kb kp=1

min

i

yib

0h(xi):

Proof of Lemma Using (5) and the deÔ¨Ånition of Ce, we get for both loss functions:

C(yi; db

i

0
1h(xi)) (cid:20) nexp((cid:0)d (cid:1) m1):

Now, since b 1 separates better, we can Ô¨Ånd our desired

D = D(m1; m2) =

logn + log2

m1 (cid:0) m2

such that

8d > D; nexp((cid:0)d (cid:1) m1) < 0:5exp((cid:0)d (cid:1) m2):

And using (5) and the deÔ¨Ånition of Ce again we can write
C(yi; db

0:5exp((cid:0)d (cid:1) m2) (cid:20) (cid:229)

i

0
2h(xi)):

Combining these three inequalities we get our desired result:

8d > D;

i

C(yi; db

1h(xi)) (cid:20) (cid:229)

0

C(yi; db

0
2h(xi)):

i

955

(cid:229)
(cid:229)
(cid:229)
ROSSET, ZHU AND HASTIE

We thus conclude that if the lp-margin maximizing separating hyper-plane is unique, the nor-
malized constrained solution converges to it. In the case that the margin maximizing separating
hyper-plane is not unique, we can in fact prove a stronger result, which indicates that the limit of
the regularized solutions would then be determined by the second smallest margin, then by the third
and so on. This result is mainly of technical interest and we prove it in Appendix B, Section 2.

5.1 Implications of Theorem 3

We now brieÔ¨Çy discuss the implications of this theorem for boosting and logistic regression.

5.1.1 BOOSTING IMPLICATIONS

Combined with our results from Section 4, Theorem 3 indicates that the normalized boosting path
‚Äîwith either Ce or Cl used as loss‚Äî‚Äúapproximately‚Äù converges to a separating hyper-plane

b (t)

(cid:229) u(cid:20)t a u
ÀÜb
, which attains

max
kb k1=1

min

i

yib

0h(xi) = max
kb k1=1

kb k2 min

i

yidi;

(15)

where di is the (signed) Euclidean distance from the training point i to the separating hyper-plane. In
other words, it maximizes Euclidean distance scaled by an l2 norm. As we have mentioned already,
this implies that the asymptotic boosting solution will tend to be sparse in representation, due to the
fact that for Ô¨Åxed l1 norm, the l2 norm of vectors that have many 0 entries will generally be larger.
In fact, under rather mild conditions, the asymptotic solution ÀÜb = limc!¬•
ÀÜb (1)(c)=c, will have at
most n (the number of observations) non-zero coefÔ¨Åcients, if we use either Cl or Ce as the loss. See
Appendix B, Section 1 for proof.

5.1.2 LOGISTIC REGRESSION IMPLICATIONS

Recall, that the logistic regression (maximum likelihood) solution is undeÔ¨Åned if the data is sepa-
rable in the Euclidean space spanned by the predictors. Theorem 3 allows us to deÔ¨Åne a logistic
regression solution for separable data, as follows:

1. Set a high constraint value cmax
2. Find ÀÜb (p)(cmax), the solution to the logistic regression problem subject to the constraint kb k p (cid:20)
cmax. The problem is convex for any p (cid:21) 1 and differentiable for any p > 1, so interior point
methods can be used to solve this problem.

3. Now you have (approximately) the lp-margin maximizing solution for this data, described by

ÀÜb (p)(cmax)

cmax

:

This is a solution to the original problem in the sense that it is, approximately, the convergence
point of the normalized lp-constrained solutions, as the constraint is relaxed.

956

BOOSTING AS A REGULARIZED PATH

Of course, with our result from Theorem 3 it would probably make more sense to simply Ô¨Ånd the
optimal separating hyper-plane directly‚Äîthis is a linear programming problem for l1 separation and
a quadratic programming problem for l2 separation. We can then consider this optimal separator as
a logistic regression solution for the separable data.

6. Examples

We now apply boosting to several data sets and interpret the results in light of our regularization and
margin-maximization view.

6.1 Spam Data Set

We now know if the data are separable and we let boosting run forever, we will approach the same
‚Äúoptimal‚Äù separator for both Ce and Cl. However if we stop early‚Äîor if the data is not separable‚Äî
the behavior of the two loss functions may differ signiÔ¨Åcantly, since Ce weighs negative margins
exponentially, while Cl is approximately linear in the margin for large negative margins (see Fried-
man et al., 2000). Consequently, we can expect Ce to concentrate more on the ‚Äúhard‚Äù training data,
in particular in the non-separable case. Figure 7 illustrates the behavior of e -boosting with both

Minimal margins

Test error

exponential
logistic   
AdaBoost   

0.2

0

‚àí0.2

‚àí0.4

‚àí0.6

‚àí0.8

0.095

0.09

0.085

0.08

0.075

0.07

0.065

0.06

0.055

0.05

r
o
r
r
e
 
t
s
e
t

exponential
logistic   
AdaBoost   

i

n
g
r
a
m

 
l
a
m
n
m

i

i

‚àí1

100

101

||b

||1

102

103

0.045

100

101

||b

||1

102

103

Figure 7: Behavior of boosting with the two loss functions on spam data set

loss functions, as well as that of AdaBoost, on the spam data set (57 predictors, binary response).
We used 10 node trees and e = 0:1. The left plot shows the minimal margin as a function of the
l1 norm of the coefÔ¨Åcient vector kb k1. Binomial loss creates a bigger minimal margin initially,
but the minimal margins for both loss functions are converging asymptotically. AdaBoost initially
lags behind but catches up nicely and reaches the same minimal margin asymptotically. The right
plot shows the test error as the iterations proceed, illustrating that both e -methods indeed seem to
over-Ô¨Åt eventually, even as their ‚Äúseparation‚Äù (minimal margin) is still improving. AdaBoost did not
signiÔ¨Åcantly over-Ô¨Åt in the 1000 iterations it was allowed to run, but it obviously would have if it
were allowed to run on.

We should emphasize that the comparison between AdaBoost and e -boosting presented consid-
ers as a basis for comparison the l1 norm, not the number of iterations. In terms of computational
complexity, as represented by the number of iterations, AdaBoost reaches both a large minimal mar-

957

ROSSET, ZHU AND HASTIE

gin and good prediction performance much more quickly than the ‚Äúslow boosting‚Äù approaches, as
AdaBoost tends to take larger steps.

6.2 Simulated Data

To make a more educated comparison and more compelling visualization, we have constructed an
example of separation of 2-dimensional data using a 8-th degree polynomial dictionary (45 func-
tions). The data consists of 50 observations of each class, drawn from a mixture of Gaussians, and
presented in Figure 8. Also presented, in the solid line, is the optimal l1 separator for this data in
this dictionary (easily calculated as a linear programming problem - note the difference from the l2
optimal decision boundary, presented in Section 7.1, Figure 11 ). The optimal l1 separator has only
12 non-zero coefÔ¨Åcients out of 45.

2

1.5

1

0.5

0

‚àí0.5

‚àí1

‚àí1.5

‚àí2
‚àí2

optimal          
boost 105 iter  
boost 3*106 iter

‚àí1.5

‚àí1

‚àí0.5

0

0.5

1

1.5

2

Figure 8: ArtiÔ¨Åcial data set with l1-margin maximizing separator (solid), and boosting models af-
ter 105 iterations (dashed) and 106 iterations (dotted) using e = 0:001. We observe the
convergence of the boosting separator to the optimal separator

We ran an e -boosting algorithm on this data set, using the logistic log-likelihood loss Cl, with
e = 0:001, and Figure 8 shows two of the models generated after 105 and 3 (cid:1) 106 iterations. We see
that the models seem to converge to the optimal separator. A different view of this convergence is
given in Figure 9, where we see two measures of convergence: the minimal margin (left, maximum
value obtainable is the horizontal line) and the l1-norm distance between the normalized models
(right), given by

ÀÜb

j (cid:0)

j (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

;

b (t)
j

kb (t)k1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

where ÀÜb

is the optimal separator with l1 norm 1 and b (t) is the boosting model after t iterations.

We can conclude that on this simple artiÔ¨Åcial example we get nice convergence of the logistic-

boosting model path to the l1-margin maximizing separating hyper-plane.

We can also use this example to illustrate the similarity between the boosted path and the path

of l1 optimal solutions, as we have discussed in Section 4.

958

(cid:229)
BOOSTING AS A REGULARIZED PATH

i

n
g
r
a
m

 
l

i

a
m
n
M

i

0.01

0

‚àí0.01

‚àí0.02

‚àí0.03

‚àí0.04

‚àí0.05

101

102

||b

||1

103

104

2

1.8

1.6

1.4

1.2

1

e
c
n
e
r
e

f
f
i

d

 

1

l

0.8

0.6

0.4

0.2

0
101

102

||b

||1

103

104

Figure 9: Two measures of convergence of boosting model path to optimal l1 separator: minimal
margin (left) and l1 distance between the normalized boosting coefÔ¨Åcient vector and the
optimal model (right)

l
 norm:    20
1

l
 norm:   350
1

l
 norm:  2701
1

l
 norm:  5401
1

Figure 10: Comparison of decision boundary of boosting models (broken) and of optimal con-

strained solutions with same norm (full)

Figure 10 shows the class decision boundaries for 4 models generated along the boosting path,
compared to the optimal solutions to the constrained ‚Äúlogistic regression‚Äù problem with the same
bound on the l1 norm of the coefÔ¨Åcient vector. We observe the clear similarities in the way the
solutions evolve and converge to the optimal l1 separator. The fact that they differ (in some cases
signiÔ¨Åcantly) is not surprising if we recall the monotonicity condition presented in Section 4 for
exact correspondence between the two model paths. In this case if we look at the coefÔ¨Åcient paths

959

ROSSET, ZHU AND HASTIE

(not shown), we observe that the monotonicity condition is consistently violated in the low norm
ranges, and hence we can expect the paths to be similar in spirit but not identical.

7. Discussion

We can now summarize what we have learned about boosting from the previous sections:

(cid:15) Boosting approximately follows the path of l1-regularized models for its loss criterion

(cid:15) If the loss criterion is the exponential loss of AdaBoost or the binomial log-likelihood loss
of logistic regression, then the l1 regularized model converges to an l1-margin maximizing
separating hyper-plane, if the data are separable in the span of the weak learners

We may ask, which of these two points is the key to the success of boosting approaches. One
empirical clue to answering this question, can be found in Breiman (1999), who programmed an
algorithm to directly maximize the margins. His results were that his algorithm consistently got
signiÔ¨Åcantly higher minimal margins than AdaBoost on many data sets (and, in fact, a ‚Äúhigher‚Äù
margin distribution beyond the minimal margin), but had slightly worse prediction performance. His
conclusion was that margin maximization is not the key to AdaBoost‚Äôs success. From a statistical
perspective we can embrace this conclusion, as reÔ¨Çecting the importance of regularization in high-
dimensional predictor space. By our results from the previous sections, ‚Äúmargin maximization‚Äù
can be viewed as the limit of parametric regularized models, as the regularization vanishes.4 Thus
we would generally expect the margin maximizing solutions to perform worse than regularized
models. In the case of boosting, regularization would correspond to ‚Äúearly stopping‚Äù of the boosting
algorithm.

7.1 Boosting and SVMs as Regularized Optimization in High-dimensional Predictor Spaces

Our exposition has led us to view boosting as an approximate way to solve the regularized optimiza-
tion problem

(16)

C(yi;b

0h(xi)) + l kb k1

min

i

which converges as l ! 0 to ÀÜb (1), if our loss is Ce or Cl. In general, the loss C can be any convex
differentiable loss and should be deÔ¨Åned to match the problem domain.

Support vector machines can be described as solving the regularized optimization problem (see

Friedman et al., 2000, Chapter 12)

(1 (cid:0) yib

0h(xi))+ + l kb k2

2

min

i

(17)

which ‚Äúconverges‚Äù as l ! 0 to the non-regularized support vector machine solution, i.e., the optimal
Euclidean separator, which we denoted by ÀÜb (2).

An interesting connection exists between these two approaches, in that they allow us to solve

the regularized optimization problem in high dimensional predictor space:

4. It can be argued that margin-maximizing models are still ‚Äúregularized‚Äù in some sense, as they minimize a norm
criterion among all separating models. This is arguably the property which still allows them to generalize reasonably
well in many cases.

960

b
(cid:229)
b
(cid:229)
BOOSTING AS A REGULARIZED PATH

(cid:15) We are able to solve the l1- regularized problem approximately in very high dimension via
boosting by applying the ‚Äúapproximate coordinate descent‚Äù trick of building a decision tree
(or otherwise greedily selecting a weak learner) based on re-weighted versions of the data.

(cid:15) Support vector machines facilitate a different trick for solving the regularized optimization
problem in high dimensional predictor space: the ‚Äúkernel trick‚Äù. If our dictionary H spans
a Reproducing Kernel Hilbert Space, then RKHS theory tells us we can Ô¨Ånd the regularized
solutions by solving an n-dimensional problem, in the space spanned by the kernel represen-
ters fK(xi;x)g. This fact is by no means limited to the hinge loss of (17), and applies to any
convex loss. We concentrate our discussion on SVM (and hence hinge loss) only since it is
by far the most common and well-known application of this result.

So we can view both boosting and SVM as methods that allow us to Ô¨Åt regularized models in
high dimensional predictor space using a computational ‚Äúshortcut‚Äù. The complexity of the model
built is controlled by regularization. These methods are distinctly different than traditional statistical
approaches for building models in high dimension, which start by reducing the dimensionality of
the problem so that standard tools (e.g., Newton‚Äôs method) can be applied to it, and also to make
over-Ô¨Åtting less of a concern. While the merits of regularization without dimensionality reduction‚Äî
like Ridge regression or the Lasso‚Äîare well documented in statistics, computational issues make it
impractical for the size of problems typically solved via boosting or SVM, without computational
tricks.

We believe that this difference may be a signiÔ¨Åcant reason for the enduring success of boosting

and SVM in data modeling, i.e.:

Working in high dimension and regularizing is statistically preferable to a two-step
procedure of Ô¨Årst reducing the dimension, then Ô¨Åtting a model in the reduced space.

It is also interesting to consider the differences between the two approaches, in the loss (Ô¨Çexible
vs. hinge loss), the penalty (l1 vs. l2), and the type of dictionary used (usually trees vs. RKHS).
These differences indicate that the two approaches will be useful for different situations. For ex-
ample, if the true model has a sparse representation in the chosen dictionary, then l1 regularization
may be warranted; if the form of the true model facilitates description of the class probabilities via
a logistic-linear model, then the logistic loss Cl is the best loss to use, and so on.

The computational tricks for both SVM and boosting limit the kind of regularization that can
be used for Ô¨Åtting in high dimensional space. However, the problems can still be formulated and
solved for different regularization approaches, as long as the dimensionality is low enough:

(cid:15) Support vector machines can be Ô¨Åtted with an l1 penalty, by solving the 1-norm version of the
SVM problem, equivalent to replacing the l2 penalty in (17) with an l1 penalty. In fact, the 1-
norm SVM is used quite widely, because it is more easily solved in the ‚Äúlinear‚Äù, non-RKHS,
situation (as a linear program, compared to the standard SVM which is a quadratic program)
and tends to give sparser solutions in the primal domain.

(cid:15) Similarly, we describe below an approach for developing a ‚Äúboosting‚Äù algorithm for Ô¨Åtting

approximate l2 regularized models.

Both of these methods are interesting and potentially useful. However they lack what is arguably
the most attractive property of the ‚Äústandard‚Äù boosting and SVM algorithms: a computational trick
to allow Ô¨Åtting in high dimensions.

961

ROSSET, ZHU AND HASTIE

7.1.1 AN l2 BOOSTING ALGORITHM
We can use our understanding of the relation of boosting to regularization and Theorem 3 to for-
mulate lp-boosting algorithms, which will approximately follow the path of l p-regularized solutions
and converge to the corresponding lp-margin maximizing separating hyper-planes. Of particular
interest is the l2 case, since Theorem 3 implies that l2-constrained Ô¨Åtting using Cl or Ce will build a
regularized path to the optimal separating hyper-plane in the Euclidean (or SVM) sense.

To construct an l2 boosting algorithm, consider the ‚Äúequivalent‚Äù optimization problem (12), and

change the step-size constraint to an l2 constraint:

kb k2 (cid:0) kb 0k2 (cid:20) e :

It is easy to see that the Ô¨Årst order solution to this problem entails selecting for modiÔ¨Åcation the
coordinate which maximizes

(cid:209) C(b 0)k

b 0;k

and that subject to monotonicity, this will lead to a correspondence to the locally l2-optimal direc-
tion.

Following this intuition, we can construct an l2 boosting algorithm by changing only step 2(c)

of our generic boosting algorithm of Section 2 to

2(c)* Identify jt which maximizes j(cid:229)

i wih jt (xi)j
jb

jt j

:

Note that the need to consider the current coefÔ¨Åcient (in the denominator) makes the l2 algorithm
appropriate for toy examples only. In situations where the dictionary of weak learner is prohibitively
large, we will need to Ô¨Ågure out a trick like the one we presented in Section 2.1, to allow us to make
an approximate search for the optimizer of step 2(c)*.

Another problem in applying this algorithm to large problems is that we never choose the same
dictionary function twice, until all have non-0 coefÔ¨Åcients. This is due to the use of the l2 penalty,
where the current coefÔ¨Åcient value affects the rate at which the penalty term is increasing. In par-
ticular, if b
j = 0 then increasing it causes the penalty term kb k2 to increase at rate 0, to Ô¨Årst order
(which is all the algorithm is considering).

The convergence of our l2 boosting algorithm on the artiÔ¨Åcial data set of Section 6.2 is illustrated
in Figure 11. We observe that the l2 boosting models do indeed approach the optimal l2 separator.
It is interesting to note the signiÔ¨Åcant difference between the optimal l2 separator as presented in
Figure 11 and the optimal l1 separator presented in Section 6.2 (Figure 8).

8. Summary and Future Work

In this paper we have introduced a new view of boosting in general, and two-class boosting in
particular, comprised of two main points:

(cid:15) We have generalized results from Efron et al. (2004) and Hastie et al. (2001), to describe

boosting as approximate l1-regularized optimization.

(cid:15) We have shown that the exact l1-regularized solutions converge to an l1-margin maximizing

separating hyper-plane.

962

BOOSTING AS A REGULARIZED PATH

2

1.5

1

0.5

0

‚àí0.5

‚àí1

‚àí1.5

‚àí2
‚àí2

optimal          
boost 5*106 iter
boost 108 iter  

‚àí1.5

‚àí1

‚àí0.5

0

0.5

1

1.5

2

Figure 11: ArtiÔ¨Åcial data set with l2-margin maximizing separator (solid), and l2-boosting models
after 5 (cid:3)106 iterations (dashed) and 108 iterations (dotted) using e = 0:0001. We observe
the convergence of the boosting separator to the optimal separator

We hope our results will help in better understanding how and why boosting works. It is an interest-
ing and challenging task to separate the effects of the different components of a boosting algorithm:

(cid:15) Loss criterion

(cid:15) Dictionary and greedy learning method

(cid:15) Line search / slow learning

and relate them to its success in different scenarios. The implicit l1 regularization in boosting
may also contribute to its success, as it has been shown that in some situations l1 regularization is
inherently superior to others (see Donoho et al., 1995).

An important issue when analyzing boosting is over-Ô¨Åtting in the noisy data case. To deal with
over-Ô¨Åtting, R¬®atsch et al. (2001b) propose several regularization methods and generalizations of the
original AdaBoost algorithm to achieve a soft margin by introducing slack variables. Our results
indicate that the models along the boosting path can be regarded as l1 regularized versions of the
optimal separator, hence regularization can be done more directly and naturally by stopping the
boosting iterations early. It is essentially a choice of the l1 constraint parameter c.

Many other questions arise from our view of boosting. Among the issues to be considered:

(cid:15) Is there a similar ‚Äúseparator‚Äù view of multi-class boosting? We have some tentative results to

indicate that this might be the case if the boosting problem is formulated properly.

(cid:15) Can the constrained optimization view of boosting help in producing generalization error

bounds for boosting that would be more tight than the current existing ones?

963

ROSSET, ZHU AND HASTIE

Acknowledgments

We thank Stephen Boyd, Brad Efron, Jerry Friedman, Robert Schapire and Rob Tibshirani for
helpful discussions. We thank the referees for their thoughtful and useful comments. This work
was partially supported by Stanford graduate fellowship, grant DMS-0204612 from the National
Science Foundation, and grant ROI-CA-72028-01 from the National Institutes of Health.

Appendix A. Local Equivalence of InÔ¨Ånitesimal e -Boosting and l1-Constrained

Optimization

As before, we assume we have a set of training data (x1; y1); (x2; y2); : : : (xn; yn), a smooth cost
function C(y; F), and a set of basis functions (h1(x); h2(x); : : :hJ(x)).

We denote by ÀÜb (s) be the optimal solution of the l1-constrained optimization problem:

min

C(yi; h(xi)0b )

n(cid:229)
i=1
kb k1 (cid:20) s:

(18)

subject to

(19)
Suppose we initialize the e -boosting version of Algorithm 1, as described in Section 2, at ÀÜb (s) and
run the algorithm for T steps. Let b (T ) denote the coefÔ¨Åcients after T steps.
The ‚Äúglobal convergence‚Äù Conjecture 2 in Section 4 implies that 8D s > 0:

b (D s=e ) ! ÀÜb (s + D s) as e ! 0

under some mild assumptions. Instead of proving this ‚Äúglobal‚Äù result, we show here a ‚Äúlocal‚Äù result
by looking at the derivative of ÀÜb (s). Our proof builds on the proof by Efron et al. (2004, Theorem 2)
of a similar result for the case that the cost is squared error loss C(y; F) = (y(cid:0)F)2. Theorem 1 below
shows that if we start the e -boosting algorithm at a solution ÀÜb (s) of the l1-constrained optimization
problem (18)‚Äì(19), the ‚Äúdirection of change‚Äù of the e -boosting solution will agree with that of the
l1-constrained optimization problem.
Theorem 1 Assume the optimal coefÔ¨Åcient paths ÀÜb
paths b

j(T ) 8 j are also monotone as e -boosting proceeds, then

j(s) 8 j are monotone in s and the coefÔ¨Åcient

b (T ) (cid:0) ÀÜb (s)

T (cid:1) e

! (cid:209)

ÀÜb (s) as e ! 0; T ! ¬•

; T (cid:1) e ! 0:

Proof First we introduce some notations. Let

h j = (h j(x1); : : :h j(xn))0

be the jth basis function evaluated at the n training data.

Let

F = (F(x1); : : :F(xn))0

be the vector of current Ô¨Åt.

Let

r =(cid:18)(cid:0)

¬∂ C(y1; F1)

¬∂ F1

; : : : (cid:0)

¬∂ C(yn; Fn)

¬∂ Fn

(cid:19)0

964

b
BOOSTING AS A REGULARIZED PATH

be the current ‚Äúgeneralized residual‚Äù vector as deÔ¨Åned in Friedman (2001).

Let

c j = h0
be the current ‚Äúcorrelation‚Äù between h j and r.

jr;

j = 1; : : :J

Let

A = f j : jc jj = max

j

jc jjg

be the set of indices for the maximum absolute correlation.

For clarity, we re-write this e -boosting algorithm, starting from ÀÜb (s), as a special case of Algo-

rithm 1, as follows:

(1) Initialize b (0) = ÀÜb (s);F0 = F;r0 = r.

(2) For t = 1 : T

(a) Find jt = argmax j jh0
(b) Update

jrt(cid:0)1j.

(c) Update Ft and rt.

t; jt   b

t(cid:0)1; jt + e

(cid:1) sign(c jt )

Notice in the above algorithm, we start from ÀÜb (s), rather than 0. As proposed in Efron et al. (2004),
we consider an idealized e -boosting case:
and T (cid:1) e ! 0, under the
monotone paths condition, Section 3.2 and Section 6 of Efron et al. (2004) showed

e ! 0. As e ! 0, T ! ¬•

FT (cid:0) F0

T (cid:1) e
rT (cid:0) r0
T (cid:1) e

! u;

! v;

(20)

(21)

where u and v satisfy two constraints:

(Constraint 1) u is in the convex cone generated by fsign(c j)h j : j 2 Ag, i.e.:

u = (cid:229)

j2A

Pjsign(c j)h j; Pj (cid:21) 0:

(Constraint 2) v has equal ‚Äúcorrelation‚Äù with sign(c j)h j; j 2 A:

sign(c j)h0

jv = l A for j 2 A:

The Ô¨Årst constraint is true because the basis functions in AC will not be able to catch up in terms
of jc jj for sufÔ¨Åciently small T (cid:1) e ; the Pj‚Äôs are non-negative because the coefÔ¨Åcient paths b
j(T ) are
monotone. The second constraint can be seen by taking a Taylor expansion of C(y; F) around F0
to the quadratic term, letting T (cid:1) e go to zero and applying the result for the squared error loss from
Efron et al. (2004). Once the two constraints are established, we notice that

vi = (cid:0)

¬∂ 2C(yi; F)

¬∂ F 2

965

ui:

(cid:12)(cid:12)(cid:12)(cid:12)F0(xi)

b
ROSSET, ZHU AND HASTIE

Hence we can plug the constraint 1 into the constraint 2 and get the following set of equations:

where

AW ÀúHAP = l A1;
ÀúHT

ÀúHA = ((cid:1) (cid:1) (cid:1)sign(c j)h j (cid:1) (cid:1) (cid:1)) ; j 2 A;

W = diag  (cid:0)

¬∂ 2C(yi; F)

¬∂ F 2

P = ((cid:1) (cid:1) (cid:1)Pj (cid:1) (cid:1) (cid:1))0 ; j 2 A:

(cid:12)(cid:12)(cid:12)(cid:12)F0(xi)! ;

If ÀúH is of rank jAj (we will get back to this issue in details in Appendix B), then P, or equivalently
u and v, are uniquely determined up to a scale number.

Now we consider the l1-constrained optimization problem (18)‚Äì(19). Let ÀÜF(s) be the Ô¨Åtted

vector and ÀÜr(s) be the corresponding residual vector. Since ÀÜF(s) and ÀÜr(s) are smooth, deÔ¨Åne

u(cid:3) (cid:17) limD s!0
v(cid:3) (cid:17) limD s!0

ÀÜF(s + D s) (cid:0) ÀÜF(s)

ÀÜr(s + D s) (cid:0) ÀÜr(s)

D s

D s

;

(22)

(23)

:

Lemma 2 Under the monotone coefÔ¨Åcient paths assumption, u(cid:3) and v(cid:3) also satisfy constraints 1‚Äì2.
Proof Write the coefÔ¨Åcient b

j as b +

j (cid:0) b (cid:0)
j , where
j = b
j;b (cid:0)
j = 0
j = (cid:0)b
j = 0;b (cid:0)
b +

(cid:26) b +

if
if

j

j > 0;
j < 0:

The l1-constrained optimization problem (18)‚Äì(19) is then equivalent to

min
b +;b (cid:0)
subject to

n(cid:229)
i=1
kb +k1 + kb (cid:0)k1 (cid:20) s;b + (cid:21) 0;b (cid:0) (cid:21) 0:

C(cid:0)yi; h(xi)0(b + (cid:0) b (cid:0))(cid:1) ;

The corresponding Lagrangian dual is

L =

n(cid:229)

i=1
(cid:0)l

C(cid:0)yi; h(xi)0(b + (cid:0) b (cid:0))(cid:1) + l

(cid:1) s (cid:0)

J(cid:229)

j=1

l +
j

b +

j (cid:0)

l (cid:0)
j

b (cid:0)
j ;

J(cid:229)

j=1

J(cid:229)

j=1

(b +

j + b (cid:0)
j )

(24)

(25)

(26)

(27)

where l (cid:21) 0;l +

j (cid:21) 0;l (cid:0)

j (cid:21) 0 are Lagrange multipliers.

By differentiating the Lagrangian dual, we get the solution of (24)‚Äì(25) needed to satisfy the

following Karush-Kuhn-Tucker conditions:

¬∂ L
b +
j

= (cid:0)h0

j ÀÜr + l (cid:0) l +

j = 0;

966

(28)

b
b
¬∂
BOOSTING AS A REGULARIZED PATH

j ÀÜr + l (cid:0) l (cid:0)

j = 0;

= h0

¬∂ L
b (cid:0)
j
ÀÜb +
l +
j = 0;
j
ÀÜb (cid:0)
l (cid:0)
j = 0:
j

(29)

(30)

(31)

Let c j = h0
Tucker conditions:

j ÀÜr and A = f j : jc jj = max j jc jjg. We can see the following facts from the Karush-Kuhn-

(Fact 1) Use (28), (29) and l (cid:21) 0;l +

j ;l (cid:0)

j (cid:21) 0, we have jc jj (cid:20) l

.

j 6= 0, then jc jj = l and j 2 A. For example, suppose ÀÜb +

6= 0, then l +

j = 0 and

j

(Fact 2) If ÀÜb
(28) implies c j = l
(Fact 3) If ÀÜb

.

j 6= 0, sign( ÀÜb

j) = sign(c j).

We also note that:
j and ÀÜb (cid:0)
the same time.

(cid:15) ÀÜb +

j can not both be non-zero, otherwise l +

j = l (cid:0)

j = 0, (28) and (29) can not hold at

(cid:15) It is possible that ÀÜb

j = 0 and j 2 A. This only happens for a Ô¨Ånite number of s values, where

basis h j is about to enter the model.

For sufÔ¨Åciently small D s, since the second derivative of the cost function C(y; F) is Ô¨Ånite, A will

stay the same. Since j 2 A if ÀÜb

j 6= 0, the change in the Ô¨Åtted vector is

ÀÜF(s + D s) (cid:0) ÀÜF(s) = (cid:229)

j2A

Q jh j:

Since sign( ÀÜb
sign(c j). Hence we have

j) = sign(c j) and the coefÔ¨Åcients ÀÜb

j change monotonically, sign(Q j) will agree with

ÀÜF(s + D s) (cid:0) ÀÜF(s)

D s

= (cid:229)

j2A

Pjsign(c j)h j:

(32)

This implies u(cid:3) satisÔ¨Åes constraint 1. The claim v(cid:3) satisÔ¨Åes constraint 2 follows directly from fact
2, since both ÀÜr(s + D s) and ÀÜr(s) satisfy constraint 2.
Completion of proof of Theorem (1): We further notice that in both the e -boosting case and the
constrained optimization case, we have (cid:229)
j2A Pj = 1 by deÔ¨Ånition and the monotone coefÔ¨Åcient
paths condition, hence u and v are uniquely determined, i.e.:

u = u(cid:3) and v = v(cid:3):

To translate the result into ÀÜb (s) and b (T ), we notice F(x) = h(x)0b
that for (cid:209)
conditions for when this is true in Appendix B.

. Efron et al. (2004) showed
ÀÜb (s) to be well deÔ¨Åned, A can have at most n elements, i.e., jAj (cid:20) n. We give sufÔ¨Åcient

Now Let

HA = ((cid:1) (cid:1) (cid:1)h j(xi) (cid:1) (cid:1) (cid:1)) ; i = 1; : : :n; j 2 A

967

¬∂
ROSSET, ZHU AND HASTIE

be a n (cid:2) jAj matrix, which we assume is of rank jAj. Then (cid:209)

ÀÜb (s) is given by

and

Hence the theorem is proved.

AW u(cid:3);

ÀÜb (s) =(cid:0)H0
AW HA(cid:1)(cid:0)1 H0
AW HA(cid:1)(cid:0)1 H0
!(cid:0)H0

T (cid:1) e

b (T ) (cid:0) ÀÜb (s)

AW u:

Appendix B. Uniqueness and Existence Results

In this appendix, we give some details on the properties of regularized solution paths. In section B.1
we formulate and prove sparseness and uniqueness results on l1-regularized solutions for any convex
loss. In section B.2 we extend Theorem 3 of Section 5‚Äîwhich proved the margin maximizing
property of the limit of lp-regularized solutions, as regularization varies‚Äîto the case that the margin
maximizing solution is not unique.

B.1 Sparseness and Uniqueness of l1-Regularized Solutions and Their Limits
Consider the l1-constrained optimization problem:

min
kb k1(cid:20)c

n(cid:229)

i=1

C(yi;b

0h(xi)):

(33)

In this section we give sufÔ¨Åcient conditions for the following properties of the solutions of (33):

1. Existence of a sparse solution (with at most n non-zero coefÔ¨Åcients),

2. Non-existence of non-sparse solutions with more than n non-zero coefÔ¨Åcients,

3. Uniqueness of the solution,

4. Convergence of the solutions to sparse solution, as c increases.

Theorem 3 Assume that the unconstrained solution for problem (33) has l1 norm bigger than c.
Then there exists a solution of (33) which has at most n non-zero coefÔ¨Åcients.

Proof As Lemma 2 in the Appendix A, we will prove the theorem using the Karush-Kuhn-Tucker
(KKT) formulation of the optimization problem.
The chain rule for differentiation gives us that

iC(yi;b

0h(xi))

j

= (cid:0)h0

jr(b );

(34)

where h j and r(b ) are deÔ¨Åned in the Appendix A; r(b ) is the ‚Äúgeneralized residual‚Äù vector. Using
this simple relationship and fact 2 of Lemma 2 we can write a system of equations for all non-
zero coefÔ¨Åcients at the optimal constrained solution as follows (denote by A the set of indices for
non-zero coefÔ¨Åcients):

(35)

Ar(b ) = l
H0

(cid:1) signb A :

968

(cid:209)
¬∂
(cid:229)
¬∂
b
BOOSTING AS A REGULARIZED PATH

In other words, we get jAj equations in jAj variables, corresponding to the non-zero b

j‚Äôs.

However, each column of the matrix HA is of length n, and so HA can have at most n linearly
independent columns, rank(HA ) (cid:20) n. Assume now that we have an optimal solution for (33) with
jAj > n. Then there exists l 2 A such that

hl = (cid:229)

j2A; j6=l

jh j:

Substituting (36) into the l‚Äôth row in (35) we get

jh j)0r(b ) = l

(cid:1) signb

l:

(

j2A; j6=l

But from (35) we know that h0

jr(b ) = l

(cid:1) signb

j ; 8 j 2 A, meaning we can re-phrase (37) as

j (cid:1) signb

j (cid:1) signb

l = 1:

j2A; j6=l

(36)

(37)

(38)

In other words, we get that hl is a linear combination of the columns of HA(cid:0)flg which must obey
the speciÔ¨Åc numeric relation in (38).

Now we can construct an alternative optimal solution for (33) with one less non-zero coefÔ¨Åcient,

as follows:

1. Start from b
2. DeÔ¨Åne the direction g
j = a

l = (cid:0)signb

l ; g

in coefÔ¨Åcient space implied by (36), that is:
j (cid:1) signb

l ; 8 j 2 A (cid:0) flg

3. Move in direction g until some coefÔ¨Åcient in A hits zero, i.e., deÔ¨Åne:

d (cid:3) = min(cid:8)d > 0 : 9 j 2 A s.t. b

lj)

j + g

jd = 0(cid:9)

(we know that d (cid:3) (cid:20) jb

4. Set Àúb = b + d (cid:3)g

Then from (36) we get that Àúb

0h(xi) = b
kÀúb k1 = kb k1 (cid:0) (cid:229)

0h(xi) ; 8i and from (38) we get that

[jb

jd (cid:3)j (cid:0) jb

jj] =

(39)

j2A

j + g
= kb k1 (cid:0) d (cid:3) (cid:1) 1 (cid:0) (cid:229)

j (cid:1) signb

jsignb

l! = kb k1:

j2A(cid:0)l

So Àúb generates the same Ô¨Åt as b and has the same l1 norm, therefore it is also an optimal solution,
with at least one less non-zero coefÔ¨Åcient (from the deÔ¨Ånition of d (cid:3)).

We can obviously apply this process repeatedly until we get a solution with at most n non-zero

coefÔ¨Åcients.

This theorem has the following immediate implication:

969

a
(cid:229)
a
(cid:229)
a
g
a
ROSSET, ZHU AND HASTIE

Corollary 4 If there is no set of more than n dictionary functions which obeys the equalities (36,38)
on the training data, then any solution of (33) has at most n non-zero coefÔ¨Åcients.

This corollary implies, for example, that if the basis functions come from a ‚Äúcontinuous non-
redundant‚Äù distribution (which means that any equality would hold with probability 0) then with
probability 1 any solution of (33) has at most n non-zero coefÔ¨Åcients.

Theorem 5 Assume that there is no set of more than n dictionary functions which obeys the equal-
ities (36,38) on the training data. In addition assume:

1. The loss function C is strictly convex (squared error loss, Cl and Ce obviously qualify),

2. No set of dictionary functions of size (cid:20) n is linearly dependent on the training data.

Then the problem (33) has a unique solution.

Proof The previous corollary tells us that any solution has at most n non-zero coefÔ¨Åcients. Now
assume b 1; b 2 are both solutions of (33). From strict convexity of the loss we get that

h(X)0b 1 = h(X)0b 2 = h(X)0(a

1 + (1 (cid:0) a )b 2) ; 80 (cid:20) a (cid:20) 1;

and from convexity of the l1 norm we get

ka

1 + (1 (cid:0) a )b 2k1 (cid:20) kb 1k1 = kb 2k1 = c:

(40)

(41)

So (a
1 + (1 (cid:0) a )b 2) must also be a solution. Thus, the total number of variables with non-zero
coefÔ¨Åcients in either b 1 or b 2 cannot be bigger than n, since then (a
1 + (1 (cid:0) a )b 2), would have
> n non-zero coefÔ¨Åcients for almost all values of a
, contradicting Corollary 4. Thus, by ignoring
all coefÔ¨Åcients which are 0 in both b 1 and b 2 we get that both b 1 and b 2 can be represented in the
same n (cid:0) dimensional (maximum) sub-space of RJ. Which leads to a contradiction between (40)
and assumption 2.

: 0 (cid:20) c (cid:20) ¬• g of normalized solutions to the problem (33).
Corollary 6 Consider a sequence f
Assume that all these solutions have at most n non-zero coefÔ¨Åcients. Then any limit point of the
sequence has at most n non-zero coefÔ¨Åcients.

ÀÜb (c)
c

Proof This is a trivial consequence of convergence. Assume by contradiction b (cid:3) is a convergence
jj : b (cid:3)
point with more than n non-zero coefÔ¨Åcients. Let k = argmin jfjb (cid:3)
j 6= 0g. Then for any vector
Àúb with at most n non-zero coefÔ¨Åcients we know that k Àúb (cid:0) b (cid:3)k (cid:21) jb (cid:3)
jj > 0 so we get a contradiction
to convergence.

970

b
b
b
b
BOOSTING AS A REGULARIZED PATH

B.2 Uniqueness of Limiting Solution in Theorem 3 when Margin Maximizing Separator is

not Unique

ÀÜb (p)(c)

Recall, that we are interested in convergence points of the normalized regularized solutions
.
Theorem 3 proves that any such convergence point corresponds to an l p-margin maximizing sep-
arating hyper-plane. We now extend it to the case that this Ô¨Årst-order separator is not unique, by
extending the result to consider the second smallest margin as a ‚Äútie breaker‚Äù. We show that any
convergence point maximizes the second smallest margin among all models with maximal minimal
margin. If there are also ties in the second smallest margin, then any limit point maximizes the third
smallest margin among all models which still remain, and so on. It should be noted that the minimal
margin is typically not attained by one observation only in margin maximizing models. In case of
ties in the smallest margins our reference to ‚Äúsmallest‚Äù, ‚Äúsecond smallest‚Äù etc.
implies arbitrary
tie-breaking (i.e., our decision on which one of the tied margins is considered smallest, and which
one second smallest is of no consequence).

c

Theorem 7 Assume that the data is separable and that the margin-maximizing separating hyper-
c will correspond to a
plane, as deÔ¨Åned in (4) is not unique. Then any convergence point of
margin-maximizing separating hyper-plane which also maximizes the second smallest margin.

ÀÜb (p)(c)

Proof The proof is essentially the same as that of Theorem 3. We outline it below.

From Theorem 3 we know that we only need to consider margin-maximizing models as limit
points. Thus let b 1, b 2 be two margin maximizing models with lp norm 1, but let b 1 have a bigger
second smallest margin. Assume that b 1 attains its smallest margin on observation i1 and b 2 attains
the same smallest margin on observation i2. Now deÔ¨Åne

m1 = min
i6=i1

yih(xi)0b 1 > min
i6=i2

yih(xi)0b 2 = m2:

Then we have that Lemma 4 of Theorem 3 holds for b 1 and b 2 (the proof is exactly the same, except
that we ignore the smallest margin observation for each model, since these always contribute the
same amount to the combined loss).

Let b (cid:3) be a convergence point. We know b (cid:3) maximizes the margin from Theorem 3. Now
assume Àúb also maximizes the margin but has bigger second-smallest margin than b (cid:3). Then we can
proceed exactly as the proof of Theorem 3, considering only n (cid:0) 1 observations for each model and
using our modiÔ¨Åed Lemma 4, to conclude that b (cid:3) cannot be a convergence point (again note that the
smallest margin observation always contributes the same to the loss of both models).

In the case that the two smallest margins still do not deÔ¨Åne a unique solution, we can continue
up the list of margins, applying this result recursively. The conclusion is that the limit of the normal-
ized, lp -regularized models ‚Äúmaximizes the margins‚Äù, and not just the minimal margin. The only
case when this convergence point is not unique is, therefore, the case that the whole order statistic of
the optimal separator is not unique. It is an interesting research question to investigate under which
conditions this scenario is possible.

971

ROSSET, ZHU AND HASTIE

References

C. L. Blake

and C.

J. Merz.

databases.
[http://www.ics.uci.edu/ mlearn/MLRepository.html]. Irvine, CA: University of California,
Department of Information and Computer Science., 1998.

of machine

Repository

learning

L. Breiman. Prediction games and arcing algorithms. Neural Computation, 11(7):1493‚Äì1517, 1999.

M. Collins, R. E. Schapire, and Y. Singer. Logistic regression, adaboost and bregman distances. In

Computational Learing Theory, pages 158‚Äì169, 2000.

D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Wavelet shrinkage: Asymptopia?

J. R. Statist. Soc. B., 57(2):301‚Äì337, 1995.

B. Efron, T. Hastie, I. M. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics,

32(2), 2004.

Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an appli-
cation to boosting. In European Conference on Computational Learning Theory, pages 23‚Äì37,
1995.

J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics,

29(5), 2001.

J. H. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of

boosting. Annals of Statistics, 28:337‚Äì407, 2000.

T. Hastie, T. Tibshirani, and J. H. Friedman. Elements of Statistical Learning. Springer-Verlag,

New York, 2001.

O. Mangasarian. Arbitrary-norm separating plane. Operations Research Letters, 24(1‚Äì2):15‚Äì23,

1999.

L. Mason, J. Baxter, P. Bartlett, and M. Frean. Boosting algorithms as gradient descent. In Neural

Information Processing Systems, volume 12, 1999.

G. R¬®atsch, S. Mika, and M. K. Warmuth. On the convergence of leveraging. NeuroCOLT2 Technical

Report 98, Royal Holloway College, London, 2001a.

G. R¬®atsch, T. Onoda, and K.-R. M¬®uller. Soft margins for AdaBoost. Machine Learning, 42(3):

287‚Äì320, March 2001b.

G. R¬®atsch and M. W. Warmuth. EfÔ¨Åcient margin maximization with boosting. submitted to JMLR,

December 2002.

S. Rosset and E. Segal. Boosting density estimation. NIPS-02, 2003.

R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. Boosting the margin: a new explanation for

the effectiveness of voting methods. Annals of Statistics, 26:1651‚Äì1686, 1998.

T. Zhang. Sequential greedy approximation for certain convex optimization problems. IEEE Trans-

actions on Information Theory, 49, 2003.

972

BOOSTING AS A REGULARIZED PATH

T. Zhang and B. Yu. Boosting with early stopping: Convergence and results. Techincal report, Dept.

of Statistics, Univ. of California, Berkeley, 2003.

973

