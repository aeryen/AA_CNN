Hamming Distance Metric Learning

Mohammad Norouzi†

David J. Fleet†

Ruslan Salakhutdinov†,‡

Departments of Computer Science† and Statistics‡

University of Toronto

[norouzi,fleet,rsalakhu]@cs.toronto.edu

Abstract

Motivated by large-scale multimedia applications we propose to learn mappings
from high-dimensional data to binary codes that preserve semantic similarity.
Binary codes are well suited to large-scale applications as they are storage ef-
ﬁcient and permit exact sub-linear kNN search. The framework is applicable
to broad families of mappings, and uses a ﬂexible form of triplet ranking loss.
We overcome discontinuous optimization of the discrete mappings by minimizing
a piecewise-smooth upper bound on empirical loss, inspired by latent structural
SVMs. We develop a new loss-augmented inference algorithm that is quadratic in
the code length. We show strong retrieval performance on CIFAR-10 and MNIST,
with promising classiﬁcation results using no more than kNN on the binary codes.

Introduction

1
Many machine learning algorithms presuppose the existence of a pairwise similarity measure on
the input space. Examples include semi-supervised clustering, nearest neighbor classiﬁcation, and
kernel-based methods, When similarity measures are not given a priori, one could adopt a generic
function such as Euclidean distance, but this often produces unsatisfactory results. The goal of
metric learning techniques is to improve matters by incorporating side information, and optimizing
parametric distance functions such as the Mahalanobis distance [7, 12, 30, 34, 36].
Motivated by large-scale multimedia applications, this paper advocates the use of discrete mappings,
from input features to binary codes. Compact binary codes are remarkably storage efﬁcient, allow-
ing one to store massive datasets in memory. The Hamming distance, a natural similarity measure
on binary codes, can be computed with just a few machine instructions per comparison. Further, it
has been shown that one can perform exact nearest neighbor search in Hamming space signiﬁcantly
faster than linear search, with sublinear run-times [15, 25]. By contrast, retrieval based on Ma-
halanobis distance requires approximate nearest neighbor (ANN) search, for which state-of-the-art
methods (e.g., see [18, 23]) do not always perform well, especially with massive, high-dimensional
datasets when storage overheads and distance computations become prohibitive.
Most approaches to discrete (binary) embeddings have focused on preserving the metric (e.g. Eu-
clidean) structure of the input data, the canonical example being locality-sensitive hashing (LSH)
[4, 17]. Based on random projections, LSH and its variants (e.g., [26]) provide guarantees that metric
similarity is preserved for sufﬁciently long codes. To ﬁnd compact codes, recent research has turned
to machine learning techniques that optimize mappings for speciﬁc datasets (e.g., [20, 28, 29, 32, 3]).
However, most such methods aim to preserve Euclidean structure (e.g. [13, 20, 35]).
In metric learning, by comparison, the goal is to preserve semantic structure based on labeled at-
tributes or parameters associated with training exemplars. There are papers on learning binary hash
functions that preserve semantic similarity [29, 28, 32, 24], but most have only considered ad hoc
datasets and uninformative performance measures, for which it is difﬁcult to judge performance with
anything but the qualitative appearance of retrieval results. The question of whether or not it is pos-
sible to learn hash functions capable of preserving complex semantic structure, with high ﬁdelity,
has remained unanswered.

1

To address this issue, we introduce a framework for learning a broad class of binary hash functions
based on a triplet ranking loss designed to preserve relative similarity (c.f. [11, 5]). While certainly
useful for preserving metric structure, this loss function is very well suited to the preservation of
semantic similarity. Notably, it can be viewed as a form of local ranking loss. It is more ﬂexible
than the pairwise hinge loss of [24], and is shown below to produce superior hash functions.
Our formulation is inspired by latent SVM [10] and latent structural SVM [37] models, and it gen-
eralizes the minimal loss hashing (MLH) algorithm of [24]. Accordingly, to optimize hash function
parameters we formulate a continuous upper-bound on empirical loss, with a new form of loss-
augmented inference designed for efﬁcient optimization with the proposed triplet loss on the Ham-
ming space. To our knowledge, this is one of the most general frameworks for learning a broad class
of hash functions. In particular, many previous loss-based techniques [20, 24] are not capable of
optimizing mappings that involve non-linear projections, e.g., by neural nets.
Our experiments indicate that the framework is capable of preserving semantic structure on chal-
lenging datasets, namely, MNIST [1] and CIFAR-10 [19]. We show that k-nearest neighbor (kNN)
search on the resulting binary codes retrieves items that bear remarkable similarity to a given query
item. To show that the binary representation is rich enough to capture salient semantic structure,
as is common in metric learning, we also report classiﬁcation performance on the binary codes.
Surprisingly, on these datasets, simple kNN classiﬁers in Hamming space are competitive with so-
phisticated discriminative classiﬁers, including SVMs and neural networks. An important appeal of
our approach is the scalability of kNN search on binary codes to billions of data points, and of kNN
classiﬁcation to millions of class labels.

2 Formulation
The task is to learn a mapping b(x) that projects p-dimensional real-valued inputs x ∈ Rp onto
q-dimensional binary codes h ∈ H ≡ {−1, 1}q, while preserving some notion of similarity. This
mapping, referred to as a hash function, is parameterized by a real-valued vector w as

(1)
where sign(.) denotes the element-wise sign function, and f (x; w) : Rp → Rq is a real-valued
transformation. Different forms of f give rise to different families of hash functions:

b(x; w) = sign (f (x; w)) ,

1. A linear transform f (x) = W x, where W ∈ Rq×p and w ≡ vec(W ), is the simplest and most
well-studied case [4, 13, 24, 33]. Under this mapping the kth bit is determined by a hyperplane
in the input space whose normal is given by the kth row of W . 1

2. In [35], linear projections are followed by an element-wise cosine transform, i.e. f (x) =
cos(W x). For such mappings the bits correspond to stripes of 1 and −1 regions, oriented
parallel to the corresponding hyperplanes, in the input space.

3. Kernelized hash functions [20, 21].
4. More complex hash functions are obtained with multilayer neural networks [28, 32]. For
example, a two-layer network with a p(cid:48)-dimensional hidden layer and weight matrices W1 ∈
Rp(cid:48)×p and W2 ∈ Rq×p(cid:48)
can be expressed as f (x) = tanh(W2 tanh(W1x)), where tanh(.)
is the element-wise hyperbolic tangent function.

Our Hamming distance metric learning framework applies to all of the above families of hash func-
tions. The only restriction is that f must be differentiable with respect to its parameters, so that one
is able to compute the Jacobian of f (x; w) with respect to w.
2.1 Loss functions
The choice of loss function is crucial for learning good similarity measures. To this end, most ex-
isting supervised binary hashing techniques [13, 22, 24] formulate learning objectives in terms of
pairwise similarity, where pairs of inputs are labelled as either similar or dissimilar. Similarity-
preserving hashing aims to ensure that Hamming distances between binary codes for similar (dis-
similar) items are small (large). For example, MLH [24] uses a pairwise hinge loss function. For

1For presentation clarity, in linear and nonlinear cases, we omit bias terms. They are incorporated by adding

one dimension to the input vectors, and to the hidden layers of neural networks, with a ﬁxed value of one.

2

(cid:26) [(cid:107)h−g(cid:107)H − ρ + 1 ]+

two binary codes h, g ∈ H with Hamming distance2 (cid:107)h−g(cid:107)H, and a similarity label s ∈ {0, 1},
the pairwise hinge loss is deﬁned as:

for s = 1
for s = 0

(similar)
(dissimilar) ,

(cid:96)pair(h, g, s) =

[ ρ − (cid:107)h−g(cid:107)H + 1 ]+

(2)
where [α]+ ≡ max(α, 0), and ρ is a Hamming distance threshold that separates similar from dis-
similar codes. This loss incurs zero cost when a pair of similar inputs map to codes that differ by
less than ρ bits. The loss is zero for dissimilar items whose Hamming distance is more than ρ bits.
One problem with such loss functions is that ﬁnding a suitable threshold ρ with cross-validation is
slow. Furthermore, for many problems one cares more about the relative magnitudes of pairwise
distances than their precise numerical values. So, constraining pairwise Hamming distances over
all pairs of codes with a single threshold is overly restrictive. More importantly, not all datasets are
amenable to labeling input pairs as similar or dissimilar. One way to avoid some of these problems is
to deﬁne loss in terms of relative similarity. Such loss functions have been used in metric learning [5,
11], and, as shown below, they are also naturally suited to Hamming distance metric learning.
To deﬁne relative similarity, we assume that the training data includes triplets of items (x, x+, x−),
such that the pair (x, x+) is more similar than the pair (x, x−). Our goal is to learn a hash function
b such that b(x) is closer to b(x+) than to b(x−) in Hamming distance. Accordingly, we propose a
ranking loss on the triplet of binary codes (h, h+, h−), obtained from b applied to (x, x+, x−):

(cid:96)triplet(h, h+, h−) = (cid:2)(cid:107)h−h+(cid:107)H − (cid:107)h−h−(cid:107)H + 1(cid:3)

(3)
This loss is zero when the Hamming distance between the more-similar pair, (cid:107)h− h+(cid:107)H, is at
least one bit smaller than the Hamming distance between the less-similar pair, (cid:107)h− h−(cid:107)H. This
loss function is more ﬂexible than the pairwise loss function (cid:96)pair, as it can be used to preserve
rankings among similar items, for example based on Euclidean distance, or perhaps using path
distance between category labels within a phylogenetic tree.

+ .

3 Optimization

Given a training set of triplets, D =(cid:8)(xi, x+

i )(cid:9)n

i , x−

loss and a simple regularizer on the vector of unknown parameters w:

(cid:88)

L(w) =

(cid:96)triplet

(x,x+,x−)∈D

i=1, our objective is the sum of the empirical

(cid:0) b(x; w), b(x+; w), b(x−; w)(cid:1) +

(cid:107)w(cid:107)2
2 .

λ
2

(4)

This objective is discontinuous and non-convex. The hash function is a discrete mapping and em-
pirical loss is piecewise constant. Hence optimization is very challenging. We cannot overcome the
non-convexity, but the problems owing to the discontinuity can be mitigated through the construction
of a continuous upper bound on the loss.
The upper bound on loss that we adopt is inspired by previous work on latent structural SVMs [37].
The key observation that relates our Hamming distance metric learning framework to structured
prediction is as follows,

b(x; w) = sign (f (x; w))

= argmax

hTf (x; w) ,

(5)
where H ≡ {−1, +1}q. The argmax on the RHS effectively means that for dimensions of f (x; w)
with positive values, the optimal code should take on values +1, and when elements of f (x; w) are
negative the corresponding bits of the code should be −1. This is identical to the sign function.
3.1 Upper bound on empirical loss
The upper bound on loss that we exploit for learning hash functions takes the following form:

h∈H

(cid:0) b(x; w), b(x+; w), b(x−; w)(cid:1) ≤
(cid:0) g, g+, g−(cid:1) + gTf (x; w) + g+T
(cid:8)hTf (x; w)(cid:9) − max

max
g,g+,g−
− max

(cid:110)

(cid:110)

(cid:96)triplet

h−T
2The Hamming norm (cid:107)v(cid:107)H is deﬁned as the number of non-zero entries of vector v.

f (x+; w)

h+T

h−

h+

h

f (x+; w) + g−T

f (x−; w)

(cid:111) − max

(cid:110)

f (x−; w)

,

(6)

(cid:111)

(cid:111)

(cid:96)triplet

3

where g, g+, g−, h, h+, and h− are constrained to be q-dimensional binary vectors. To prove
the inequality in Eq. 6, note that if the ﬁrst term on the RHS were maximized3 by (g, g+, g−) =
(b(x), b(x+), b(x−)), then using Eq. 5, it is straightforward to show that Eq. 6 would become an
equality. In all other cases of (g, g+, g−) which maximize the ﬁrst term, the RHS can only be as
large or larger than when (g, g+, g−) = (b(x), b(x+), b(x−)), hence the inequality holds.
Summing the upper bound instead of the loss in Eq. 4 yields an upper bound on the regularized
empirical loss in Eq. 4.
Importantly, the resulting bound is easily shown to be continuous and
piecewise smooth in w as long as f is continuous in w. The upper bound of Eq. 6 is a generalization
of a bound introduced in [24] for the linear case, f (x) = W x. In particular, when f is linear in
w, the bound on regularized empirical loss becomes piecewise linear and convex-concave. While
the bound in Eq. 6 is more challenging to optimize than the bound in [24], it allows us to learn
hash functions based on non-linear functions f, e.g. neural networks. While the bound in [24] was
deﬁned for (cid:96)pair-type loss functions and pairwise similarity labels, the bound here applies to the more
ﬂexible class of triplet loss functions.

3.2 Loss-augmented inference
To use the upper bound in Eq. 6 for optimization, we must be able to ﬁnd the binary codes given by

(cid:110)

(cid:0) g, g+, g−(cid:1) + gTf (x) + g+T

f (x+) + g−T

f (x−)

.

(7)

(cid:111)

(ˆg, ˆg+, ˆg−) = argmax
(g,g+,g−)

(cid:96)triplet

In the structured prediction literature this maximization is called loss-augmented inference. The
challenge stems from the 23q possible binary codes over which one has to maximize the RHS.
Fortunately, we can show that this loss-augmented inference problem can be solved efﬁciently for
the class of triplet loss functions that depend only on the value of

d(g, g+, g−) ≡ (cid:107)g−g+(cid:107)H − (cid:107)g−g−(cid:107)H .

Importantly, such loss functions do not depend on the speciﬁc binary codes, but rather just the
differences. Further, note that d(g, g+, g−) can take on only 2q +1 possible values, since it is an
integer between −q and +q. Clearly the triplet ranking loss only depends on d since

(cid:0)g, g+, g−(cid:1) = (cid:96) (cid:48)(cid:0)d(g, g+, g−)(cid:1) , where

(cid:96) (cid:48)(α) = [ α − 1 ]+ .

(8)

(cid:96)triplet

For this family of loss functions, given the values of f (.) in Eq. 7, loss-augmented inference can be
performed in time O(q2). To prove this, ﬁrst consider the case d(g, g+, g−) = m, where m is an
integer between −q and q. In this case we can replace the loss augmented inference problem with

gTf (x) + g+T

f (x+) + g−T

f (x−)

s.t. d(g, g+, g−) = m .

(9)

(cid:110)

(cid:96) (cid:48)(m) + max
g,g+,g−

(cid:111)

One can solve Eq. 9 for each possible value of m. It is straightforward to see that the largest of those
2q + 1 maxima is the solution to Eq. 7. Then, what remains for us is to solve Eq. 9.
To solve Eq. 9, consider the ith bit for each of the three codes, i.e. a = g[i], b = g+[i], and c =
g−[i], where v[i] denotes the ith element of vector v. There are 8 ways to select a, b and c, but no
matter what values they take on, they can only change the value of d(g, g+, g−) by −1, 0, or +1.
Accordingly, let ei ∈ {−1, 0, +1} denote the effect of the ith bits on d(g, g+, g−). For each value
of ei, we can easily compute the maximal contribution of (a, b, c) to Eq. 9 by:

(cid:8)af (x)[i] + bf (x+)[i] + cf (x−)[i](cid:9)

(10)

such that a, b, c ∈ {−1, +1} and (cid:107)a−b(cid:107)H − (cid:107)a−c(cid:107)H = ei.

Therefore, to solve Eq. 9, we aim to select values for ei, for all i, such that(cid:80)q
(cid:80)q

i=1 ei = m and
i=1 cont(i, ei) is maximized. This can be solved for any m using a dynamic programming algo-
rithm, similar to knapsack, in O(q2). Finally, we choose m that maximizes Eq. 9 and set the bits to
the conﬁgurations that maximized cont(i, ei).

cont(i, ei) = max
a,b,c

3For presentation clarity we will sometimes drop the dependence of f and b on w, and write b(x) and f (x).

4

3.3 Perceptron-like learning
Our learning algorithm is a form of stochastic gradient descent, where in the tth iteration we sample
a triplet (x, x+, x−) from the dataset, and then take a step in the direction that decreases the upper
bound on the triplet’s loss in Eq. 6. To this end, we randomly initialize w(0). Then, at each iteration
t + 1, given w(t), we use the following procedure to update the parameters, w(t+1):

1. Select a random triplet (x, x+, x−) from dataset D.
2. Compute (ˆh, ˆh+, ˆh−) = (b(x; w(t)), b(x+; w(t)), b(x−; w(t))) using Eq. 5.
3. Compute (ˆg, ˆg+, ˆg−), the solution to the loss-augmented inference problem in Eq. 7 .
4. Update model parameters using

(cid:21)
w(t+1) = w(t) + η
,
where η is the learning rate, and ∂f (x)/∂w ≡ ∂f (x; w)/∂w|w=w(t) ∈ R|w|×q is the trans-
pose of the Jacobian matrix, where |w| is the number of parameters.

(cid:16)ˆh−− ˆg−(cid:17)− λw(t)

(cid:16)ˆh+− ˆg+(cid:17)

(cid:20) ∂f (x)

(cid:16)ˆh− ˆg

∂f (x−)

(cid:17)

∂w

+

∂f (x+)

∂w

∂w

+

This update rule can be seen as gradient descent in the upper bound of the regularized empirical loss.
Although the upper bound in Eq. 6 is not differentiable at isolated points (owing to the max terms),
in our experiments we ﬁnd that this update rule consistently decreases both the upper bound and the
actual regularized empirical loss L(w).

4 Asymmetric Hamming distance
When Hamming distance is used to score and retrieve the nearest neighbors to a given query, there is
a high probability of a tie, where multiple items are equidistant from the query in Hamming space.
To break ties and improve the similarity measure, previous work suggests the use of an asymmetric
Hamming (AH) distance [9, 14]. With an AH distance, one stores dataset entries as binary codes (for
storage efﬁciency) but the queries are not binarized. An asymmetric distance function is therefore
deﬁned on a real-valued query vector, v ∈ Rq, and a database binary code, h ∈ H. Computing AH
distance is slightly less efﬁcient than Hamming distance, and efﬁcient retrieval algorithms, such as
[25], are not directly applicable. Nevertheless, the AH distance can also be used to re-rank items
retrieved using Hamming distance, with a negligible increase in run-time. To improve efﬁciency
further when there are many codes to be re-ranked, AH distance from the query to binary codes can
be pre-computed for each 8 or 16 consecutive bits, and stored in a query-speciﬁc lookup table.
In this work, we use the following asymmetric Hamming distance function

1
4

(cid:107) h − tanh(Diag(s) v)(cid:107)2
2 ,

AH(h, v; s) =

(11)
where s ∈ Rq is a vector of scaling parameters that control the slope of hyperbolic tangent applied
to different bits; Diag(s) is a diagonal matrix with the elements of s on its diagonal. As the scaling
factors in s approach inﬁnity, AH and Hamming distances become identical. Here we use the AH
distance between a database code b(x(cid:48)) and the real-valued projection for the query f (x). Based
on our validation sets, the AH distance of Eq. 11 is relatively insensitive to values in s. For the
experiments we simply use s to scale the average absolute values of the elements of f (x) to be 0.25.
5
In practice, the basic learning algorithm described in Sec. 3 is implemented with several modiﬁca-
tions. First, instead of using a single training triplet to estimate the gradients, we use mini-batches
comprising 100 triplets and average the gradient. Second, for each triplet (x, x+, x−), we replace
x− with a “hard” example by selecting an item among all negative examples in the mini-batch that is
closest in the current Hamming distance to b(x). By harvesting hard negative examples, we ensure
that the Hamming constraints for the triplets are not too easily satisﬁed. Third, to ﬁnd good binary
codes, we encourage each bit, averaged over the training data, to be mean-zero before quantization
(motivated in [35]). This is accomplished by adding the following penalty to the objective function:

Implementation details

(cid:0)f (x; w)(cid:1)(cid:107)2

2 ,

1
2

(cid:107) mean

x

5

(12)

Figure 1: MNIST precision@k: (left) four methods (with 32-bit codes); (right) three code lengths.

where mean(f (x; w)) denotes the mean of f (x; w) across the training data. In our implementation,
for efﬁciency, the stochastic gradient of Eq. 12 is computed per mini-batch using the Jacobian matrix
in the update rule (see Sec. 3.3). Empirically, we observe that including this term in the objective
improves the quality of binary codes, especially with the triplet ranking loss.
We use a heuristic to adapt learning rates, known as bold driver [2]. For each mini-batch we evaluate
the learning objective before the parameters are updated. As long as the objective is decreasing we
slowly increase the learning rate η, but when the objective increases, η is halved. In particular, after
every 25 epochs, if the objective, averaged over the last 25 epochs, decreased, we increase η by 5%,
otherwise we decrease η by 50%. We also used a momentum term; i.e. the previous gradient update
is scaled by 0.9 and then added to the current gradient.
All experiments are run on a GPU for 2, 000 passes through the datasets. The training time for
our current implementation is under 4 hours of GPU time for most of our experiments. The two
exceptions involve CIFAR-10 with 6400-D inputs and relatively long code-lengths of 256 and 512
bits, for which the training times are approximated 8 and 16 hours respectively.

6 Experiments
Our experiments evaluate Hamming distance metric learning using two families of hash functions,
namely, linear transforms and multilayer neural networks (see Sec. 2). For each, we examine two
loss functions, the pairwise hinge loss (Eq. 2) and the triplet ranking loss (Eq. 3).
Experiments are conducted on two well-known image corpora, MNIST [1] and CIFAR-10 [19].
Ground-truth similarity labels are derived from class labels; items from the same class are deemed
similar4. This deﬁnition of similarity ignores intra-class variations and the existence of sub-
categories, e.g. styles of handwritten fours, or types of airplanes. Nevertheless, we use these coarse
similarity labels to evaluate our framework. To that end, using items from the test set as queries,
we report precision@k, i.e. the fraction of k closest items in Hamming distance that are same-class
neighbors. We also show kNN retrieval results for qualitative inspection. Finally, we report Ham-
ming (H) and asymmetric Hamming (AH) kNN classiﬁcation rates on the test sets.
Datasets. The MNIST [1] digit dataset contains 60, 000 training and 10, 000 test images (28×28
pixels) of ten handwritten digits (0 to 9). Of the 60, 000 training images, we set aside 5, 000 for
validation. CIFAR-10 [19] comprises 50, 000 training and 10, 000 test color images (32×32 pixels).
Each image belongs to one of 10 classes, namely airplane, automobile, bird, cat, deer, dog, frog,
horse, ship, and truck. The large variability in scale, viewpoint, illumination, and background clutter
poses a signiﬁcant challenge for classiﬁcation. Instead of using raw pixel values, we borrow a bag-
of-words representation from Coates et al [6]. Its 6400-D feature vector comprises one 1600-bin
histogram per image quadrant, the codewords of which are learned from 6×6 image patches. Such
high-dimensional inputs are challenging for learning similarity-preserving hash functions. Of the
50, 000 training images, we set aside 5, 000 for validation.
MNIST: We optimize binary hash functions, mapping raw MNIST images to 32, 64, and 128-bit
codes. For each test code we ﬁnd the k closest training codes using Hamming distance, and report
precision@k in Fig. 1. As one might expect, the non-linear mappings5 signiﬁcantly outperform lin-
ear mappings. We also ﬁnd that the triplet loss (Eq. 3) yields better performance than the pairwise

4Training triplets are created by taking two items from the same class, and one item from a different class.
5The two-layer neural nets for Fig. 1 and Table 1 had 1 hidden layer with 512 units. Weights were initialized

randomly, and the Jacobian with respect to the parameters was computed with the backprop algorithm [27].

6

101001000100000.870.90.930.960.99kPrecision @k  Two−layer net, tripletTwo−layer net, pairwiseLinear, tripletLinear, pairwise [24]101001000100000.870.90.930.960.99kPrecision @k  128−bit, linear, triplet64−bit, linear, triplet32−bit, linear, tripletEuclidean distanceDistance

.

g
n
i
m
m
a
H

g
n
i
m
m
a
H

Hash function, Loss
Linear, pairwise hinge [24]
Linear, triplet ranking
Two-layer Net, pairwise hinge
Two-layer Net, triplet ranking
Linear, pairwise hinge
Linear, triplet ranking
Two-layer Net, pairwise hinge
Two-layer Net, triplet ranking
Baseline
Deep neural nets with pre-training [16]
Large margin nearest neighbor [34]
RBF-kernel SVM [8]
Neural network [31]
Euclidean 3NN

m
y
s
A

64 bits
3.16
3.06
1.45
1.38
2.78
2.90
1.36
1.29

128 bits

2.61
2.44
1.44
1.27
2.46
2.51
1.35
1.20

kNN
2 NN
2 NN
30 NN
30 NN
3 NN
3 NN
30 NN
30 NN

32 bits
4.66
4.44
1.50
1.45
4.30
3.88
1.50
1.45

Error
1.2
1.3
1.4
1.6
2.89

Table 1: Classiﬁcation error rates on MNIST test set.

loss (Eq. 2). The sharp drop in precision at k = 6000 is a consequence of the fact that each digit in
MNIST has approximately 6000 same-class neighbors. Fig. 1 (right) shows how precision improves
as a function of the binary code length. Notably, kNN retrieval, for k > 10 and all code lengths,
yields higher precision than Euclidean NN on the 784-D input space. Further, note that these Euclid-
ian results effectively provide an upper bound on the performance one would expect with existing
hashing methods that preserve Eucliean distances (e.g., [13, 17, 20, 35]).
One can also evaluate the ﬁdelity of the Hamming space represenation in terms of classiﬁcation
performance from the Hamming codes. To focus on the quality of the hash functions, and the speed
of retrieval for large-scale multimedia datasets, we use a kNN classiﬁer; i.e. we just use the retrieved
neighbors to predict class labels for each test code. Table 1 reports classiﬁcation error rates using
kNN based on Hamming and asymmetric Hamming distance. Non-linear mappings, even with
only 32-bit codes, signiﬁcantly outperform linear mappings (e.g.with 128 bits). The ranking hinge
loss also improves upon the pairwise hinge loss, even though the former has no hyperparameters.
Table 1 also indicates that AH distance provides a modest boost in performance. For each method
the parameter k in the kNN classiﬁer is chosen based on the validation set.
For baseline comparison, Table 1 reports state-of-the-art performance on MNIST with sophisticated
discriminative classiﬁers (excluding those using examplar deformations and convolutional nets).
Despite the simplicity of a kNN classiﬁer, our model achieves error rates of 1.29% and 1.20% using
64- and 128-bit codes. This is compared to 1.4% with RBF-SVM [8], and to 1.6%, the best published
neural net result for this version of the task [31]. Our model also out performs the metric learning
approach of [34], and is competitive with the best known Deep Belief Network [16]; although they
used unsupervised pre-training while we do not.
The above results show that our Hamming distance metric learning framework can preserve sufﬁ-
cient semantic similarity, to the extent that Hamming kNN classiﬁcation becomes competitive with
state-of-the-art discriminative methods. Nevertheless, our method is not solely a classiﬁer, and it
can be used within many other machine learning algorithms.
In comparison, another hashing technique called iterative quantization (ITQ) [13] achieves 8.5%
error on MNIST and 78% accuracy on CIFAR-10. Our method compares favorably, especially on
MNIST. However, ITQ [13] inherently binarizes the outcome of a supervised classiﬁer (Canonical
Correlation Analysis with labels), and does not explicitly learn a similarity measure on the input
features based on pairs or triplets.
CIFAR-10: On CIFAR-10 we optimize hash functions for 64, 128, 256, and 512-bit codes. The
supplementary material includes precision@k curves, showing superior quality of hash functions
learned by the ranking loss compared to the pairwise loss. Here, in Fig. 2, we depict the quality
of retrieval results for two queries, showing the 16 nearest neighbors using 256-bit codes, 64-bit
codes (both learned with the triplet ranking loss), and Euclidean distance in the original 6400-D
feature space. The number of class-based retrieval errors is much smaller in Hamming space, and
the similarity in visual appearance is also superior. More such results, including failure modes, are
shown in the supplementary material.

7

(Hamming on 256 bit codes)

(Hamming on 64 bit codes)

(Euclidean distance)

Figure 2: Retrieval results for two CIFAR-10 test images using Hamming distance on 256-bit and
64-bit codes, and Euclidean distance on bag-of-words features. Red rectangles indicate mistakes.

Hashing, Loss
Linear, pairwise hinge [24]
Linear, pairwise hinge
Linear, triplet ranking
Linear, triplet ranking

Distance

H
AH
H
AH

kNN
7 NN
8 NN
2 NN
2 NN

64 bits
72.2
72.3
75.1
75.7

128 bits

256 bits

512 bits

72.8
73.5
75.9
76.8

73.8
74.3
77.1
77.5

74.6
74.9
77.9
78.0

Baseline
One-vs-all linear SVM [6]
Euclidean 3NN

Accuracy

77.9
59.3

Table 2: Recognition accuracy on the CIFAR-10 test set (H ≡ Hamming, AH ≡ Asym. Hamming).

Table 2 reports classiﬁcation performance (showing accuracy instead of error rates for consistency
with previous papers). Euclidean NN on the 6400-D input features yields under 60% accuracy,
while kNN with the binary codes obtains 76− 78%. As with MNIST data, this level of perfor-
mance is comparable to one-vs-all SVMs applied to the same features [6]. Not surprisingly, training
fully-connected neural nets on 6400-dimensional features with only 50, 000 training examples is
challenging and susceptible to over-ﬁtting, hence the results of neural nets on CIFAR-10 were not
competitive. Previous work [19] had some success training convolutional neural nets on this dataset.
Note that our framework can easily incorporate convolutional neural nets, which are intuitively bet-
ter suited to the intrinsic spatial structure of natural images.

7 Conclusion

We present a framework for Hamming distance metric learning, which entails learning a discrete
mapping from the input space onto binary codes. This framework accommodates different families
of hash functions, including quantized linear transforms, and multilayer neural nets. By using a
piecewise-smooth upper bound on a triplet ranking loss, we optimize hash functions that are shown
to preserve semantic similarity on complex datasets.
In particular, our experiments show that a
simple kNN classiﬁer on the learned binary codes is competitive with sophisticated discriminative
classiﬁers. While other hashing papers have used CIFAR or MNIST, none report kNN classiﬁcation
performance, often because it has been thought that the bar established by state-of-the-art classiﬁers
is too high. On the contrary our kNN classiﬁcation performance suggests that Hamming space can
be used to represent complex semantic structures with high ﬁdelity. One appeal of this approach is
the scalability of kNN search on binary codes to billions of data points, and of kNN classiﬁcation to
millions of class labels.

8

References
[1] http://yann.lecun.com/exdb/mnist/.
[2] R. Battiti. Accelerated backpropagation learning: Two optimization methods. Complex Systems, 1989.
[3] A. Bergamo, L. Torresani, and A. Fitzgibbon. Picodes: Learning a compact code for novel-category

[4] M. Charikar. Similarity estimation techniques from rounding algorithms. STOC, 2002.
[5] G. Chechik, V. Sharma, U. Shalit, and S. Bengio. Large scale online learning of image similarity through

[6] A. Coates, H. Lee, and A. Ng. An analysis of single-layer networks in unsupervised feature learning.

recognition. NIPS, 2011.

ranking. JMLR, 2010.

AISTATS, 2011.

[7] J. Davis, B. Kulis, P. Jain, S. Sra, and I. Dhillon. Information-theoretic metric learning. ICML, 2007.
[8] D. Decoste and B. Sch¨olkopf. Training invariant support vector machines. Machine Learning, 2002.
[9] W. Dong, M. Charikar, and K. Li. Asymmetric distance estimation with sketches for similarity search in

high-dimensional spaces. SIGIR, 2008.

[10] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively

trained part-based models. IEEE Trans. PAMI, 2010.

[11] A. Frome, Y. Singer, F. Sha, and J. Malik. Learning globally-consistent local distance functions for

shape-based image retrieval and classiﬁcation. ICCV, 2007.

[12] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. NIPS,

[13] Y. Gong and S. Lazebnik. Iterative quantization: A procrustean approach to learning binary codes. CVPR,

[14] A. Gordo and F. Perronnin. Asymmetric distances for binary embeddings. CVPR, 2011.
[15] D. Greene, M. Parnas, and F. Yao. Multi-index hashing for information retrieval. FOCS, 1994.
[16] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,

[17] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality.

[18] H. J´egou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search.

IEEE Trans.

[19] A. Krizhevsky. Learning multiple layers of features from tiny images. MSc. thesis, Univ. Toronto, 2009.
[20] B. Kulis and T. Darrell. Learning to hash with binary reconstructive embeddings. NIPS, 2009.
[21] B. Kulis and K. Grauman. Kernelized locality-sensitive hashing for scalable image search. ICCV, 2009.
[22] W. Liu, J. Wang, R. Ji, Y. Jiang, and S. Chang. Supervised hashing with kernels. CVPR, 2012.
[23] M. Muja and D. Lowe. Fast approximate nearest neighbors with automatic algorithm conﬁguration.

VISSAPP, 2009.

2012.

Press, 1986.

ICCV, 2003.

[24] M. Norouzi and D. J. Fleet. Minimal Loss Hashing for Compact Binary Codes. ICML, 2011.
[25] M. Norouzi, A. Punjani, and D. Fleet. Fast search in hamming space with multi-index hashing. CVPR,

[26] M. Raginsky and S. Lazebnik. Locality-sensitive binary codes from shift-invariant kernels. NIPS, 2009.
[27] D. Rumelhart, G. Hinton, and R. Williams. Learning internal representations by error propagation. MIT

[28] R. Salakhutdinov and G. Hinton. Semantic hashing. Int. J. Approximate Reasoning, 2009.
[29] G. Shakhnarovich, P. A. Viola, and T. Darrell. Fast pose estimation with parameter-sensitive hashing.

[30] S. Shalev-Shwartz, Y. Singer, and A. Ng. Online and batch learning of pseudo-metrics. ICML, 2004.
[31] P. Simard, D. Steinkraus, and J. Platt. Best practice for convolutional neural networks applied to visual

document analysis. ICDR, 2003.

[32] A. Torralba, R. Fergus, and Y. Weiss. Small codes and large image databases for recognition. CVPR,

2004.

2011.

2006.

STOC, 1998.

PAMI, 2011.

2008.

ICML, 2010.

ﬁcation. NIPS, 2006.

[33] J. Wang, S. Kumar, and S. Chang. Sequential Projection Learning for Hashing with Compact Codes.

[34] K. Weinberger, J. Blitzer, and L. Saul. Distance metric learning for large margin nearest neighbor classi-

[35] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. NIPS, 2008.
[36] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance metric learning, with application to clustering with

side-information. NIPS, 2002.

[37] C. N. J. Yu and T. Joachims. Learning structural SVMs with latent variables. ICML, 2009.

9

