Correlated Topic Models

David M. Blei

Department of Computer Science

Princeton University

John D. Lafferty

School of Computer Science
Carnegie Mellon University

Abstract

Topic models, such as latent Dirichlet allocation (LDA), can be useful
tools for the statistical analysis of document collections and other dis-
crete data. The LDA model assumes that the words of each document
arise from a mixture of topics, each of which is a distribution over the vo-
cabulary. A limitation of LDA is the inability to model topic correlation
even though, for example, a document about genetics is more likely to
also be about disease than x-ray astronomy. This limitation stems from
the use of the Dirichlet distribution to model the variability among the
topic proportions. In this paper we develop the correlated topic model
(CTM), where the topic proportions exhibit correlation via the logistic
normal distribution [1]. We derive a mean-ﬁeld variational inference al-
gorithm for approximate posterior inference in this model, which is com-
plicated by the fact that the logistic normal is not conjugate to the multi-
nomial. The CTM gives a better ﬁt than LDA on a collection of OCRed
articles from the journal Science. Furthermore, the CTM provides a nat-
ural way of visualizing and exploring this and other unstructured data
sets.

1 Introduction

The availability and use of unstructured historical collections of documents is rapidly grow-
ing. As one example, JSTOR (www.jstor.org) is a not-for-proﬁt organization that main-
tains a large online scholarly journal archive obtained by running an optical character recog-
nition engine over the original printed journals. JSTOR indexes the resulting text and pro-
vides online access to the scanned images of the original content through keyword search.
This provides an extremely useful service to the scholarly community, with the collection
comprising nearly three million published articles in a variety of ﬁelds.

The sheer size of this unstructured and noisy archive naturally suggests opportunities for
the use of statistical modeling. For instance, a scholar in a narrow subdiscipline, searching
for a particular research article, would certainly be interested to learn that the topic of
that article is highly correlated with another topic that the researcher may not have known
about, and that is not explicitly contained in the article. Alerted to the existence of this new
related topic, the researcher could browse the collection in a topic-guided manner to begin
to investigate connections to a previously unrecognized body of work. Since the archive
comprises millions of articles spanning centuries of scholarly work, automated analysis is
essential.

Several statistical models have recently been developed for automatically extracting the
topical structure of large document collections.
In technical terms, a topic model is a
generative probabilistic model that uses a small number of distributions over a vocabulary
to describe a document collection. When ﬁt from data, these distributions often correspond
to intuitive notions of topicality. In this work, we build upon the latent Dirichlet allocation
(LDA) [4] model. LDA assumes that the words of each document arise from a mixture
of topics. The topics are shared by all documents in the collection; the topic proportions
are document-speciﬁc and randomly drawn from a Dirichlet distribution. LDA allows each
document to exhibit multiple topics with different proportions, and it can thus capture the
heterogeneity in grouped data that exhibit multiple latent patterns. Recent work has used
LDA in more complicated document models [9, 11, 7], and in a variety of settings such
as image processing [12], collaborative ﬁltering [8], and the modeling of sequential data
and user proﬁles [6]. Similar models were independently developed for disability survey
data [5] and population genetics [10].

Our goal in this paper is to address a limitation of the topic models proposed to date: they
fail to directly model correlation between topics. In many—indeed most—text corpora, it
is natural to expect that subsets of the underlying latent topics will be highly correlated. In
a corpus of scientiﬁc articles, for instance, an article about genetics may be likely to also
be about health and disease, but unlikely to also be about x-ray astronomy. For the LDA
model, this limitation stems from the independence assumptions implicit in the Dirichlet
distribution on the topic proportions. Under a Dirichlet, the components of the proportions
vector are nearly independent; this leads to the strong and unrealistic modeling assumption
that the presence of one topic is not correlated with the presence of another.

In this paper we present the correlated topic model (CTM). The CTM uses an alterna-
tive, more ﬂexible distribution for the topic proportions that allows for covariance structure
among the components. This gives a more realistic model of latent topic structure where
the presence of one latent topic may be correlated with the presence of another. In the
following sections we develop the technical aspects of this model, and then demonstrate its
potential for the applications envisioned above. We ﬁt the model to a portion of the JSTOR
archive of the journal Science. We demonstrate that the model gives a better ﬁt than LDA,
as measured by the accuracy of the predictive distributions over held out documents. Fur-
thermore, we demonstrate qualitatively that the correlated topic model provides a natural
way of visualizing and exploring such an unstructured collection of textual data.

2 The Correlated Topic Model

The key to the correlated topic model we propose is the logistic normal distribution [1]. The
logistic normal is a distribution on the simplex that allows for a general pattern of variability
between the components by transforming a multivariate normal random variable. Consider
the natural parameterization of a K-dimensional multinomial distribution:

p(z | η) = exp{ηT z − a(η)}.

(1)
The random variable Z can take on K values; it can be represented by a K-vector with
exactly one component equal to one, denoting a value in {1, . . . , K}. The cumulant gener-
ating function of the distribution is

(cid:16)PK
i=1 exp{ηi}(cid:17)

a(η) = log

.

(2)

The mapping between the mean parameterization (i.e., the simplex) and the natural param-
eterization is given by

(3)
Notice that this is not the minimal exponential family representation of the multinomial
because multiple values of η can yield the same mean parameter.

ηi = log θi/θK.

Figure 1: Top: Graphical model representation of the correlated topic model. The logistic
normal distribution, used to model the latent topic proportions of a document, can represent
correlations between topics that are impossible to capture using a single Dirichlet. Bottom:
Example densities of the logistic normal on the 2-simplex. From left: diagonal covariance
and nonzero-mean, negative correlation between components 1 and 2, positive correlation
between components 1 and 2.

exp ηi/P

The logistic normal distribution assumes that η is normally distributed and then mapped
to the simplex with the inverse of the mapping given in equation (3); that is, f(ηi) =
j exp ηj. The logistic normal models correlations between components of the
simplicial random variable through the covariance matrix of the normal distribution. The
logistic normal was originally studied in the context of analyzing observed compositional
data such as the proportions of minerals in geological samples. In this work, we extend its
use to a hierarchical model where it describes the latent composition of topics associated
with each document.
Let {µ, Σ} be a K-dimensional mean and covariance matrix, and let topics β1:K be K
multinomials over a ﬁxed word vocabulary. The correlated topic model assumes that an
N-word document arises from the following generative process:

1. Draw η |{µ, Σ} ∼ N (µ, Σ).
2. For n ∈ {1, . . . , N}:

(a) Draw topic assignment Zn | η from Mult(f(η)).
(b) Draw word Wn |{zn, β1:K} from Mult(βzn).

This process is identical to the generative process of LDA except that the topic proportions
are drawn from a logistic normal rather than a Dirichlet. The model is shown as a directed
graphical model in Figure 1.

The CTM is more expressive than LDA. The strong independence assumption imposed
by the Dirichlet in LDA is not realistic when analyzing document collections, where one
may ﬁnd strong correlations between topics. The covariance matrix of the logistic normal
in the CTM is introduced to model such correlations. In Section 3, we illustrate how the
higher order structure given by the covariance can be used as an exploratory tool for better
understanding and navigating a large corpus of documents. Moreover, modeling correlation
can lead to better predictive distributions. In some settings, such as collaborative ﬁltering,

Zd,nWd,nNDKΣµηdβkthe goal is to predict unseen items conditional on a set of observations. An LDA model
will predict words based on the latent topics that the observations suggest, but the CTM
has the ability to predict items associated with additional topics that are correlated with the
conditionally probable topics.

2.1 Posterior inference and parameter estimation

Posterior inference is the central challenge to using the CTM. The posterior distribution of
the latent variables conditional on a document, p(η, z1:N | w1:N ), is intractable to compute;
once conditioned on some observations, the topic assignments z1:N and log proportions
η are dependent. We make use of mean-ﬁeld variational methods to efﬁciently obtain an
approximation of this posterior distribution.

In brief, the strategy employed by mean-ﬁeld variational methods is to form a factorized
distribution of the latent variables, parameterized by free variables which are called the vari-
ational parameters. These parameters are ﬁt so that the Kullback-Leibler (KL) divergence
between the approximate and true posterior is small. For many problems this optimization
problem is computationally manageable, while standard methods, such as Markov Chain
Monte Carlo, are impractical. The tradeoff is that variational methods do not come with
the same theoretical guarantees as simulation methods. See [13] for a modern review of
variational methods for statistical inference.

In graphical models composed of conjugate-exponential family pairs and mixtures, the
variational inference algorithm can be automatically derived from general principles [2,
14]. In the CTM, however, the logistic normal is not conjugate to the multinomial. We
will therefore derive a variational inference algorithm by taking into account the special
structure and distributions used by our model.

We begin by using Jensen’s inequality to bound the log probability of a document:

log p(w1:N | µ, Σ, β) ≥

Eq [log p(η | µ, Σ)] +PN

(4)
n=1(Eq [log p(zn | η)] + Eq [log p(wn | zn, β)]) + H (q) ,
where the expectation is taken with respect to a variational distribution of the latent vari-
ables, and H (q) denotes the entropy of that distribution. We use a factorized distribution:

q(η1:K, z1:N | λ1:K, ν2

i=1 q(ηi | λi, ν2

n=1 q(zn | φn).

(5)

1:K, φ1:N ) =QK

i )QN

The variational distributions of the discrete variables z1:N are speciﬁed by the K-
dimensional multinomial parameters φ1:N . The variational distribution of the continuous
variables η1:K are K independent univariate Gaussians {λi, νi}. Since the variational pa-
rameters are ﬁt using a single observed document w1:N , there is no advantage in introduc-
ing a non-diagonal variational covariance matrix.

The nonconjugacy of the logistic normal leads to difﬁculty in computing the expected log
probability of a topic assignment:

h

log(PK

i
i=1 exp{ηi})

.

(6)

Eq [log p(zn | η)] = Eq

(cid:2)ηT zn
(cid:3) − Eq
i=1 exp{ηi}(cid:17)i ≤ ζ−1(PK
(cid:16)PK

h

To preserve the lower bound on the log probability, we upper bound the log normalizer
with a Taylor expansion,

Eq

log

(7)
where we have introduced a new variational parameter ζ. The expectation Eq [exp{ηi}] is
the mean of a log normal distribution with mean and variance obtained from the variational
parameters {λi, ν2

i }; thus, Eq [exp{ηi}] = exp{λi + ν2

i /2} for i ∈ {1, . . . , K}.

i=1 Eq [exp{ηi}]) − 1 + log(ζ),

Figure 2: A portion of the topic graph learned from 16,351 OCR articles from Science.
Each node represents a topic, and is labeled with the ﬁve most probable phrases from its
distribution (phrases are found by the “turbo topics” method [3]). The interested reader can
browse the full model at http://www.cs.cmu.edu/˜lemur/science/.

Given a model {β1:K, µ, Σ} and a document w1:N , the variational inference algorithm op-
timizes equation (4) with respect to the variational parameters {λ1:K, ν1:K, φ1:N , ζ}. We
use coordinate ascent, repeatedly optimizing with respect to each parameter while holding
the others ﬁxed. In variational inference for LDA, each coordinate can be optimized ana-
lytically. However, iterative methods are required for the CTM when optimizing for λi and
ν2
i . The details are given in Appendix A.
Given a collection of documents, we carry out parameter estimation in the correlated topic
model by attempting to maximize the likelihood of a corpus of documents as a function
of the topics β1:K and the multivariate Gaussian parameters {µ, Σ}. We use variational
expectation-maximization (EM), where we maximize the bound on the log probability of a
collection given by summing equation (4) over the documents.

In the E-step, we maximize the bound with respect to the variational parameters by per-
forming variational inference for each document. In the M-step, we maximize the bound
with respect to the model parameters. This is maximum likelihood estimation of the top-
ics and multivariate Gaussian using expected sufﬁcient statistics, where the expectation
is taken with respect to the variational distributions computed in the E-step. The E-step
and M-step are repeated until the bound on the likelihood converges. In the experiments
reported below, we run variational inference until the relative change in the probability
bound of equation (4) is less than 10−6, and run variational EM until the relative change in
the likelihood bound is less than 10−5.

3 Examples and Empirical Results: Modeling Science

In order to test and illustrate the correlated topic model, we estimated a 100-topic CTM
on 16,351 Science articles spanning 1990 to 1999. We constructed a graph of the la-
tent topics and the connections among them by examining the most probable words from
each topic and the between-topic correlations. Part of this graph is illustrated in Fig-
ure 2. In this subgraph, there are three densely connected collections of topics: material
science, geology, and cell biology. Furthermore, an estimated CTM can be used to ex-
plore otherwise unstructured observed documents.
In Figure 4, we list articles that are
assigned to the cognitive science topic and articles that are assigned to both the cog-

wild typemutantmutationsmutantsmutationgeneyeastrecombinationphenotypegenesp53cell cycleactivitycyclinregulationproteinphosphorylationkinaseregulatedcell cycle progressionamino acidscdnasequenceisolatedproteinamino acidmrnaamino acid sequenceactinclonegenediseasemutationsfamiliesmutationalzheimers diseasepatientshumanbreast cancernormaldevelopmentembryosdrosophilagenesexpressionembryodevelopmentalembryonicdevelopmental biologyvertebratemantlecrustupper mantlemeteoritesratiosrocksgrainsisotopicisotopic compositiondepthco2carboncarbon dioxidemethanewaterenergygasfuelproductionorganic matterearthquakeearthquakesfaultimagesdataobservationsfeaturesvenussurfacefaultsancientfoundimpactmillion years agoafricasitebonesyears agodaterockclimateoceanicechangesclimate changenorth atlanticrecordwarmingtemperaturepastgeneticpopulationpopulationsdifferencesvariationevolutionlocimtdnadataevolutionarymalesmalefemalesfemalespermsexoffspringeggsspecieseggfossil recordbirdsfossilsdinosaursfossilevolutiontaxaspeciesspecimensevolutionarysynapsesltpglutamatesynapticneuronslong term potentiation ltpsynaptic transmissionpostsynapticnmda receptorshippocampusca2calciumreleaseca2 releaseconcentrationip3intracellular calciumintracellularintracellular ca2ca2 irasatpcampgtpadenylyl cyclasecftradenosine triphosphate atpguanosine triphosphate gtpgapgdpneuronsstimulusmotorvisualcorticalaxonsstimulimovementcortexeyeozoneatmosphericmeasurementsstratosphereconcentrationsatmosphereairaerosolstropospheremeasuredbrainmemorysubjectslefttaskbrainscognitivelanguagehuman brainlearningFigure 3: (L) The average held-out probability; CTM supports more topics than LDA. See
ﬁgure at right for the standard error of the difference. (R) The log odds ratio of the held-out
probability. Positive numbers indicate a better ﬁt by the correlated topic model.

nitive science and visual neuroscience topics. The interested reader is invited to visit
http://www.cs.cmu.edu/˜lemur/science/ to interactively explore this model, in-
cluding the topics, their connections, and the articles that exhibit them.

We compared the CTM to LDA by ﬁtting a smaller collection of articles to models of vary-
ing numbers of topics. This collection contains the 1,452 documents from 1960; we used
a vocabulary of 5,612 words after pruning common function words and terms that occur
once in the collection. Using ten-fold cross validation, we computed the log probability of
the held-out data given a model estimated from the remaining data. A better model of the
document collection will assign higher probability to the held out data. To avoid comparing
bounds, we used importance sampling to compute the log probability of a document where
the ﬁtted variational distribution is the proposal.

Figure 3 illustrates the average held out log probability for each model and the average
difference between them. The CTM provides a better ﬁt than LDA and supports more
topics; the likelihood for LDA peaks near 30 topics while the likelihood for the CTM peaks
close to 90 topics. The means and standard errors of the difference in log-likelihood of the
models is shown at right; this indicates that the CTM always gives a better ﬁt.

Another quantitative evaluation of the relative strengths of LDA and the CTM is how well
the models predict the remaining words after observing a portion of the document. Sup-
pose we observe words w1:P from a document and are interested in which model provides
a better predictive distribution p(w | w1:P ) of the remaining words. To compare these dis-
tributions, we use perplexity, which can be thought of as the effective number of equally
likely words according to the model. Mathematically, the perplexity of a word distribu-
tion is deﬁned as the inverse of the per-word geometric average of the probability of the
observations,

(cid:17)
QNd
i=P +1 p(wi | Φ, w1:P )

PD
d=1(Nd−P ) ,

−1

(cid:16)QD

d=1

Perp(Φ) =

where Φ denotes the model parameters of an LDA or CTM model. Note that lower numbers
denote more predictive power.

The plot in Figure 4 compares the predictive perplexity under LDA and the CTM. When a

Number of topicsHeld−out log likelihood5102030405060708090100110120−116400−116000−115600−115200−114800−114400−114000−113600−113200−112800llllllllllllllllllllllllllCTMLDAlllllllllllll10203040506070809010011012002004006008001000120014001600180020002200Number of topicsL(CTM) − L(LDA)lllllllllllllFigure 4: (Left) Exploring a collection through its topics. (Right) Predictive perplexity for
partially observed held-out documents from the 1960 Science corpus.

small number of words have been observed, there is less uncertainty about the remaining
words under the CTM than under LDA—the perplexity is reduced by nearly 200 words, or
roughly 10%. The reason is that after seeing a few words in one topic, the CTM uses topic
correlation to infer that words in a related topic may also be probable. In contrast, LDA
cannot predict the remaining words as well until a large portion of the document as been
observed so that all of its topics are represented.
Acknowledgments Research supported in part by NSF grants IIS-0312814 and IIS-
0427206 and by the DARPA CALO project.

References
[1] J. Aitchison. The statistical analysis of compositional data. Journal of the Royal

Statistical Society, Series B, 44(2):139–177, 1982.

[2] C. Bishop, D. Spiegelhalter, and J. Winn. VIBES: A variational inference engine for

Bayesian networks. In NIPS 15, pages 777–784. Cambridge, MA, 2003.

[3] D. Blei, J. Lafferty, C. Genovese, and L. Wasserman. Turbo topics. In progress, 2006.
[4] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine

Learning Research, 3:993–1022, January 2003.

[5] E. Erosheva. Grade of membership and latent structure models with application to
disability survey data. PhD thesis, Carnegie Mellon University, Department of Statis-
tics, 2002.

[6] M. Girolami and A. Kaban. Simplicial mixtures of Markov chains: Distributed mod-

elling of dynamic user proﬁles. In NIPS 16, pages 9–16, 2004.

[7] T. Grifﬁths, M. Steyvers, D. Blei, and J. Tenenbaum. Integrating topics and syntax.

In Advances in Neural Information Processing Systems 17, 2005.

[8] B. Marlin. Collaborative ﬁltering: A machine learning perspective. Master’s thesis,

University of Toronto, 2004.

[9] A. McCallum, A. Corrada-Emmanuel, and X. Wang. The author-recipient-topic

model for topic and role discovery in social networks. 2004.

(1) Separate Neural Bases of Two Fundamental Memory      Processes in the Human Medial Temporal Lobe(2) Inattentional Blindness Versus Inattentional Amnesia for     Fixated but Ignored Words(3) Making Memories: Brain Activity that Predicts How Well     Visual Experience Will be Remembered       (4) The Learning of Categories: Parallel Brain Systems for      Item Memory and Category Knowledge(5) Brain Activation Modulated by Sentence Comprehension                                              (1) A Head for Figures(2) Sources of Mathematical Thinking: Behavioral and Brain     Imaging Evidence(3) Natural Language Processing(4) A Romance Blossoms Between Gray Matter and Silicon(5) Computer VisionTop Articles with{brain, memory, human, visual, cognitive}Top Articles with {brain, memory, human, visual, cognitive} and {computer, data, information, problem, systems}% observed wordsPredictive perplexity10203040506070809018002000220024002600llllllllllllllllllCTMLDA[10] J. Pritchard, M. Stephens, and P. Donnelly. Inference of population structure using

multilocus genotype data. Genetics, 155:945–959, June 2000.

[11] M. Rosen-Zvi, T. Grifﬁths, M. Steyvers, and P. Smith. In UAI ’04: Proceedings of

the 20th Conference on Uncertainty in Artiﬁcial Intelligence, pages 487–494.

[12] J. Sivic, B. Rusell, A. Efros, A. Zisserman, and W. Freeman. Discovering object

categories in image collections. Technical report, CSAIL, MIT, 2005.

[13] M. Wainwright and M. Jordan. A variational principle for graphical models. In New

Directions in Statistical Signal Processing, chapter 11. MIT Press, 2005.

[14] E. Xing, M. Jordan, and S. Russell. A generalized mean ﬁeld algorithm for variational

inference in exponential families. In Proceedings of UAI, 2003.

A Variational Inference
We describe a coordinate ascent optimization algorithm for the likelihood bound in equa-
tion (4) with respect to the variational parameters.

(cid:2)(η − µ)T Σ−1(η − µ)(cid:3) ,

(8)

(9)

(10)

(11)

(12)

The second term of equation (4), using the additional bound in equation (7), is

The ﬁrst term of equation (4) is
Eq [log p(η | µ, Σ)] = (1/2) log |Σ−1| − (K/2) log 2π − (1/2)Eq

(cid:2)(η − µ)T Σ−1(η − µ)(cid:3) = Tr(diag(ν2)Σ−1) + (λ − µ)T Σ−1(λ − µ).

where

Eq

Eq [log p(zn | η)] =PK

The third term of equation (4) is

i=1 λiφn,i − ζ−1(cid:16)PK
Eq [log p(wn | zn, β)] =PK
i + log 2π + 1) −PN

2(log ν2

1

PK

i=1

i=1 φn,i log βi,wn.
Finally, the fourth term is the entropy of the variational distribution:

Pk

n=1

i=1 φn,i log φn,i.

i /2}(cid:17)

i=1 exp{λi + ν2

+ 1 − log ζ.

We maximize the bound in equation (4) with respect to the variational parameters λ1:K,
ν1:K, φ1:N , and ζ. We use a coordinate ascent algorithm, iteratively maximizing the bound
with respect to each parameter.

First, we maximize equation (4) with respect to ζ, using the second bound in equation (7).
The derivative with respect to ζ is

(cid:16)
ζ−2(cid:16)PK
ˆζ =PK

i=1 exp{λi + ν2
i=1 exp{λi + ν2

i /2}.

i /2}(cid:17) − ζ−1(cid:17)

,

(13)

(14)

f0(ζ) = N

which has a maximum at

Second, we maximize with respect to φn. This yields a maximum at

ˆφn,i ∝ exp{λi}βi,wn,

i ∈ {1, . . . , K}.

(15)
Third, we maximize with respect to λi. Since equation (4) is not amenable to analytic
maximization, we use a conjugate gradient algorithm with derivative

dL/dλ = −Σ−1(λ − µ) +PN

n=1 φn,1:K − (N/ζ) exp{λ + ν2/2} .
(16)
i . Again, there is no analytic solution. We use

Finally, we maximize with respect to ν2
Newton’s method for each coordinate, constrained such that νi > 0:

dL/dν2

(17)
Iterating between these optimizations deﬁnes a coordinate ascent algorithm on equa-
tion (4).

i = −Σ−1

ii /2 − N/2ζ exp{λ + ν2

i /2} + 1/(2ν2
i ).

