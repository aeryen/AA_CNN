Carnegie Mellon University
Research Showcase @ CMU

Computer Science Department

School of Computer Science

6-2006

Dynamic Topic Models

David M. Blei
Princeton University

John D. Lafferty
Carnegie Mellon University

Follow this and additional works at: http://repository.cmu.edu/compsci

Published In
Proceedings of the 23rd international Conference on Machine Learning. ICML '06, 113-120.

This Conference Proceeding is brought to you for free and open access by the School of Computer Science at Research Showcase @ CMU. It has been
accepted for inclusion in Computer Science Department by an authorized administrator of Research Showcase @ CMU. For more information, please
contact research-showcase@andrew.cmu.edu.

Dynamic Topic Models

David M. Blei
Computer Science Department, Princeton University, Princeton, NJ 08544, USA

BLEI@CS.PRINCETON.EDU

John D. Lafferty
School of Computer Science, Carnegie Mellon University, Pittsburgh PA 15213, USA

LAFFERTY@CS.CMU.EDU

Abstract

A family of probabilistic time series models is
developed to analyze the time evolution of topics
in large document collections. The approach is
to use state space models on the natural param-
eters of the multinomial distributions that repre-
sent the topics. Variational approximations based
on Kalman ï¬lters and nonparametric wavelet re-
gression are developed to carry out approximate
posterior inference over the latent topics. In addi-
tion to giving quantitative, predictive models of a
sequential corpus, dynamic topic models provide
a qualitative window into the contents of a large
document collection. The models are demon-
strated by analyzing the OCRâ€™ed archives of the
journal Science from 1880 through 2000.

1. Introduction

Managing the explosion of electronic document archives
requires new tools for automatically organizing, searching,
indexing, and browsing large collections. Recent research
in machine learning and statistics has developed new tech-
niques for ï¬nding patterns of words in document collec-
tions using hierarchical probabilistic models (Blei et al.,
2003; McCallum et al., 2004; Rosen-Zvi et al., 2004; Grif-
ï¬ths and Steyvers, 2004; Buntine and Jakulin, 2004; Blei
and Lafferty, 2006). These models are called â€œtopic mod-
elsâ€ because the discovered patterns often reï¬‚ect the under-
lying topics which combined to form the documents. Such
hierarchical probabilistic models are easily generalized to
other kinds of data; for example, topic models have been
used to analyze images (Fei-Fei and Perona, 2005; Sivic
et al., 2005), biological data (Pritchard et al., 2000), and
survey data (Erosheva, 2002).

In an exchangeable topic model, the words of each docu-

Appearing in Proceedings of the 23 rd International Conference
on Machine Learning, Pittsburgh, PA, 2006. Copyright 2006 by
the author(s)/owner(s).

ment are assumed to be independently drawn from a mix-
ture of multinomials. The mixing proportions are randomly
drawn for each document; the mixture components, or top-
ics, are shared by all documents. Thus, each document
reï¬‚ects the components with different proportions. These
models are a powerful method of dimensionality reduction
for large collections of unstructured documents. Moreover,
posterior inference at the document level is useful for infor-
mation retrieval, classiï¬cation, and topic-directed brows-
ing.

Treating words exchangeably is a simpliï¬cation that it is
consistent with the goal of identifying the semantic themes
within each document. For many collections of interest,
however, the implicit assumption of exchangeable doc-
uments is inappropriate. Document collections such as
scholarly journals, email, news articles, and search query
logs all reï¬‚ect evolving content. For example, the Science
article â€œThe Brain of Professor Labordeâ€ may be on the
same scientiï¬c path as the article â€œReshaping the Corti-
cal Motor Map by Unmasking Latent Intracortical Connec-
tions,â€ but the study of neuroscience looked much different
in 1903 than it did in 1991. The themes in a document col-
lection evolve over time, and it is of interest to explicitly
model the dynamics of the underlying topics.

In this paper, we develop a dynamic topic model which
captures the evolution of topics in a sequentially organized
corpus of documents. We demonstrate its applicability by
analyzing over 100 years of OCRâ€™ed articles from the jour-
nal Science, which was founded in 1880 by Thomas Edi-
son and has been published through the present. Under this
model, articles are grouped by year, and each yearâ€™s arti-
cles arise from a set of topics that have evolved from the
last yearâ€™s topics.

In the subsequent sections, we extend classical state space
models to specify a statistical model of topic evolution.
We then develop efï¬cient approximate posterior inference
techniques for determining the evolving topics from a se-
quential collection of documents. Finally, we present qual-
itative results that demonstrate how dynamic topic models
allow the exploration of a large document collection in new

Dynamic Topic Models

ways, and quantitative results that demonstrate greater pre-
dictive accuracy when compared with static topic models.

2. Dynamic Topic Models

While traditional time series modeling has focused on con-
tinuous data, topic models are designed for categorical
data. Our approach is to use state space models on the nat-
ural parameter space of the underlying topic multinomials,
as well as on the natural parameters for the logistic nor-
mal distributions used for modeling the document-speciï¬c
topic proportions.

First, we review the underlying statistical assumptions of
a static topic model, such as latent Dirichlet allocation
(LDA) (Blei et al., 2003). Let Î²1:K be K topics, each of
which is a distribution over a ï¬xed vocabulary. In a static
topic model, each document is assumed drawn from the
following generative process:

1. Choose topic proportions Î¸ from a distribution over

the (K âˆ’ 1)-simplex, such as a Dirichlet.

2. For each word:

(a) Choose a topic assignment Z âˆ¼ Mult(Î¸).
(b) Choose a word W âˆ¼ Mult(Î²z).

This process implicitly assumes that the documents are
drawn exchangeably from the same set of topics. For many
collections, however, the order of the documents reï¬‚ects
an evolving set of topics. In a dynamic topic model, we
suppose that the data is divided by time slice, for example
by year. We model the documents of each slice with a K-
component topic model, where the topics associated with
slice t evolve from the topics associated with slice t âˆ’ 1.

For a K-component model with V terms, let Î²t,k denote
the V -vector of natural parameters for topic k in slice t.
The usual representation of a multinomial distribution is by
its mean parameterization. If we denote the mean param-
eter of a V -dimensional multinomial by Ï€, the ith com-
ponent of the natural parameter is given by the mapping
Î²i = log(Ï€i/Ï€V ). In typical language modeling applica-
tions, Dirichlet distributions are used to model uncertainty
about the distributions over words. However, the Dirichlet
is not amenable to sequential modeling. Instead, we chain
the natural parameters of each topic Î²t,k in a state space
model that evolves with Gaussian noise; the simplest ver-
sion of such a model is

Î²t,k | Î²tâˆ’1,k âˆ¼ N (Î²tâˆ’1,k, Ïƒ2I) .

(1)

Our approach is thus to model sequences of compositional
random variables by chaining Gaussian distributions in a
dynamic model and mapping the emitted values to the sim-
plex. This is an extension of the logistic normal distribu-

Î±

Î¸

z

w

N

A

Î²

Î±

Î¸

z

w

N

Î²

A

Î±

Î¸

z

w

N

A

Î²

K

Figure1.Graphical representation of a dynamic topic model (for
three time slices). Each topicâ€™s natural parameters Î²t,k evolve
over time, together with the mean parameters Î±t of the logistic
normal distribution for the topic proportions.

tion (Aitchison, 1982) to time-series simplex data (West
and Harrison, 1997).

In LDA, the document-speciï¬c topic proportions Î¸ are
drawn from a Dirichlet distribution. In the dynamic topic
model, we use a logistic normal with mean Î± to express
uncertainty over proportions. The sequential structure be-
tween models is again captured with a simple dynamic
model

Î±t | Î±tâˆ’1 âˆ¼ N (Î±tâˆ’1, Î´2I) .

(2)

For simplicity, we do not model the dynamics of topic cor-
relation, as was done for static models by Blei and Lafferty
(2006).

By chaining together topics and topic proportion distribu-
tions, we have sequentially tied a collection of topic mod-
els. The generative process for slice t of a sequential corpus
is thus as follows:

1. Draw topics Î²t | Î²tâˆ’1 âˆ¼ N (Î²tâˆ’1, Ïƒ2I).
2. Draw Î±t | Î±tâˆ’1 âˆ¼ N (Î±tâˆ’1, Î´2I).
3. For each document:

(a) Draw Î· âˆ¼ N (Î±t, a2I)
(b) For each word:

i. Draw Z âˆ¼ Mult(Ï€(Î·)).
ii. Draw Wt,d,n âˆ¼ Mult(Ï€(Î²t,z)).

Note that Ï€ maps the multinomial natural parameters to the
mean parameters, Ï€(Î²k,t)w = exp(Î²k,t,w)

Pw exp(Î²k,t,w) .

The graphical model for this generative process is shown in
Figure 1. When the horizontal arrows are removed, break-
ing the time dynamics, the graphical model reduces to a set
of independent topic models. With time dynamics, the kth

Dynamic Topic Models

topic at slice t has smoothly evolved from the kth topic at
slice t âˆ’ 1.

For clarity of presentation, we now focus on a model with
K dynamic topics evolving as in (1), and where the topic
proportion model is ï¬xed at a Dirichlet. The technical is-
sues associated with modeling the topic proportions in a
time series as in (2) are essentially the same as those for
chaining the topics together.

3. Approximate Inference

Working with time series over the natural parameters en-
ables the use of Gaussian models for the time dynamics;
however, due to the nonconjugacy of the Gaussian and
multinomial models, posterior inference is intractable. In
this section, we present a variational method for approx-
imate posterior inference. We use variational methods as
deterministic alternatives to stochastic simulation, in or-
der to handle the large data sets typical of text analysis.
While Gibbs sampling has been effectively used for static
topic models (Grifï¬ths and Steyvers, 2004), nonconjugacy
makes sampling methods more difï¬cult for this dynamic
model.

The idea behind variational methods is to optimize the free
parameters of a distribution over the latent variables so that
the distribution is close in Kullback-Liebler (KL) diver-
gence to the true posterior; this distribution can then be
used as a substitute for the true posterior. In the dynamic
topic model, the latent variables are the topics Î²t,k, mixture
proportions Î¸t,d, and topic indicators zt,d,n. The variational
distribution reï¬‚ects the group structure of the latent vari-
ables. There are variational parameters for each topicâ€™s se-
quence of multinomial parameters, and variational param-
eters for each of the document-level latent variables. The
approximate variational posterior is

q(Î²k,1, . . . , Î²k,T | Ë†Î²k,1, . . . , Ë†Î²k,T ) Ã—

(3)

KYk=1

TYt=1  DtYd=1

q(Î¸t,d | Î³t,d)QNt,d

n=1 q(zt,d,n | Ï†t,d,n)! .

In the commonly used mean-ï¬eld approximation, each la-
tent variable is considered independently of the others. In
the variational distribution of {Î²k,1, . . . , Î²k,T }, however,
we retain the sequential structure of the topic by positing
a dynamic model with Gaussian â€œvariational observationsâ€
{ Ë†Î²k,1, . . . , Ë†Î²k,T }. These parameters are ï¬t to minimize the
KL divergence between the resulting posterior, which is
Gaussian, and the true posterior, which is not Gaussian.
(A similar technique for Gaussian processes is described
in Snelson and Ghahramani, 2006.)

The variational distribution of the document-level latent

!Î±

Î±

Î¸

z

!Î±

Î±

Î¸

z

w

N

A

w

N

A

Î²

!Î²

Î²

!Î²

!Î±

Î±

Î¸

z

w

N

A

Î²

!Î²

K

Figure2.A graphical representation of the variational approxima-
tion for the time series topic model of Figure 1. The variational
parameters Ë†Î² and Ë†Î± are thought of as the outputs of a Kalman
ï¬lter, or as observed data in a nonparametric regression setting.

variables follows the same form as in Blei et al. (2003).
Each proportion vector Î¸t,d is endowed with a free Dirichlet
parameter Î³t,d, each topic indicator zt,d,n is endowed with
a free multinomial parameter Ï†t,d,n, and optimization pro-
ceeds by coordinate ascent. The updates for the document-
level variational parameters have a closed form; we use
the conjugate gradient method to optimize the topic-level
variational observations. The resulting variational approx-
imation for the natural topic parameters {Î²k,1, . . . , Î²k,T }
incorporates the time dynamics; we describe one approx-
imation based on a Kalman ï¬lter, and a second based on
wavelet regression.

3.1. Variational Kalman Filtering

The view of the variational parameters as outputs is
based on the symmetry properties of the Gaussian density,
fÂµ,Î£(x) = fx,Î£(Âµ), which enables the use of the standard
forward-backward calculations for linear state space mod-
els. The graphical model and its variational approximation
are shown in Figure 2. Here the triangles denote varia-
tional parameters; they can be thought of as â€œhypothetical
outputsâ€ of the Kalman ï¬lter, to facilitate calculation.

To explain the main idea behind this technique in a sim-
pler setting, consider the model where unigram models Î²t
(in the natural parameterization) evolve over time. In this
model there are no topics and thus no mixing parameters.
The calculations are simpler versions of those we need for
the more general latent variable models, but exhibit the es-

Dynamic Topic Models

sential features. Our state space model is

Î²t | Î²tâˆ’1 âˆ¼ N (Î²tâˆ’1, Ïƒ2I)
wt,n | Î²t âˆ¼ Mult(Ï€(Î²t))

and we form the variational state space model where

Ë†Î²t | Î²t âˆ¼ N (Î²t, Ë†Î½2

t I)

The variational parameters are Ë†Î²t and Ë†Î½t. Using standard
Kalman ï¬lter calculations (Kalman, 1960), the forward
mean and variance of the variational posterior are given by

mt â‰¡ E (Î²t | Ë†Î²1:t) =

(cid:18)

Ë†Î½2
t

Vtâˆ’1 + Ïƒ2 + Ë†Î½2

t(cid:19) mtâˆ’1 +(cid:18)1 âˆ’

Ë†Î½2
t

Vtâˆ’1 + Ïƒ2 + Ë†Î½2

t(cid:19) Ë†Î²t

Vt â‰¡ E ((Î²t âˆ’ mt)2 | Ë†Î²1:t)

= (cid:18)

Ë†Î½2
t

Vtâˆ’1 + Ïƒ2 + Ë†Î½2

t(cid:19) (Vtâˆ’1 + Ïƒ2)

take n = 2J and J = 7. To be consistent with our earlier
notation, we assume that

Ë†Î²t = emt + Ë†Î½t

where t âˆ¼ N (0, 1). Our variational wavelet regression
algorithm estimates { Ë†Î²t}, which we view as observed data,
just as in the Kalman ï¬lter method, as well as the noise
level Ë†Î½.

For concreteness, we illustrate the technique using the Haar
wavelet basis; Daubechies wavelets are used in our actual
examples. The model is then

Ë†Î²t = Î±Ï†(xt) +

J âˆ’1Xj=0

2j

âˆ’1Xk=0

DjkÏˆjk(xt)

where xt = t/n, Ï†(x) = 1 for 0 â‰¤ x â‰¤ 1,

Ïˆ(x) =(cid:26) âˆ’1 if 0 â‰¤ x â‰¤ 1

2 ,
2 < x â‰¤ 1

1 if 1

with initial conditions speciï¬ed by ï¬xed m0 and V0. The
backward recursion then calculates the marginal mean and
variance of Î²t given Ë†Î²1:T as

and Ïˆjk(x) = 2j/2Ïˆ(2jx âˆ’ k). Our variational estimate
for the posterior mean becomes

Ïƒ2

Ïƒ2

emtâˆ’1 â‰¡ E (Î²tâˆ’1 | Ë†Î²1:T ) =
Vtâˆ’1 + Ïƒ2(cid:19) mtâˆ’1 +(cid:18)1 âˆ’
Vtâˆ’1 + Ïƒ2(cid:19)emt
(cid:18)
eVtâˆ’1 â‰¡ E ((Î²tâˆ’1 âˆ’ emtâˆ’1)2 | Ë†Î²1:T )
= Vtâˆ’1 +(cid:18) Vtâˆ’1
Vtâˆ’1 + Ïƒ2(cid:19)2(cid:16)eVt âˆ’ (Vtâˆ’1 + Ïƒ2)(cid:17)
with initial conditions emT = mT and eVT = VT . We ap-

proximate the posterior p(Î²1:T | w1:T ) using the state space
posterior q(Î²1:T | Ë†Î²1:T ). From Jensenâ€™s inequality, the log-
likelihood is bounded from below as

log p(d1:T ) â‰¥

Z q(Î²1:T | Ë†Î²1:T ) log  p(Î²1:T ) p(d1:T | Î²1:T )

q(Î²1:T | Ë†Î²1:T )

(4)

! dÎ²1:T

= E q log p(Î²1:T ) +

TXt=1

E q log p(dt | Î²t) + H(q)

Details of optimizing this bound are given in an appendix.

3.2. Variational Wavelet Regression

The variational Kalman ï¬lter can be replaced with varia-
tional wavelet regression; for a readable introduction stan-
dard wavelet methods, see Wasserman (2006). We rescale
time so it is between 0 and 1. For 128 years of Science we

emt = Ë†Î±Ï†(xt) +

where Ë†Î± = nâˆ’1Pn

t=1
olding the coefï¬cients

J âˆ’1Xj=0

2j

âˆ’1Xk=0

Ë†DjkÏˆjk(xt).

Ë†Î²t, and Ë†Djk are obtained by thresh-

Zjk =

1
n

nXt=1

Ë†Î²tÏˆjk(xt).

To estimate Ë†Î²t we use gradient ascent, as for the Kalman

soft thresholding is used, then we have that

ï¬lter approximation, requiring the derivatives âˆ‚emt/âˆ‚ Ë†Î²t. If

with âˆ‚ Ë†Î±/âˆ‚ Ë†Î²s = nâˆ’1 and

=

âˆ‚ Ë†Î²s

Ï†(xt) +

âˆ‚ Ë†Î±
âˆ‚ Ë†Î²s

âˆ‚emt
âˆ‚ Ë†Djk/âˆ‚ Ë†Î²s = ( 1

J âˆ’1Xj=0

2j

âˆ’1Xk=0

âˆ‚ Ë†Djk
âˆ‚ Ë†Î²s

Ïˆjk(xt).

n Ïˆjk(xs)
0

if |Zjk| > Î»
otherwise.

Note also that |Zjk| > Î» if and only if | Ë†Djk| > 0. These
derivatives can be computed using off-the-shelf software
for the wavelet transform in any of the standard wavelet
bases.

Sample results of running this and the Kalman variational
algorithm to approximate a unigram model are given in
Figure 3. Both variational approximations smooth out the

Dynamic Topic Models

Darwin

Einstein

moon

4
0
âˆ’
e
6

4
0
âˆ’
e
4

4
0
âˆ’
e
2

0
0
+
e
0

3
0
âˆ’
e
1

4
0
âˆ’
e
8

4
0
âˆ’
e
6

4
0
âˆ’
e
4

4
0
âˆ’
e
2

0
0
+
e
0

1880

1900

1920

1940

1960

1980

2000

1880

1900

1920

1940

1960

1980

2000

1880

1900

1920

1940

1960

1980

2000

4
0
âˆ’
e
6

4
0
âˆ’
e
4

4
0
âˆ’
e
2

0
0
+
e
0

3
0
âˆ’
e
1

4
0
âˆ’
e
8

4
0
âˆ’
e
6

4
0
âˆ’
e
4

4
0
âˆ’
e
2

0
0
+
e
0

1880

1900

1920

1940

1960

1980

2000

1880

1900

1920

1940

1960

1980

2000

1880

1900

1920

1940

1960

1980

2000

2
1
0
0

.

0

8
0
0
0
0

.

4
0
0
0
0

.

0
0
0
0

.

0

2
1
0
0

.

0

8
0
0
0

.

0

4
0
0
0

.

0

0
0
0
0

.

0

Figure3.Comparison of the Kalman ï¬lter (top) and wavelet regression (bottom) variational approximations to a unigram model. The
variational approximations (red and blue curves) smooth out the local ï¬‚uctuations in the unigram counts (gray curves) of the words
shown, while preserving the sharp peaks that may indicate a signiï¬cant change of content in the journal. The wavelet regression is able
to â€œsuperresolveâ€ the double spikes in the occurrence of Einstein in the 1920s. (The spike in the occurrence of Darwin near 1910 may
be associated with the centennial of Darwinâ€™s birth in 1809.)

local ï¬‚uctuations in the unigram counts, while preserving
the sharp peaks that may indicate a signiï¬cant change of
content in the journal. While the ï¬t is similar to that ob-
tained using standard wavelet regression to the (normal-
ized) counts, the estimates are obtained by minimizing the
KL divergence as in standard variational approximations.

In the dynamic topic model of Section 2, the algorithms
are essentially the same as those described above. How-
ever, rather than ï¬tting the observations from true ob-
served counts, we ï¬t them from expected counts under the
document-level variational distributions in (3).

4. Analysis of Science

We analyzed a subset of 30,000 articles from Science, 250
from each of the 120 years between 1881 and 1999. Our
data were collected by JSTOR (www.jstor.org), a not-
for-proï¬t organization that maintains an online scholarly
archive obtained by running an optical character recogni-
tion (OCR) engine over the original printed journals. JS-
TOR indexes the resulting text and provides online access
to the scanned images of the original content through key-
word search.

Our corpus is made up of approximately 7.5 million words.
We pruned the vocabulary by stemming each term to its
root, removing function terms, and removing terms that oc-
curred fewer than 25 times. The total vocabulary size is

15,955. To explore the corpus and its themes, we estimated
a 20-component dynamic topic model. Posterior inference
took approximately 4 hours on a 1.5GHZ PowerPC Mac-
intosh laptop. Two of the resulting topics are illustrated in
Figure 4, showing the top several words from those topics
in each decade, according to the posterior mean number of
occurrences as estimated using the Kalman ï¬lter variational
approximation. Also shown are example articles which ex-
hibit those topics through the decades. As illustrated, the
model captures different scientiï¬c themes, and can be used
to inspect trends of word usage within them.

To validate the dynamic topic model quantitatively, we con-
sider the task of predicting the next year of Science given all
the articles from the previous years. We compare the pre-
dictive power of three 20-topic models: the dynamic topic
model estimated from all of the previous years, a static
topic model estimated from all of the previous years, and a
static topic model estimated from the single previous year.
All the models are estimated to the same convergence crite-
rion. The topic model estimated from all the previous data
and dynamic topic model are initialized at the same point.

The dynamic topic model performs well; it always assigns
higher likelihood to the next yearâ€™s articles than the other
two models (Figure 5). It is interesting that the predictive
power of each of the models declines over the years. We
can tentatively attribute this to an increase in the rate of
specialization in scientiï¬c language.

Dynamic Topic Models

B

@

9

?

:
?

<

A

I

I

<

I

9
@

?

?

K

:

>

?

H

A

<

@

>

?

K

J

?

A

J

?

C

F
?

F

C

C

C

?

[

]

^

M

?

@

B

?

<

<
I

<

@

?

K

A

>

:

?

A

@

J

I

A

>

J

?

L

H

?
?

K

a

b

C

C

?

C

?
F

F

`

9

@

?

Â

Â

{

Â„
Â‡

Â„

~

z

Âƒ

|

Âˆ

Â†

Â„

Â

Â„

Â‚

~

Â

Â‰

Â

Â‚

~

Â

Â€

Â

Â

{

Âˆ

Âƒ

Â‚

Â„

Â†

Â€

Â€

q

Â€

z

Â€

ÂŠ

Â„

Â„

Â€

ÂŠ

Â„

r

Â„

Â‘

s

u

v

Â

Â„

ÂŠ
Â€

Â€
ÂŠ

{

Â„
Â‡

Â„

~

Â‰

Â

~

Â

z

Âƒ

Â„

Âˆ
|

Â†

Â

Â‚
Â„

Â

Â

Âˆ

Â„

Â€

Â‚

Â†
Â€
Â

r

Â‚

Â

~

Â

Â’

w

B

B

@

?

<

J

P

C

B

?

>

K

>

M

<

?

A

K

?

k

l

m

K

?

|

Âˆ

C

C

?

J

C

9

?

I

<

L

A

>

@

I
?

J

<

<

?

B

F

H

N

J
<

>

M

@

<

?

?

?

@

d

`

d

Â

Â‚

Â

Â€

Âƒ

Â

w

r

c

Â€

Â€

Â„

Â„

y

?

M

?

B

C

>

@

<
I
?

9

@

I
B
?

H

O

L

?

?

J

I

<

A

<

>

@

C

?
J

?

C

M

?

B

C

>

B

@

C

9

?

?

?

K

B

f

A

>

<

J

F

e

?

h

Â€

z

Âƒ

Â„

~
Â

Â„

Â

Â‚

Â

Â„
Â‡
Â„

~

Â‰

Â

Â

Â

Â

Âˆ

Âƒ
Â‚

Â„

Â€

Â†

Â€

Â€

ÂŠ

Â

Â„

Â€

ÂŠ

Â„

Â„

Â¥

Âœ

Â¦

Â

Â¦

Â

Â§

m

Â

Â¥

|

z

Âƒ

Â‚

Â„

Â†

Â

Â„

Â‚

Â

Â

~

Â€

Â„

Â

Â€

h

h

Â¤

Âœ

Â„

ÂŠ

Â„

Â€

Â

Â„

Â„

Â

Â€

Â

Â¨

ÂŸ

Â

Â„

Â„
Â‡

Âƒ

~

Â

Â

Â‰

Â§

Â

z

Â

Âƒ

Âƒ

Âˆ
Â‚

Â€

Â„

Â§

Â†

Â

?

?

C

9

?

@

H

I

?

?

Q

<

I

h

Â¨

ÂŸ

Â¥

Âœ

L

?

I

>

?

<

@

h

?

I

Â©

Â 

Â§

Â

Â¥

Â

Â„

|

Â

Â

Â†

~

Â

Â‚

Â

Â

~

Â€

Â‚

Â

Â„

z

Â‚

~

Âƒ

Â

Â”

Â

Â†

Â„

Â’

|

Â

Â

Â„

m

Âœ

Â€

Â„

Â„

Â€

Â„

Â„

Â

l

m

ÂŠ

Â„

Â

Â„

Â‚

~

Â€

Â€

o

Â¨

ÂŸ

Â“

Â

Â€

Â†

Âƒ

Âƒ

Â„

h

Âª

Â¡

Â§

Â

?

I
?

I

L

?

F

A

I

<

>

9

@

I

H

?

I

S

Q

?

I

J
<

R

B

H

Q

K

M

K

A

I

C

?

?

B

B

>

C

?

?

C

?

?

N

9

@

I

?

I

?

I

N
H

L
?

>

A

I

<

?

<

I
F

A

K

M

<

T

H

B

J

<

C
>

@

I

m

z

Â‚

o

f

Â

Â‚

Â

Â„

Â„

Â

Âƒ

Â–

Â”

Â

~

Â

Â€

|

n

Â

Â§

Â

Â«

Â¢

Â¥

Âœ

Â¨

ÂŸ

o

e

Â

Â„

Â
|

Â

Â

Â

Â‚

~

Â

Â†

Â’

Â„

Â¥

Â„
Â„

Âœ

Â§

Â„

Â€

Â„

Â‰

Â¦

Â„

Â

Â„

Â¨

Â€

ÂŠ

~

Â

Â„

Â€

Â„

Â€

Â†

Â†

Â€

Â•

Â

Âƒ

Âƒ

k

f

Â£

ÂŸ

Â

Â

Â©

Â 

?

A

M

<

?

C

P

?

@

W

J

I
B

I

I
H

M
?

?

>

I

B

A
<

@

J

S

?

?

C

J

Y

K

N

M

F

@
C

<

?

J

B

@

C

>

K



Â€



7

	

4





9
I
@

W



?

?
I

I





4

J

#








	








'

?

N

?

<



"

C








	

Â



	






Â„

Â‰

Â„

Â„

Â†

~





	









Â®

O







	


	

	







Â

Â’

Â†

Â

Â

Â„

z
Â

Â





Â„

Â
Â†

Â†

Â¬










Â˜
Â‚

Âƒ

ÂŠ

Â„
Â€

Â†

Âƒ

Â‚





Â‚

Â

Â

'









Â

Â„

Â€

Â‰

~

ÂŠ

Â€

Â„

ÂŠ

















	

$







"





'

C
>

 

 

9

@
I

?

?
I

I

O

L
?

>

F
I

<

I

J

S

A

U

B

<

H

J

<

?

C

?

N

I

 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 

Â„

Â„
Â„


























 

 





)
*
/
1
 






)
*
/
1
 





 

)
)

/
/
/


 



)

)
*
)
)

M

<

A

I

K

@

?

B

?

N





&


2

6

'

Â

Â„

'

'


8
&


$
&
8



5



Â’



Â
Â„



Â„










Â–

'











Â€

Â

~

Â„

Â‚





Â„



z

Â„

|

Â

Â”

Â€

Â

Â„
~

Â‚

Â’

Â„

Â†

Â
Â†

Â

Â†

|

Â„

ÂŠ

Â
Â„

Â„

~

Â€

Â—

Âƒ

Â‚

Â„

Â€

Â

Â‚

Â

~

Â€

Â†

Â‚

Âƒ

Â

Â„

ÂŠ

~

B

Â§

?

@

B

?

Â§

Â’

Â

Â„

Â„

Â

Â

Â

Â§

Â

F

I

L
?

>

?

<

?





I

S



M

<

I

A

K

B

@

?

V

J

B

<

C

F

>

Q





(







&











?

N

C



	

?



~



?

9

@

I
W

I
?

I

I



C







5

(



'

Â¯

L
>
?

?

I

<












<

A

F
I

0

B

?



?







X

B

J

F

M

<

A

I

F

C

>
@

K

?











'






	




































?



Â’

@





,

"

~





	

,







Â„









	



Â„

Â€

Â‚
Â
~

Â€

|

Â„











Â¬





$








Â„

Â†

z
Â

Â†

Â†

Â„





Â

Â„



(









Â’

Â

Â

Â„

Â

Â‰







	
2

2

?

I

9

@
W
I

I

?

?

?

N



C









C

Â









	

	





Â€

Â„

Â

Â„

ÂŠ

~

Â€

Â†



Â

Â‚

~







Â›
Â‚

Â€

Âƒ

Âƒ
Â„

Â‚



5

&






	




	





(

	




















5



0






	



Â„

Â„

Â€

Â

5

C











Â

ÂŠ

Â‚

~

Â

Â„

Â„

Â
Â†





Â€

	

Âƒ

Â„
Â„

"

Â™

Âƒ























?













Â„

Â
Â†

Â’
Â

?











0



	



	



	

Â‚







	

Â



	






,

	

Â

Â„

Â

~

Â‚

Â€







z

Â

Â€



Â

Â„































	

Â

ÂŠ

Â†

Â„

|

Â’

Â†

Â„

Â‚

Â

Â‚

Â€

Â













Â‰











"


	



0



	

	

Â¬






	



	








M

<

@

B

?

:

<

B

A

I

>

F

C





?

K



F

J





L
>
?
?

F

I

<





<
S

I













Â­









4


















?

I

C

J

9

@
W

I

?

?

J

C

>

L

?
?

<

A

I

I

K

<
S

M

<

A

?

?

>

F

L

<

B

F

C

@






























{

Â‚
Â€

Â€
Â

Âƒ

Â†

Â„
Âƒ

Â’

(

Â

Â„

5

Â„

Â

Â„

Â€

Â

Â„

Â‚

K

?



Â†

@

B







~

Âˆ

Â€

Â„

Â
Â†

Â„







~



Â„

Â€

~

Â

Â

Â

Â„

Â’



Â†

~

Â†

Â†

Â

z

Â†

Â

|

Â„

Â

Â

Â’
Â‚
Â„

Â“
Âƒ

Â†

(





















4



4






















0












(



?









~
Â

Â

|

Â‰

Â€



Â„

z

Â†








Â„



Â€

Â„

Â

Â













Â„

~

Â

Â‚

Â

	


Â±



Â
Â

Â„

	



	

Â†

Â†

Â
Â„

Â’



Â








Â€

Â

Â„

Â€
Â€

Â„

Â‚

Â€

Â„



Â±

"

Â€
Â‚





Â



	

	








2

	











Â”

Â‡

8

Â
Â

Â„

~

Â

Â’

Âƒ

Â’

~
Â„

Â‚
Â€

Â‡

Â€



Â‚

Â€

Â€



Â€

~

Â

Â„

Â“
Â„

|

Â†
Â†

Â’

~

Â†

Â€

~

Â

Â

Âˆ

Â„

Â†

Â

Â„

Â‰









Â„

Â„

Â€

Â

Â„
Â‚



Â



"









	

	







Â°















0







Figure4.Examples from the posterior analysis of a 20-topic dynamic model estimated from the Science corpus. For two topics, we
illustrate: (a) the top ten words from the inferred posterior distribution at ten year lags (b) the posterior estimate of the frequency as a
function of year of several words from the same two topics (c) example articles throughout the collection which exhibit these topics.
Note that the plots are scaled to give an idea of the shape of the trajectory of the wordsâ€™ posterior probability (i.e., comparisons across
words are not meaningful).

5. Discussion

We have developed sequential topic models for discrete
data by using Gaussian time series on the natural param-
eters of the multinomial topics and logistic normal topic
proportion models. We derived variational inference algo-
rithms that exploit existing techniques for sequential data;
we demonstrated a novel use of Kalman ï¬lters and wavelet
regression as variational approximations. Dynamic topic
models can give a more accurate predictive model, and also
offer new ways of browsing large, unstructured document
collections.

There are many ways that the work described here can be
extended. One direction is to use more sophisticated state
space models. We have demonstrated the use of a simple

Gaussian model, but it would be natural to include a drift
term in a more sophisticated autoregressive model to ex-
plicitly capture the rise and fall in popularity of a topic, or
in the use of speciï¬c terms. Another variant would allow
for heteroscedastic time series.

Perhaps the most promising extension to the methods pre-
sented here is to incorporate a model of how new topics in
the collection appear or disappear over time, rather than as-
suming a ï¬xed number of topics. One possibility is to use a
simple Galton-Watson or birth-death process for the topic
population. While the analysis of birth-death or branching
processes often centers on extinction probabilities, here a
goal would be to ï¬nd documents that may be responsible
for spawning new themes in a collection.


















!
!




%
%




















+

+



-
.
.
.



















3




































;
=
=
D
E
E
;
;
=
G
E
D
=
D
E
=
D
D
D
=
=
G
D
D
E
;
=
D
=
=
E
;
;
=
E
=
D
G
E
=
G
D
G
E
D
D
G
D
=
E
=
;
=
D
=
D
D
E
G
E
E
D
E
;
;
=
;
=
D
D
=
G
D
=
E
D
D
=
G
E
=
D
D
=
G
D
=
=
=
=
G
D
=
G
E
;
=
G
=
=
=
D
=
G
E
D
=
E
D
D
G
D
=
E
G
=
=
G
D
=
G
E
D
D
=
D
E
G
=
=
E
D
E
D
D
=
=
=
E
D
=
G
D
=
=
D
=
D
E
G
D
G
E
D
=
D
=
E
D
=
G
D
=
=
=
D
G
D
=
D
E
G
=
G
D
=
=
D
=
=
E
D
G
=
G
=
D
E
G
D
G
D
=
=
=
D
E
G
G
=
E
D
G
D
D
=
=
G
D
=
D
=
D
=
D
=
G
G
D
D
D
D
=
D
D
=
G
D
=
D
G
D
D
D
D
E
Z
\
_
_
Z
g
g
i
j
g
i
g
p
t
x
p
}

Â…
Â…

}

Â…
Â‹
ÂŒ
Â…
Â‹
}
Â…
}

Â…
}

Â‹
ÂŒ
Â…
Â…

Â‹
}

Â…
}

Â…
}

Â…
Â‹
ÂŒ
Â…
}


Â‹
Â…
}

}
Â…

Â‹
Â‹
ÂŒ
Â…
}

Â…
}
Â…
Â‹
Â…

}
Â…

Â‹
Â…
}
}
Â…

}

}
Â…

Â‹
Â…
Â‹
Â…
}
}
ÂŒ
}
}
Â…

}

}
}
}
Â…

Â‹
Â…
}
Â‹
Â‹
Â…
}
Â…

}

}
Â…
Â…

Â‹
}
}
}
Â…

Â‹
Â…
}
Â‹
Â…

ÂŒ
}
Â‹
Â…
}
Â…
Â…

Â‹
Â…
}
Â…

Â‹
Â…
}
}

Â…

Â…

Â…
Â…

Â‹
Â…

Â‹
}
Âš
Â…
}
Â‹
Â‹
Â…
Â…

Â‹
Â…

Â‹
}
Â…

}
}
}
Â…
}
Â…
Â‹
Â‹
}
}
Â…

}

Â…

Â‹
Â‹
Â…
}
}
Â…
}
Â‹
Â‹
Â‹
}
Â…

}

}
Â…
}
Â‹
}
}
}
Â…
}
Â…

}

Â‹
Â‹
}
Â‹
}
Â…
}

Â‹
i
i














!
!


%
%











.





















































































Dynamic Topic Models

6
0
+
e
7

6
0
+
e
4

6
0
+
e
2

6
0
+
e
1

l

)
e
a
c
s
 

g
o
l
(
 

d
o
o
h

i
l

e
k

i
l
 

g
o

l
 

e
v
i
t

a
g
e
N

LDAâˆ’prev
LDAâˆ’all
DTM

1920

1940

1960

1980

2000

Year

Figure5. This ï¬gure illustrates the performance of using dy-
namic topic models and static topic models for prediction. For
each year between 1900 and 2000 (at 5 year increments), we es-
timated three models on the articles through that year. We then
computed the variational bound on the negative log likelihood of
next yearâ€™s articles under the resulting model (lower numbers are
better). DTM is the dynamic topic model; LDA-prev is a static
topic model estimated on just the previous yearâ€™s articles; LDA-
all is a static topic model estimated on all the previous articles.

Acknowledgments

This research was supported in part by NSF grants IIS-
0312814 and IIS-0427206, the DARPA CALO project, and
a grant from Google.

References
Aitchison, J. (1982). The statistical analysis of composi-
tional data. Journal of the Royal Statistical Society, Se-
ries B, 44(2):139â€“177.

Blei, D., Ng, A., and Jordan, M. (2003). Latent Dirich-
let allocation. Journal of Machine Learning Research,
3:993â€“1022.

Blei, D. M. and Lafferty, J. D. (2006). Correlated topic
models. In Weiss, Y., SchÃ¶lkopf, B., and Platt, J., editors,
Advances in Neural Information Processing Systems 18.
MIT Press, Cambridge, MA.

Buntine, W. and Jakulin, A. (2004). Applying discrete PCA
in data analysis. In Proceedings of the 20th Conference
on Uncertainty in Artiï¬cial Intelligence, pages 59â€“66.
AUAI Press.

Erosheva, E. (2002). Grade of membership and latent
structure models with application to disability survey

data. PhD thesis, Carnegie Mellon University, Depart-
ment of Statistics.

Fei-Fei, L. and Perona, P. (2005). A Bayesian hierarchi-
IEEE

cal model for learning natural scene categories.
Computer Vision and Pattern Recognition.

Grifï¬ths, T. and Steyvers, M. (2004). Finding scientiï¬c
topics. Proceedings of the National Academy of Science,
101:5228â€“5235.

Kalman, R. (1960). A new approach to linear ï¬ltering and
prediction problems. Transaction of the AMSE: Journal
of Basic Engineering, 82:35â€“45.

McCallum, A., Corrada-Emmanuel, A., and Wang, X.
(2004). The author-recipient-topic model for topic and
role discovery in social networks: Experiments with En-
ron and academic email. Technical report, University of
Massachusetts, Amherst.

Pritchard, J., Stephens, M., and Donnelly, P. (2000). Infer-
ence of population structure using multilocus genotype
data. Genetics, 155:945â€“959.

Rosen-Zvi, M., Grifï¬ths, T., Steyvers, M., and Smith, P.
(2004). The author-topic model for authors and docu-
ments.
In Proceedings of the 20th Conference on Un-
certainty in Artiï¬cial Intelligence, pages 487â€“494. AUAI
Press.

Sivic, J., Rusell, B., Efros, A., Zisserman, A., and Freeman,
W. (2005). Discovering objects and their location in im-
ages. In International Conference on Computer Vision
(ICCV 2005).

Snelson, E. and Ghahramani, Z. (2006). Sparse Gaussian
processes using pseudo-inputs. In Weiss, Y., SchÃ¶lkopf,
B., and Platt, J., editors, Advances in Neural Information
Processing Systems 18, Cambridge, MA. MIT Press.

Wasserman, L. (2006). All of Nonparametric Statistics.

Springer.

West, M. and Harrison, J. (1997). Bayesian Forecasting

and Dynamic Models. Springer.

A. Derivation of Variational Algorithm

In this appendix we give some details of the variational
algorithm outlined in Section 3.1, which calculates a dis-
tribution q(Î²1:T | Ë†Î²1:T ) to maximize the lower bound on

log p(d1:T ). The ï¬rst term of the righthand side of (5) is

Next, we maximize with respect to Ë†Î²s:

Dynamic Topic Models

âˆ‚`( Ë†Î², Ë†Î½)
âˆ‚ Ë†Î²sw

=

+

âˆ’

âˆ’

1
Ïƒ2

âˆ‚ Ë†Î²sw

(emtw âˆ’ emtâˆ’1,w)(cid:18) âˆ‚emtw

TXt=1
TXt=1(cid:16)ntw âˆ’ nt Ë†Î¶ âˆ’1

âˆ‚ Ë†Î²sw (cid:19)
âˆ‚emtâˆ’1,w
exp(emtw +eVtw/2)(cid:17) âˆ‚emtw
The forward-backward equations for emt can be used to de-
rive a recurrence for âˆ‚emt/âˆ‚ Ë†Î²s. The forward recurrence is

âˆ‚ Ë†Î²sw

Ë†Î½2
t

.

t

+

= (cid:18)
(cid:18)1 âˆ’

vtâˆ’1 + Ïƒ2 + Ë†Î½2

Ë†Î½2
t

vtâˆ’1 + Ïƒ2 + Ë†Î½2

âˆ‚ Ë†Î²s

t(cid:19) âˆ‚mtâˆ’1
t(cid:19) Î´s,t ,

âˆ‚mt
âˆ‚ Ë†Î²s

with the initial condition âˆ‚m0/âˆ‚ Ë†Î²s = 0. The backward
recurrence is then

+

Ïƒ2

âˆ‚ Ë†Î²s

âˆ‚ Ë†Î²s

âˆ‚emtâˆ’1

= (cid:18)
Vtâˆ’1 + Ïƒ2(cid:19) âˆ‚mtâˆ’1
Vtâˆ’1 + Ïƒ2(cid:19) âˆ‚emt
(cid:18)1 âˆ’
with the initial condition âˆ‚emT /âˆ‚ Ë†Î²s = âˆ‚mT /âˆ‚ Ë†Î²s.

âˆ‚ Ë†Î²s

Ïƒ2

,

E q log p(Î²t | Î²tâˆ’1) = âˆ’

TXt=1

V T

2 (cid:0)log Ïƒ2 + log 2Ï€(cid:1)

E q(Î²t âˆ’ Î²tâˆ’1)T (Î²t âˆ’ Î²tâˆ’1)

âˆ’

1
2Ïƒ2

TXt=1

V T

= âˆ’

TXt=1

1
2Ïƒ2

2 (cid:0)log Ïƒ2 + log 2Ï€(cid:1) âˆ’
Tr(cid:16)eVt(cid:17) +
TXt=1

kemt âˆ’ emtâˆ’1k2
2Ïƒ2(cid:16)Tr (eV0) âˆ’ Tr (eVT )(cid:17)

1
Ïƒ2

1

âˆ’

using the Gaussian quadratic form identity

E m,V (x âˆ’ Âµ)T Î£âˆ’1(x âˆ’ Âµ) =

(m âˆ’ Âµ)T Î£âˆ’1(m âˆ’ Âµ) + Tr (Î£âˆ’1V ).

The second term of (5) is

TXt=1

â‰¥

E q log p(dt | Î²t) =

TXt=1Xw
TXt=1Xw

+

ntw E q Î²tw âˆ’ logXw
t Xw
ntw emtw âˆ’ nt Ë†Î¶ âˆ’1
TXt=1

nt âˆ’ nt log Ë†Î¶t

exp(Î²tw)!
exp(emtw +eVtw/2)

where nt = Pw ntw, introducing additional variational

parameters Ë†Î¶1:T . The third term of (5) is the entropy

H(q) =

=

2

TXt=1(cid:18) 1
TXt=1Xw

1
2

T
2

log |eVt| +
logeVtw +

log 2Ï€(cid:19)

T V
2

log 2Ï€.

To maximize the lower bound as a function of the varia-
tional parameters we use a conjugate gradient algorithm.
First, we maximize with respect to Ë†Î¶; the derivative is

âˆ‚`
âˆ‚ Ë†Î¶t

=

nt
Ë†Î¶ 2

t Xw

exp(emtw +eVtw/2) âˆ’

nt
Ë†Î¶t

.

Setting to zero and solving for Ë†Î¶t gives

Ë†Î¶t =Xw

exp(emtw +eVtw/2).

